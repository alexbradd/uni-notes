# 8 ottobre

## Spazi vettoriali
[...] 

### Sottospazi vettoriali
[...]

#### Esempi di sottospazi
Dimostrare che $A \in Mat(m,n;K) \implies Ker(A)$ è un sottospazio di $Mat(m,1;K)$

Dimostriamo che è chiuso rispetto alle operazioni. Siano $B,C \in Ker(A)$, allora
$AB=0, AC=0$ avremo quindi:

- $A(B+C) = AB + AC = 0 + 0 = 0 \implies B+C \in Ker(A)$
- $A(tB) = t(AB) = t * 0 = 0 \implies t*B \in Ker(A)$
- $0 \in Ker(A)$ sempre perché $0$ è sempre soluzione di un sistema omogeneo

Allora $Ker(A)$ è un sottospazio di $Mat(m,1;K)$

#### Operazioni con i sottospazi
Le operazioni con i sottospazi sono quasi le stesse che si possono fare con gli 
insiemi:

- intersezione
- unione
- somma (Disp 4.15): 
    $U_1, U_2 \subseteq V, U_1 + U_2 = \{v \in V \mid v = u_1 + u_2 \quad u_1 \in U_1, u_2 \in U_2\}$
- somma diretta (capitolo a parte)

##### Proprietà delle operazioni (dispensa 4.16):

- $U_1 \cap U_2$ è ancora un sottospazio
- $U_1 \cup U_2$ è ancora uno sottospazio $\iff U_1 \subseteq U_2 \vee U_2 \subseteq U_1$
- $U_1 + U_2$ è il più piccolo sottospazio che contiene $U_1 \cup U_2$

##### Somma diretta (dispensa 4.19)
Dati $U_1, \dotsc, U_n \subseteq V$ sottospazi. Si dice $V = U_1 \oplus \ldots \oplus u_n = \bigoplus_{i=1}^n$ è 
somma diretta dei sottospazi se per ogni $v \in V$ esiste un'unica m-upla
contenuta in $U_1 \times \ldots \times U_n$ tale che $v = u_1 + \ldots + u_n$.

Esempi interessanti: 

- Una matrice può essere scritta come la somma diretta di una matrice triangolare 
    alta e una matrice strettamente basse: $Mat(n;K) = T_a(n;K) \oplus T_{sb}(n;K)$
- Una matrice può essere scritta come somma diretta di una matrice a scala e una
    matrice antisimmetrica

##### Caratterizzazione della somma diretta
$V = U_1 \oplus \ldots \oplus U_n$ con $n \geq 2$:

- $V = U_1 + \ldots - U_n$
- $U_k \cap U_{\hat{k}} = \{\underline{0}\} \quad \forall k \in K$,
    $U_{\hat{k}} = U_1 + \ldots + U_{k-1} + U_{k+1} + \ldots + U_n$

## Dimensione
Il numero di dimensioni è il numero minimo di spostamenti fondamentali che
bisogna eseguire per definire qualsiasi vettore in quello spazio.

## Combinazione lineare
$v = t_1 * u_1 + \ldots + t_n * u_n$ questa è detta combinazione lineare di 
vettori.

## Base
E' detta base di di $V$ un insieme $B = \{ v_1, \dotsc, v_m \} \subset V$ se per
ogni $v \in V$ esiste unica la decomposizione $v = t_1 * u_1 + \ldots + t_n * u_n$

In uno spazio non per forza esistono le basi.

Poiché i coefficienti della scomposizione sono unici (per definizione di base),
è possibile definire una funzione (la mappa delle componenti) che associa a 
un vettore di uno spazio la su

### Basi canoniche
Le basi canoniche sono le basi più semplici che vengono convenientemente usate:

- $Mat(m,n;K): \quad B_{mn} = \{ E_{11}, E_{12}, \dotsc, E_{mn} \}$ dove $E$ è 
    una matrice che solo l'elemento $a_{mn}$ pari a 1.

    Una matrice $A$ si può scrivere come $A = \sum_{i=1}^m \sum_{j=1}^n a_{ij} * E_{ij}$
- $K^n: \quad B_n = \{ e_1 = (1, \dotsc, 0), \dotsc, e_n = (0, \dotsc, n)\}$

    Un generico vettore sarà: $x §= \sum_{i=1}^n x_i * e_i$
- $K[x]: \quad B = \{1, x, \dotsc, x^n, \dotsc \}$ (spazio vettoriale con base infinita)

    Un generico polimonio sarà: $P(x) = \sum_{i=0}^n a^0 * x^n$

### Mappa delle componenti
Sia $B = \{v_1, \dotsc, v_n\}$ base di $V$ e $v = t_1v_1 + \ldots + t_nv_n$,
definiamo mappa delle componenti la funzione:
$$\begin{align}
    \phi_{B} : &V &\to &Mat(n, 1;K) \\
    &v &\mapsto &v_{|B} =   \begin{bmatrix}
                                t_1 \\
                                \vdots \\
                                t_n
                            \end{bmatrix}
\end{align}$$

#### Mappa delle componenti come isomorfismo
La funzione mappa è un isomorfismo di spazi vettoriali da $V$ in $Mat(m,1;K)$.

Dimostrazione: La mappa è biunivoca, quindi ammette inversa. Per proprietà
delle applicazioni lineare, ci basta dimostrare che la mappa è anche un omomorfismo
per dimostrare che è un isomorfismo: siano $v, \tilde{v} \in V$, allora
$v = \sum_{i=1}^n t_iv_i, \tilde{v} = \sum_{i=1}^n \tilde{t}_iv_i$. Allora
$$\begin{align}
    \phi_B(\alpha v + \beta \tilde{v}) &= \\
    &= \phi_B(\alpha \sum_{i=1}^n t_iv_i + \beta \sum_{i=1}^n \tilde{t}_iv_i) = \\
    &= \phi_B(\sum_{i=1}^n (\alpha t_i + \beta \tilde{t}_i)v_i = \\
    &= [\alpha t_i + \beta \tilde{t}_i] = \\
    &= \alpha [t_i] + \beta [\tilde{t_i}] = \\
    &= \alpha \phi_B(v) + \beta \phi_B(\tilde{v})
\end{align}$$
# 9 ottobre

## Basi
[...]

In uno spazio vettoriale non è detto che esista solo una base: possono esistere
infinite basi (tipo in $R^2$)

### Spazio generato e vettori generatori (Dispensa 4.27)
Prendiamo un insieme di vettori $U = \{u_1, \dotsc, u_n \} \subseteq V$, lo
spazio generato da $U$ ($Span(u_1. \dotsc, u_n) = L(u_1, \dotsc, u_n)$) è
l'insieme di tutti i vettori che possono ottenere combinando gli elementi
di $U$: $\{v \in V \mid v = t_1 * u_1 + \ldots + t_n u_n \}$. I vettori di $U$
si dicono generatori.

A differenza delle basi, i vettori generatori perdono il concetto di unicità:
lo stesso spazio può essere generato da più insiemi di generatori. ad esempio, 
uno spazio U può essere generato da $Span(u_1, u_2) = Span(u_1, u_2, u_3)$. 
Per questo i componenti di una base sono un sottoinsieme di tutti i possibili 
vettori generatori.

Lo spazio generato dai generatori è un sottospazio di $V$.

### Indipendenza lineare
Dato $U = \{u_1, \dotsc, u_n\}$ è linearmente indipendente se l'unica combinazione
lineare nulla è quella banale ($\underline{0} = 0*u_1 * \dotsm * 0*u_n$).

Di solito, quando abbiamo una dipendenza lineare, significa che uno dei generatori
in $U$ può essere scritto come combinazione lineare: questo significa che il numero
di generatori può essere ridotto e ottenere una base. Formalmente si può scrivere:
$U$ è dipendente se e solo se $\exists u_i \in Span(u_1, \dotsc, u_{i-1}, u_{i+1}, \dotsc, u_m)$.
Nota bene: se $\underline{0} \in U$, allora $U$ è sicuramente dipendente.

### Caratterizzazione di una base
$B = \{v_1, \dotsc, v_n\} \subseteq V$ è una base se e solo se:

- $B$ è un insieme di generatori
- $B$ è linearmente indipendente

Dimostrazione: la prima richiesta è ovvia: è equivalente a chiedere l'esistenza 
della decomposizione di ogni vettore rispetto alla base.  
La seconda condizione ci assicura invece che tale scomposizione è unica: 

- se $B$ è base, allora esiste solo un'unica combinazione lineare grazie al cui è 
    possibile ottenere il vettore nullo, e questa combinazione è quella banale. Per 
    definizione di indipendenza lineare, allora, $B$ è linearmente indipendente.

Poiché c'è $\iff$, bisogna dimostrarlo anche nell'altro senso: 

- Se $B$ è linearmente indipendente, allora avremo che
    $$\begin{align}
        v = \sum_{i=1}^n t_i v_i = \sum_{i=1}^n \tilde{t_i} v_i = \\
        \implies \underline{0} = v-v = \sum_{i=1} (t_i - \tilde{t_i})v_i = \\
        \implies t_i - \tilde{t_i} = 0 \forall i \implies t_i = \tilde{t_i}
    \end{align}$$
    $v$ è decomponibile in modo unico

### Spazi finitamente generati
$V$ è finitamente generato se $V = Span(v_1, \dotsc, v_m), m \in N$.

Esempi di spazi generati da un numero finito di vettori: $Mat(m,n;K), K^n, K[x]_n$
$K[x], R^R$ non sono spazi finitamente generati (esistono infiniti vettori generatori)

#### Algoritmo di eliminazione
Sia $U$ finito un insieme di generatori ($V = Span(U) \neq \{\underline{0}\}$),
allora esiste $B \subseteq V$ base di $V$. (Dimostrazione in dispensa).

L'algoritmo consiste nel cercare se uno dei componenti di $U$ è combinazione 
degli altri e rimuoverlo, mantenendo lo spazio generato invariato. L'algoritmo
termina quando si raggiunge un insieme $U$ linearmente indipendente, ossia
una base.

L'algoritmo termina sempre poiché $U$ è un insieme finito.

## Lemma di Steiniz (Dispensa 4.39)
Sia $V = Span(v_1, \dotsc v_m)$, ogni $U = \{u_1, \dotsc, u_q\}, q > m$ è
linearmente dipendente.

Dimostrazione: per $V= \{\underline{0}\}$ è banale. Supponiamo allora 
$V \neq \{\underline{0}\}$. Esisterà una base $B = \{v_1, \dotsc, v_n\}, n \leq m$
e ogni $u_j = \sum_{i=1}^n a_{ij} v_i$ per ogni $j = 1, \dotsc, q$. Considero
la combinazione lineare:
$$\begin{align}
    \underline{0} = \sum_{j=1}^q t_j u_j &= \sum_{j=1}^q t_j (\sum_{i=1}^n a_{ij} v_i) = \\
    &= \sum_{i=1}^n (\sum_{j=1}^q a_{ij}t_j)v_i
\end{align}$$
che è una combinazione lineare dei vettori all'interno di $B$. Poichè $B$ è linearmente
indipendente, allora $\sum_{j=1}^q a_{ij}t_i = 0 \forall i = 1, \dotsc, n$. Questo è,
però, un sistema lineare omogeneo in $t_j$ con $A = [a_{ij}] \in Mat(n,q;K)$.
Il rango di $A$ è limitato dal numero di righe e colonne: $r(A) \leq min(n,q)$
quindi $r(A) \leq n$. Allora avremo $\infty^{q-r(a)}$ soluzioni (per il teorema di 
Rouchè-Capelli). Di conseguenza esistono soluzioni non banali al sistema quindi
U è linearmente dipendente.

## Dimensione dello spazio vettoriale
La dimensione di uno spazio vettoriale è il numero di vettori di una sua base:
$$dim(V) = \begin{cases}
    0 &V = \{\underline{0}\} \\
    |B| &V \neq \{\underline{0}\} \\
    +\infty &V \text{ non finitamente generato}
\end{cases}$$

Esempi: 

- $Dim(K[x]_n) = n+1$ (parte da $x^0$)
- $Dim(K[x]) = +\infty$ (spazio non finitamente generato)

Sia $V$ uno spazio con dimensione non nulla. $U \subseteq V$ con $|U| = m$, 
allora:

- Se $U$ è indipendente, allora $m \leq n$. Se $m=n$ $U$ è una base.
- Se $V = Span(U)$, allora $m \geq n$. Se $m=n$ $U$ è una base.

### Teorema delle dimensioni (Dispensa 4.40)
Sia $V$ uno spazio finitamente generato, allora ogni base avrà lo stesso numero
di elementi.

Dimostrazione: siano $B_1, B_2$ due basi di $V$ con $n_1 = |B_1|, n_2 = |B_2|$.
Allora possiamo dire che:

- $B_1$ è insieme di generatori, $B_2$ è linearmente indipendente allora per il
    lemma di Steiniz $n_2 \leq n_1$
- $B_2$ è insieme di generatori, $B_1$ è linearmente indipendente allora per il
    lemma di Steiniz $n_1 \leq n_2$

Quindi $n_1 = n_2$

### Algoritmo di completamento
Sia V con dimensione non nulla e $U \subseteq V$. Allora $U$ può essere esteso
a una base di $V$.

## Dimensione e sottospazi
Sia $V$ uno spazio e $U$ un suo sottospazio:

- $dim(U) \leq dim(V)$
- $dim(U) = dim(V)$ se $U=V$

### Proprietà e formula di Grassmann
Sia $V$ spazio vettoriale e $U, W$ sottospazi, allora:

- $dim(U \cap W) \leq min(dim(U), dim(W))$
- $B_U \cup B_W = $ insieme dei generatori di $U+W$
- $dim(U + W) + dim(U \cup W) = dim(U) + dim(W)$: formula di Grassmann

