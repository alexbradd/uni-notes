# 1 ottobre 2019

## Matrici
[...]

### Matrici inverse
[...]

Se abbiamo $A^{-1}, B^{-1}$, allora $(AB)^1 = B^{-1}A^{-1}$. Generalizzato, 
possiamo affermare che $(A1 * ... * Ak)^{-1} = Ak^{-1} * ... * A1^{-1}$

#### Caratterizzazione delle matrici inverse (vedi dispensa)
Per dimostrare questo teorema andremo a passi:

1. Dimostrare l'unicità della matrice inversa
2. Operazioni di Gauss e le matrici elementari
3. Condizione sufficiente di invertibilità
4. Se $r(A) < n$ allora la matrice inversa non esiste

##### L'unicità della matrice inversa (Dispensa 3.43)
Sia una matrice $A \in Mat(m;n;K)$ con $B,C$ inverse da destra e sinistra. Allora
$B = C$ e sono uniche, quindi le indichiamo con $A^{-1}$.

##### Matrici elementari (Dispensa 3.45)
A ogni operazione di gauss corrisponde una matrice elementare (Esempi in 1.0):

- prendiamo $I_n$ e scambiamo le righe: otteniamo la matrice di permutazione $P(i,j)$
    (prima matrice elementare)
- prendiamo $I_n$ e moltiplico una riga per uno scalare: otteniamo la matrice di
    'prodotto' $T(i,t)$ con $t \in K*$
- prendiamo $I_n$ e moltiplico una riga per uno scalare e la sommo ad un'altra: 
    otteniamo la matrice di 'somma' $T(i,j,t)$ con $t \in K*$

Le operazioni di Gauss possono essere tradotte con le matrici elementari: data
$A \in Mat(m,n;K)$, le mosse elementari sono:

- $A \leftrightarrow B = P(i,j)A$
- $A \leftrightarrow B = T(i,t)A$
- $A \leftrightarrow B = T(i,j,t)A$

La riduzione a scala, quindi, consiste nel moltiplicare una matrice con determinate 
matrici elementari in modo da ottenere una matrice a scala (Dispensa 3.48).

Le inverse delle matrici elementari esistono e sono a loro volta matrici 
elementari:

- $P(i,j)^{-1} = P(i,j)$
- $T(i;r)^{-1} ) T(i;\frac{1}{t})$
- $T(i,j;t)^{-1} ) T(i,j;\frac{1}{t})$

Ciò ci permette di affermare che l'algoritmo di riduzione è sempre invertibile.


Se $S = t_1 ... E_k A \implies A = E_k^{-1} ... E_1^{-1}S$. Ossia ogni matrice
è decomponibile in un prodotto di $K$ matrici elementari e di una matrice a
scala.

**Dimostrazione**: $(E_1 ... E_k)^{-1} = E_1^{-1} ... E_k^{-1}$ per proprietà
dimostrata precedentemente. Quindi
$E_1^{-1} ... E_k^{-1} S = (E_1 ... E_k)^{-1} (E_1 ... E_k)A = A$ per ipotesi.

##### Condizione sufficiente di invertibilità
Se $A \in Mat(m;K)$, esiste la riduzione da $A$ alla matrice $I_n$ se e solo
se $r(A) = n$ (Disp 3.5.2). Di conseguenza si può affermare che $A$ è prodotto
di matrici elementari se e solo se $r(A) = n$ e vale anche il contrario.

##### Collegamento tra il rango e la matrice inversa (Dispensa 3.59)
Se $r(A) < n$ allora non esistono inverse a sinistra e a destra.

### Algoritmo di Gauss-Jordan
Supponiamo che $A \in Mat(m,n;K)$ sia invertibile. Allora:

- esiste l'inversa che può scrivere come $A^{-1} = E_1 \ldots E_k$
- se faccio $A \cdot [A|I_n]$ allora $A \cdot [A|I_n] = [I_n|A^{-1}]$

Questa forma di inversione di una matrice è nota come algoritmo di Gauss-Jordan.

### La traccia (Disp 3.63)
Partiti da una matrice i cui elementi sono $A[a_{ij}] \in Mat(m,n;K)$, la traccia
è la somma degli elementi sulla diagonale.

#### Proprietà
- $Tr(s*A + t*B) = s*Tr(A) + t*Tr(B)$
- $Tr(A^T) = Tr(A)$
- $Tr(AB) = Tr(BA)$

### Sottomatrice (Disp 3.65)
Presa una matrice $A$, una sottomatrice di $A$ è un'altra matrice ottenuta 
eliminando alcune righe e alcune colonne. Si indica con $A_{\widehat{i...j}\widehat{i...j}}$

Il determinante di una sottomatrice è detto minore.

### Il determinante (Disp 3.67)
Presa una matrice $A \in Mat(m,n;K)$ il determinante $|A| = det(A)$ è un numero
appartenente a $K$. Si calcola come:

- per $n = 1, det([a_{11}]) = a_{11}$
- per $n > 1, det(A) = \sum_{j=1}^n a_{1j} * C_{1j}$ dove $C$ è il complemento
    algebrico ($C_{ij} = (-1)^{i+j} * det(A_{\hat{j}\hat{j}}$)

Esempi in 1.1. Sempre in 1.1 il determinante di una matrice di ordine 2.

Nota bene: $det(I_n) = 1$

# 2 ottobre 2019

## Matrici
[...]

### Il determinante
[...]

#### Legame tra determinante e operazioni elementari (Disp 3.72, 3.77)
Per ognuna operazione vediamo come modifica il determinante:

- Permutazione         : $|B| = -|A|$
- Prodotto per scalare : $|B| = t*|A|$
- Somma tra righe      : $|B| = |A|$

##### Dimostrazione
Si può dimostrare in generale, ma lo faremo solo per le matrici di ordine 2:

- Permutazione:

        a b     |A|= ad-bc
        c d

        c d     |B|= bc-ad = -|A|
        a b

- Scalare:

        a b     |A|= ad-bc
        c d

        ta tb     |B|= tad-tbc = t*|A|
        c  d

- Somma tra righe:

        a b             |A|= ad-bc
        c d

        ta      tb      |B|= a(d+tb)-b(c+ta) = |A|
        c+ta  d+tb

#### Il determinante e l'algebra delle matrici
- $|A+B| \neq |A| + |B|$
- $t*|A| = t^n * |A|$
- è legata alla moltiplicazione dal teorema di Binet
- la trasposizione di $A$ non varia il determinante
- $|A^{-1}| = |A|^{-1}$ (conseguenza del teorema di Binet)

#### Teorema di Binet (Disp 3.83)
Si ha che $|A*B| = |A|*|B|$. Il determinante, quindi, commuta con la moltiplicazione,
quindi è una funzione moltiplicativa. (Dimostrazione nella dispensa).

Usando Binet si può anche dimostrare che:
$$AA^{-1} = |A||A^{-1}| \implies |AA^{-1}| = |I_m| \implies |A||A^{-1}| = 1$$

#### Gruppo generale lineare e gruppo speciale lineare
Introduciamo due gruppi:

- Il gruppo generale lineare: il sottoinsieme di tutte le matrici invertibili
    rispetto alla moltiplicazione matriciale ($GL(n;K)$)
- Il gruppo speciale lineare: è l'insieme delle matrici con determinante 1, un
    sottogruppo di $GL(n;K)$ ($SL(n;K)$)

#### Il determinante come omomorfismo
Definendo il determinante come:
$$det: GL(n;K) \to (K^*, *)$$
si può affermare che il determinante è un omomorfismo suriettivo di gruppi.

##### Dimostrazione
Se $A,B\in GL(m;K)$ allora $|AB| = |A||B| \neq 0$ quindi $AB \in GL(n;K)$
Dimostrato che $GL(n;K)$ è un gruppo e che il determinante è ben definito, allora 
per il teorema di Binet, il prodotto commuta tra i due gruppi.

#### Gli sviluppi di Laplace per righe
Per calcolare il determinante, non bisogna partire per forza dalla prima
riga, ma da qualsiasi riga:
$$det(A) = \sum_{j=0}^n a_{ij} * C_{ij} \quad \forall 1 \leq i \leq n$$

##### Dimostrazione
Sia $B$ la matrice in cui come prima riga è la matrice $i$-esima di $A$
(spostare la riga $i$ per prima):
$$\begin{bmatrix}
    A_{R(i)} \\
    A_{R(1)} \\ 
    A_{R(2)} \\
    \vdots \\
    A_{R(i-1)} \\
    A_{R(i+1)} \\
    \vdots \\
    A_{R(n)} \\
\end{bmatrix}$$

Calcoliamo il determinante di questa matrice:
$$\begin{align}
    det(B) &= \sum_{j=0}^n b_{1j} (-1)^{1+j} |B_{\hat{1}\hat{j}} \\
    &= \sum_{j=0}^n a_{ij} (-1)^{1+j} |A_{\hat{i}{j}}|
\end{align}$$

La matrice $B$ è ottenuta attraverso $i-1$ permutazioni, quindi avremo che:
$$|B| = (-1)^{i-1} |A|$$

Quindi il determinante di $A$ sarà:
$$\begin{align}
    |A| &= \frac{1}{(-1)^{i-1}} \sum_{j=0}^n a_{ij} (-1)^{1+j} |A_{\hat{i}{j}}| \\
    &= \sum_{j=0}^n a_{ij} (-1)^{2-i+j} |A_{\hat{i}{j}}| \\
    &= \sum_{j=0}^n a_{ij} (-1)^2 (-1)^{i+j} |A_{\hat{i}{j}}| \\
    &= \sum_{j=0}^n a_{ij} (-1)^{i+j} |A_{\hat{i}{j}}| \\
    &= \sum_{j=0}^n a_{ij} C_{ij}
\end{align}$$

#### Gli sviluppi di Laplace per colonne
Per calcolare il determinante, non bisogna partire per forza dalla prima
riga, ma da qualsiasi colonna:
$$|A| = \sum_{i=1}^n a_{ij} C_{ij} \quad \forall 1 \leq j \leq n$$

##### Dimostrazione
Visto che $|A^T| = |A|$, allora questo teorema è una conseguenza dello sviluppo
per righe.

#### Conseguenze di Laplace
- Il determinante è nullo se una riga o una colonna di $A$ è nulla
- Il determinante è nullo se due righe o due colonne sono uguali $A$

#### Cosa succede quando riduco a scala?
Prendiamo $S$ riduzione a scala di $A$ ottenuta tramite le $k$ permutazioni di
righe ed $r$ moltiplicazioni di righe per gli scalari $t_1, \dotsc, t_r$. I
determinante di $S$ sarà:
$$|S| = (-1)^k * t_1 * \dotsm * t_r * |A|$$

Usando queste proprietà posso calcolare il determinante di una matrice partendo
da quello della sua riduzione a scala che è molto più facile da calcolare. Possiamo
anche riassumere tutto nella formula:
$$|A| = \frac{(-1)^k}{t_1 * \dotsm * t_r} * |S|$$

#### Il determinante di una matrice triangolare
Sia $A$ una matrice triangolare, allora $|A| = \prod_{i=1}^n a_{ii}$. Questo metodo
è una conseguenza degli sviluppi di Laplace.

Questo metodo può anche essere usato per calcolare il determinante delle matrici
a scala, in quando matrici triangolari alte.

#### Legame tra determinante e rango
Il determinante è legato al rango di una matrice da un teorema (Disp 3.69):
$$det(A) \neq 0 \iff r(A) = n$$

Questo comporta che:

- Data un sistema lineare $[A|B]$, questo sistema ha soluzioni uniche se e solo
    se $|A| \neq 0$
- $A$ è invertibile se e solo se $|A| \neq 0$

##### Dimostrazione
Abbiamo che:
$$ |A| = \frac{(-1)^k}{t_1 * \dotsm * t_r} |S| \neq 0 \iff |S| \neq 0 $$

Allora, affinché $|S|$ non sia 0, la diagonale di $S$ non deve contenere 0 e quindi
$r(S) = n$ 

### Matrice aggiunta (Disp 3.88)
Prendiami $A \in Mat(n;K)$ con $C_{ij}$ i suoi complimenti algebrici. La matrice
aggiunta $A^* \in Mat(n;K)$ è $(A^*) = C_{ji} = C_{ij}^T$

#### Teorema di Laplace (3.89)
Il prodotto $AA^* = A^* A = |A| * I_n$. Questo vale sia se il determinante di A
è nullo o no.

#### Formula chiusa per l'inversione di una matrice
Dal teorema di Laplace di può dedurre che:
$$A^{-1} = \frac{1}{|A|}A^*$$

Ciò ribadisce che una matrice è invertibile se e solo se il suo determinante non
è nullo.

# 4 ottobre 2019

## Strutture algebriche
...

### Sottogruppi
Sono i sottoinsiemi di un gruppo. Sono a loro volta una struttura algebrica

### Studio di una struttura algebrica
Prima si studiano gli insiemi e le operazioni della struttura. Poi gli
omomorfismi e isomorfismi. Lo studio di questi fornisce le proprietà della
struttura. Poi si studiano eventuali sottogruppi.

## Spazi vettoriali
Sono una struttura algebrica. Il loro studio rientra in generale nello studio
dell'algebra.

### Definizione
$V$ è un insieme di elementi $v$ detti vettori, $K$ è un campo. Oltre alle
operazioni definite nel campo definisco altre due operazioni:

- $+: V \times X \to V; (v_1, v_2) \mapsto v_1 + v_2$ (somma tra vettori)
- $*: K \times X \to V; (t, v_1) \mapsto t * v_1$ (prodotto per scalare)

La struttura formata da $(V, K, +, *)$ è detta spazio vettoriale se:

- $(V, +)$ deve essere un gruppo abeliano ($v_0$ è il neutro, $-v$ è l'inverso)
- valga la proprietà distributiva tra $+$ e $*$
- valga la proprietà distributiva tra $+$ del campo e $*$ vettoriale
- valga l'omogeneità tra i prodotti di $V$ e $K$
- $1 * v = v$ (proprietà di normalizzazione)

Un esempio di spazio vettoriale è $(Mat(m,n;K), K, +, *)$, oppure $(R^2, R, +, *)$
(i vettori come li intendiamo), $(K[x], K, +, *)$, $(R^R, R, +, *)$.

### Proprietà elementari
Proprietà che diamo per scontato a causa della abitudine con le operazioni
fondamentali, ma anch'esse vanno dimostrate.

- $\underline{u} + \underline{v} = \underline{u} + \underline{w} \iff w = v$
- $t * \underline{v} = \underline{0} \iff t = \underline{0} \wee v = \underline{0}$
- $\underline{0}$ e $-\underline{v}$ sono unici. In particolare 
  $-\underline{v} = (-1)*\underline{v}$

Dimostrazione della terza proprietà: siano $e_1, e_2$ elementi neutri della somma,
allora $v + e_1 = v = v + e_2$ quindi $e_1 = e_2 = \underline{0}$

Siano $v', v''$ inversi di $v$, allora $v' + v = \underline{0} = v'' + v$ quindi
$v' = v''$.

Dimostriamo che $-v = (-1)*v)$: $v + (-1)*v = (1*v) + (-1*v) = (1-1) * v = 0 * v = \underline{0}$

### Omomorfismi
Dati $(V, K, +, *)$, $(W, K, +, *)$ due spazi vettoriali, una funzione $f: V \to W$
è detta applicazione lineare (omomorfismo di spazi vettoriali) se:

- $f(v +_v \tilde{v}) = f(v) +_w f(\tilde{v}) \forall v, \tilde{v} \in V$
- $f(t *_v \tilde{v}) = t *_w f(\tilde{v}) \forall v, \tilde{v} \in V$

L'insieme di tutte le applicazioni lineari da $V$ in $W$ è $Hom(V,W)$

Se un'applicazione lineare è invertibile, allora esso sarà un isomorfismo

#### Proprietà
$f: V \to W$ è applicazione lineare se e solo se $f(t_1v_1 + t_2v_2) = t_1f(v_1) + t_2f(v_2)$

Dimostrazione: (verso 1) sia $f$ lineare, allora 
$f(t_1v_1 + t_2v_2) = f(t_1v_1) + f(t_2v_2) = t_1f(v_1) + t_2f(v_2)$

(verso 2) Esercizio

### Sottostrutture (sottospazi vettoriali)
Dato uno spazio vettoriale $(V, K, +, *)$ con $U \subsetqe V$ sottoinsieme. Se
$(U, K, +, *)$ si dive sottospazio vettoriale se a sua volta è uno spazio 
vettoriale.

In un sottospazio le proprietà delle operazioni sono ereditate dallo spazio
vettoriale che lo contiene.

#### Caratterizzazione dei sottospazi
Dato $U \subseteq V$, $U$ è un sottospazio se e solo se:

- $U$ è chiuso rispetto alla somma
- $U$ è chiuso rispetto al prodotto
- $\underline{0} \in U$ (impone che il sottospazio non sia vuoto (vedi verso 2))

Dimostrazione: (verso 1) se $U$ è sottospazio, le proprietà sono soddisfatte per
definizione.

(verso 2) se le prime due sono vere, per ipotesi dobbiamo solo verificare che 
$\underline{0} \in U$ e $-v \in U$:

- dato un vettore $u \in U$, se faccio $0u = \underline{0}$ e, poiché $U$ è chiuso
    rispetto al prodotto, allora $\underline{0} \in U$
- dato un vettore $u \in U$, se faccio $-1 * u = -u$ e poiché $U$ è chiuso rispetto
    al prodotto, allora $-v \in U$

Questi due passaggi sono necessari per verificare che $U$ non sia vuoto: infatti,
per poter fare un prodotto tra due vettori, devi contenerne almeno uno.

