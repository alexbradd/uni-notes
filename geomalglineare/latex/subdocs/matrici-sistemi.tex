\section{Matrici}
Le matrici sono uno strumento fondamentale per fare i conti in matematica.

Dati due insiemi \hl{$M = {1, \ldots, m}$ e $N = {1, \ldots, n}$, una matrice di
ordine $(m, n)$ ad elementi nel campo $K$ è una funzione definita come}:
\[
    \begin{array}{cccc}
        A: &M \times N &\to &K \\
        &(i,j) &\mapsto &a_{ij}
    \end{array}
\]
L'insieme di \hl{tutte le matrici di ordine $(m,n)$ su $K$ viene indicato con
$Mat(m,n;K)$}.

\paragraph{Matrici particolari} La matrice nulla è indicata con $0_{mn}$. La
matrice identità $I_{mn}$ è, invece, una matrice del tipo:
\[
    \begin{array}{cccc}
        I_{mn}: & M \times N & \to & K \\
        & (m,n) & \mapsto & \Delta
    \end{array} \text{ con } \Delta{} = 1 \text{ se } i = j
\]

\paragraph{Rappresentazione} Una matrice può essere pensata come una tabella di
numeri di $m$ righe ed $n$ colonne:
\[
    A =
    \bordermatrix{~ & 1 & \cdots & n \cr
                  1 & a_{11} & \cdots & a_{1n} \cr
                  \vdots & \vdots & \ddots & \vdots \cr
                  m & a_{m1} & \cdots & a_{mn} \cr }
    \in Mat(m,n;K)
\]
Una matrice si può anche indicare con la notazione $[a_{ij}]$.

\paragraph{Il rango di una matrice} \hl{Il rango $r(A)$ di una matrice $A$ è il
rango di una matrice a scala $S$ ottenuta tramite riduzione di Gauss di $A$}.

\subsection{Le matrici quadrate}
Data una matrice $A \in Mat(m,n;\mathbb{K})$, essa è quadrata se \hl{$m = n$}.
Una matrice quadrata si \hl{indica con $Mat(m,\mathbb{K})$}. Solo le matrici
quadrate sono matrici invertibili (teorema che non abbiamo fatto).

\paragraph{Matrici quadrate particolari} Se $a_{ij} = 0$ per
\begin{itemize}
    \item $i>j$ è triangolare alta
    \item $i \leq j$ è strettamente triangolare alta
    \item $i<j$ è triangolare bassa
    \item $i \geq j$ è strettamente triangolare bassa
    \item $i \neq j$ è diagonale
\end{itemize}
Inoltre se $a_{ij} = a_{ji}$ la matrice è simmetrica, invece se $a_{ij} = a_{ji}$
viene detta antisimmetrica.

\subsection{Matrice invertibile}
Prese tre matrici quadrate $A, B, C$ allora:
\begin{itemize}
    \item $B$ si dice \hl{inversa sinistra se $BA = I_{n}$}
    \item $C$ si dice \hl{inversa destra se $AC = I_{n}$}
\end{itemize}
\hl{$A$ si dice invertibile se $B = C$}. La matrice inversa di $A$ è unica e
si indica con $A^{-1}$.

\paragraph{Teorema di caratterizzazione delle matrici invertibili} Possiamo
affermare che \hl{$\exists A^{-1}$} se e solo se:
\begin{itemize}
    \item \hl{$r(A) = n$}
    \item Esiste l'inversa sinistra o la destra
\end{itemize}

\subsection{La matrice trasposta}
Data una matrice $A$, la matrice trasposta è \hl{un'altra matrice $A^T$ che si
ottiene trasformando tutte le righe in colonne e viceversa}:
\[
    A \in Mat(m,n;K); A^T \in Mat(n,m,K)
\]

\paragraph{Proprietà} La matrice trasposta gode di queste proprietà:
\begin{itemize}
    \item $(A^T)^T = A$
    \item $(tA + tB)^T = tA^T + tB^T$
    \item $(AB)^T = B^T \cdot A^T$
\end{itemize}

\subsection{Operazioni con le matrici}
\begin{description}
    \item[Somma] \'E un'operazione binaria interna. Somma gli elementi uno a uno:
        \[
            \begin{array}{cccc}
                +: & Mat(m,n;K) \times Mat(m,n;K) & \to &Mat(m,n;K) \\
                & ([a_{ij}], [b_{ij}]) &\mapsto  & [a_{ij} + b_{ij}]
            \end{array}
        \]
    \item[Prodotto con scalare] \'E un'operazione binaria esterna:
        \[
            \begin{array}{cccc}
                \cdot: & K \times Mat(m,n;K) & \to &Mat(m,n;K) \\
                & (t, [a_{ij}]) &\mapsto  & [t \cdot a_{ij}]
            \end{array}
        \]
    \item[Prodotto matriciale] \'E un'operazione binaria esterna:
        \[
            \begin{array}{cccc}
                \ast: & Mat(m,p;K) \times Mat(p,n;K) & \to &Mat(m,n;K) \\
                & ([a_{ij}], [b_{ij}]) &\mapsto  & [\sum_{k=0}^p a_{ik}b_{kj}]
            \end{array}
        \]
        \hl{Osserva: il numero di colonne della prima deve essere uguale al numero
            di righe della seconda!}
\end{description}

\paragraph{Proprietà della somma} La struttura $(Mat(m,n:\mathbb{R}), +)$ è un
gruppo abeliano quindi:
\begin{itemize}
    \item \hl{\'E commutativa}: $A + B = B + A$
    \item \hl{\'E associativa}: $(A + B) + C = A + (B + C)$
    \item \hl{Esiste l'elemento neutro} $e = 0_{mn}$
    \item \hl{Esiste l'inverso}: $A + (-A) = 0_{mn}$
\end{itemize}

\paragraph{Proprietà del prodotto con scalare}
\begin{itemize}
    \item \hl{\'E distributiva con la somma}: $k (A + B) = kA + kB$
    \item \hl{\'E distributiva con la somma in $K$}: $(j + k)A = jA + kA$
    \item \hl{\'E omogenea rispetto alla moltiplicazione in $K$}:
        $(jk)A = j(kA)$
    \item \hl{Esiste la normalizzazione} $A = 1 \cdot A$
\end{itemize}

\paragraph{Proprietà del prodotto matriciale}
\begin{itemize}
    \item \hl{Non vale la proprietà commutativa}
    \item \hl{Non esiste l'annullamento}
    \item \hl{L'elemento neutro è $I_{m}A_{mn} = A, A_{mn}I_{n} = A$}
    \item \hl{Non esiste l'inverso}
    \item \hl{\'E associativo}
    \item \hl{\'E distributivo con la somma}: $A(B+C) = (AB) + (AC)$
    \item \hl{\'E omogenea con l'altro prodotto}: $t(AB) = (tA)B = A(tB)$
\end{itemize}

\subsection{Pivot e matrice a scala}
\paragraph{Pivot} Un pivot $Pi$ è \hl{il primo elemento non nullo della riga $i$}
della matrice.

\paragraph{Matrice a scala} Una matrice nella quale \hl{il pivot della prima riga
compare prima del pivot della seconda riga, che a sua volta compare prima del
pivot della terza riga e così andare}, con le eventuali righe nulle per ultime, si
dice matrice a scala.
\[
    A =
    \begin{bmatrix}
        P1 & *  & *  & * \\
        0  & P2 & *  & * \\
        0  & 0  & P3 & * \\
        0  & 0  & 0  & P4 \\
        & & 0_{m-r, n} &
    \end{bmatrix} \in Mat(m,n;K)
\]

\paragraph{Il rango di una matrice a scala} \hl{Il rango $r(S)$ di una matrice
a scala $S$ è il numero di pivot in $S$}.

\subsection{Eliminazione di Gauss}
Il metodo di eliminazione di Gauss ci permette \hl{di ridurre qualsiasi matrice
in una nuova matrice a scala tramite alcune operazioni }. Esisteranno \hl{diverse
riduzioni, ma tutte hanno lo stesso rango}.

\paragraph{Le operazioni di Gauss}
\begin{description}
    \item[Permutazione] \hl{Scambio due righe tra di loro}
    \item[Moltiplicazione per uno scalare non nullo] \hl{Moltiplico tutti gli elementi
        di una riga per un numero} diverso da $0$
    \item[Somma tra righe] \hl{Sommo uno a uno gli elementi di due righe e la riga
        risultante la inserisco al posto di uno dei due addendi}:
        $A_{R(i)} \to A_{R(i)} + tA_{R(j)} \text{ con } i \neq j$
\end{description}

\section{Sistemi lineari}
Un sistema lineare è un insieme di espressioni che hanno questa forma:
\[
    \begin{cases}
        a_{11}x_1 + \cdots + a_{1n}x_n = b_1 \\
        \cdots + \cdots + \cdots = \cdots \\
        a_{m1}x_1 + \cdots + a_{mn}a_n = b_m
    \end{cases}
\]
Dove:
\begin{itemize}
    \item $a_{ij}, b_i \in K$
    \item $x_n$ sono incognite
    \item $1 \leq i \leq m, 1 \leq j \leq n$
\end{itemize}

\hl{Un sistema lineare può anche essere scritto come un'equazione matriciale:}
\begin{align*}
    & A =
    \begin{bmatrix}
        a_{11} & \cdots & a_{1n} \\
        \vdots & \ddots & \vdots \\
        a_{m1} & \cdots & a_{mn}
    \end{bmatrix}, B =
    \begin{bmatrix}
        b_1 \\ \vdots \\ b_m
    \end{bmatrix}, X =
    \begin{bmatrix}
        x_1 \\ \vdots \\ x_n
    \end{bmatrix} \\
    & AX = B
\end{align*}

\hl{La matrice $[A|B]$ è detta la matrice completa del sistema. $[A|0]$ è detta
matrice del sistema omogeneo associato.}

\subsection{Sistema lineare omogeneo}
Un sistema lineare omogeneo è un \hl{sistema lineare del tipo $[A|0_{m1}]$} e avrà
sempre \hl{almeno una soluzione, di cui una $X=0_{n1}$}. La soluzione del sistema
lineare omogeneo \hl{è detta soluzione generale}. Un \hl{sistema lineare omogeneo e
il corrispettivo sistema normale condividono la soluzione generale}:
\begin{align*}
    \begin{cases}
        x + y + z = 0 \\
        z = 0
    \end{cases} & X_0 =
    \begin{bmatrix}
        -t \\
        t \\
        0
    \end{bmatrix} \\
    \begin{cases}
        x + y + z = 1 \\
        z = 0
    \end{cases} & X =
    \begin{bmatrix}
        1-t \\
        t \\
        0
    \end{bmatrix} =
    \begin{bmatrix}
        1 \\
        0 \\
        0
    \end{bmatrix} +
    \begin{bmatrix}
        -t \\
        t \\
        0
    \end{bmatrix} =
    \begin{bmatrix}
        1 \\
        0 \\
        0
    \end{bmatrix} + X_0
\end{align*}
Questo ci permette di enunciare il \hl{teorema di costruzione delle soluzioni}.

\paragraph{Nucleo di una matrice} In nucleo di $A = Ker(A)$ è \hl{l'insieme delle
soluzioni del sistema lineare omogeneo associato}
($Ker(A) = \{x \in Mat(m,n;K) \mid AX = 0\} \neq \emptyset$)

\subsection{Teorema di struttura delle soluzioni}
Sia $[A|B]$ risolvibile, \hl{la soluzione del sistema sarà la soluzione particolare di
$[A|B]$ sommata alla soluzione generale del sistema omogeneo associato $[A|0_{m1}]$}

\subsection{Forma chiusa per il calcolo della soluzione di un sistema lineare}
La forma chiusa per \hl{la risoluzione di un generico sistema lineare $AX = B$ è}
\[
    A^{-1}AX = BA^{-1}
\]
Questa forma chiusa è il \hl{teorema di Cramer}

\paragraph{Teorema di Cramer} Se una matrice $A$ è invertibile, allora il
sistema lineare $[A|B]$ associato avrà soluzione $X = BA^{-1}$.

\subparagraph{Dimostrazione} Se $A$ è invertibile, allora $r(A) = n$. Per il
teorema di Rouché-Capelli, la soluzione del sistema associato ad $A$ sarà unica.
Possiamo allora scrivere:
\[
    AX = A(A^{-1}B) = (AA^{-1})B = I_n B = B
\]
Confermando il fatto che $X = BA^{-1}$ è soluzione del sistema.

\subsection{Equivalenza dei sistemi lineari}
Sia $[A|B]$ un generico sistema lineare e $[S|B]$ una sua riduzione a scala.
\hl{I due sistemi avranno le stesse soluzioni}.

\paragraph{Dimostrazione} Per dimostrare il teorema verifichiamo che ogni
operazione di Gauss non modifichi le soluzioni:
\begin{itemize}
    \item Permutazione: modifica solo l'ordine delle equazioni, ma non le soluzioni
    \item Moltiplicazione per scalare: se lo scalare $t \neq 0$ le soluzioni non
        cambiano in quanto
        \[\not{t}(a_{i1}x_1 + \cdots + a_{ij}) = \not{t}(b_i)\]
    \item Somma tra righe: Prendiamo due sistemi così definiti:
        \[
            \begin{cases}
                a_{i1}x_1 + \cdots + a_{in} - b_i = 0 \\
                a_{j1}x_1 + \cdots + a_{jn} - b_j = 0
            \end{cases}
            \begin{cases}
                (a_{i1}x_1 + \cdots + a_{in} - b_i) + t(a_{j1}x_1 + \cdots + a_{jn} - b_j) = 0 \\
                a_{j1}x_1 + \cdots + a_{jn} - b_j = 0
            \end{cases}
        \]
        Siano $(x_1, \ldots, x_n)$ le soluzioni del primo sistema. Allora
        \begin{align*}
            &a_{i1}x_1 + \cdots + a_{in} - b_i = 0 \text{ per ipotesi}\\
            &a_{j1}x_1 + \cdots + a_{jn} - b_j = 0 \text{ per ipotesi}\\
            &(a_{i1}x_1 + \cdots + a_{in} - b_i) + t(a_{j1}x_1 + \cdots + a_{jn} - b_j) = 0
        \end{align*}
        Siano $(x_1, \ldots, x_n)$ le soluzioni anche per il secondo sistema.
        Allora:
        \begin{align*}
            &a_{i1}x_1 + \cdots + a_{in} - b_i = 0 \text{ per soluzione del primo sistema}\\
            &a_{j1}x_1 + \cdots + a_{jn} - b_j = 0 \text{ per ipotesi}\\
            &(a_{i1}x_1 + \cdots + a_{in} - b_i) + t(a_{j1}x_1 + \cdots + a_{jn} - b_j) = 0
        \end{align*}
        Le stesse $(x_1, \ldots, x_n)$ risolvono entrambi i sistemi
\end{itemize}
Le operazioni gaussiane, quindi, non modificano le soluzioni.

\subsection{Algoritmo di Gauss per la risoluzione dei sistemi lineari}
L'algoritmo di Gauss per risolvere i sistemi lineari \hl{si basa sull'equivalenza
dei sistemi lineari}. Consiste nella \hl{riduzione a scala della matrice completa
del sistema lineare}.

\subsection{Teorema di Rouché-Capelli}
Il teorema di Rouché-Capelli ci \hl{permette di capire la risolvibilità di un sistema
lineare in base al rango della sua matrice associata}
($r(A) \leq r([A|B]) \leq r([A]) + 1$). Sia un sistema che ha come matrice dei
coefficienti $[A]$ e matrice completa $[A|B]$. Se:
\begin{itemize}
    \item \hl{$r([A|B]) > r(A)$: il sistema sarà impossibile} Il sistema si dice
        \hl{sovradeterminato}
    \item \hl{$r([A|B]) = r(A) = n$: il sistema avrà un'unica soluzione}
    \item \hl{$r([A|B]) = r(A) < n$: il sistema avrà infinite soluzioni}
\end{itemize}

Consideriamo il sistema associato ad $[A|B] \in Mat(m,n;K)$, per il teorema
sopra enunciato:
\begin{itemize}
    \item La soluzione non esiste se $r([A|B]) > r(A)$
    \item La soluzione esiste unica se $r([A|B]) = r(A) = n$ ($\infty^{0} = 1$
        soluzioni)
    \item Esistono infinite soluzioni dipendenti da $n-r(A)$ parametri se e
        solo se $r([A|B]) = r(A) < n$ ($\infty^{n-r}$ soluzioni)
\end{itemize}
