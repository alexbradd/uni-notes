\section{Matrici}
Le matrici sono uno strumento fondamentale per fare i conti in matematica.

Dati due insiemi \hl{$M = {1, \ldots, m}$ e $N = {1, \ldots, n}$, una matrice di
ordine $(m, n)$ ad elementi nel campo $K$ è una funzione definita come}:
\[
    \begin{array}{cccc}
        A: &M \times N &\to &K \\
        &(i,j) &\mapsto &a_{ij}
    \end{array}
\]
L'insieme di \hl{tutte le matrici di ordine $(m,n)$ su $K$ viene indicato con
$Mat(m,n;K)$}.

\paragraph{Matrici particolari} La matrice nulla è indicata con $0_{mn}$. La
matrice identità $I_{mn}$ è, invece, una matrice del tipo:
\[
    \begin{array}{cccc}
        I_{mn}: & M \times N & \to & K \\
        & (m,n) & \mapsto & \Delta
    \end{array} \text{ con } \Delta{} = 1 \text{ se } i = j
\]

\paragraph{Rappresentazione} Una matrice può essere pensata come una tabella di
numeri di $m$ righe ed $n$ colonne:
\[
    A =
    \bordermatrix{~ & 1 & \cdots & n \cr
                  1 & a_{11} & \cdots & a_{1n} \cr
                  \vdots & \vdots & \ddots & \vdots \cr
                  m & a_{m1} & \cdots & a_{mn} \cr }
    \in Mat(m,n;K)
\]
Una matrice si può anche indicare con la notazione $[a_{ij}]$.

\subsection{Sottomatrice}
Presa una matrice $A$, \hl{una sottomatrice di $A$ è un'altra matrice otetnuta
eliminando alcune righe e colonne}. Si indica con 
\hl{$A_{\widehat{i,\dotsc,j}\widehat{i,\dotsc,j}}$} dove $i,\dotsc,j$ sono le 
colonne/righe rimosse.

\subsection{Operazioni con le matrici}
\begin{description}
    \item[Somma] \'E un'operazione binaria interna. Somma gli elementi uno a uno:
        \[
            \begin{array}{cccc}
                +: & Mat(m,n;K) \times Mat(m,n;K) & \to &Mat(m,n;K) \\
                & ([a_{ij}], [b_{ij}]) &\mapsto  & [a_{ij} + b_{ij}]
            \end{array}
        \]
    \item[Prodotto con scalare] \'E un'operazione binaria esterna:
        \[
            \begin{array}{cccc}
                \cdot: & K \times Mat(m,n;K) & \to &Mat(m,n;K) \\
                & (t, [a_{ij}]) &\mapsto  & [t \cdot a_{ij}]
            \end{array}
        \]
    \item[Prodotto matriciale] \'E un'operazione binaria esterna:
        \[
            \begin{array}{cccc}
                \ast: & Mat(m,p;K) \times Mat(p,n;K) & \to &Mat(m,n;K) \\
                & ([a_{ij}], [b_{ij}]) &\mapsto  & [\sum_{k=0}^p a_{ik}b_{kj}]
            \end{array}
        \]
        \hl{Osserva: il numero di colonne della prima deve essere uguale al numero
            di righe della seconda!}
\end{description}

\paragraph{Proprietà della somma} La struttura $(Mat(m,n:\mathbb{R}), +)$ è un
gruppo abeliano quindi:
\begin{itemize}
    \item \hl{\'E commutativa}: $A + B = B + A$
    \item \hl{\'E associativa}: $(A + B) + C = A + (B + C)$
    \item \hl{Esiste l'elemento neutro} $e = 0_{mn}$
    \item \hl{Esiste l'inverso}: $A + (-A) = 0_{mn}$
\end{itemize}

\paragraph{Proprietà del prodotto con scalare}
\begin{itemize}
    \item \hl{\'E distributiva con la somma}: $k (A + B) = kA + kB$
    \item \hl{\'E distributiva con la somma in $K$}: $(j + k)A = jA + kA$
    \item \hl{\'E omogenea rispetto alla moltiplicazione in $K$}:
        $(jk)A = j(kA)$
    \item \hl{Esiste la normalizzazione} $A = 1 \cdot A$
\end{itemize}

\paragraph{Proprietà del prodotto matriciale}
\begin{itemize}
    \item \hl{Non vale la proprietà commutativa}
    \item \hl{Non esiste l'annullamento}
    \item \hl{L'elemento neutro è $I_{m}A_{mn} = A, A_{mn}I_{n} = A$}
    \item \hl{Non esiste l'inverso}
    \item \hl{\'E associativo}
    \item \hl{\'E distributivo con la somma}: $A(B+C) = (AB) + (AC)$
    \item \hl{\'E omogenea con l'altro prodotto}: $t(AB) = (tA)B = A(tB)$
\end{itemize}

\subsection{Le matrici quadrate}
Data una matrice $A \in Mat(m,n;\mathbb{K})$, essa è quadrata se \hl{$m = n$}.
Una matrice quadrata si \hl{indica con $Mat(m,\mathbb{K})$}. Solo le matrici
quadrate sono matrici invertibili (teorema che non abbiamo fatto).

\paragraph{Matrici quadrate particolari} Se $a_{ij} = 0$ per
\begin{itemize}
    \item $i>j$ è triangolare alta
    \item $i \leq j$ è strettamente triangolare alta
    \item $i<j$ è triangolare bassa
    \item $i \geq j$ è strettamente triangolare bassa
    \item $i \neq j$ è diagonale
\end{itemize}
Inoltre se $a_{ij} = a_{ji}$ la matrice è simmetrica, invece se $a_{ij} = a_{ji}$
viene detta antisimmetrica.

\subsection{La matrice trasposta}
Data una matrice $A$, la matrice trasposta è \hl{un'altra matrice $A^T$ che si
ottiene trasformando tutte le righe in colonne e viceversa}:
\[
    A \in Mat(m,n;K); A^T \in Mat(n,m,K)
\]

\paragraph{Proprietà} La matrice trasposta gode di queste proprietà:
\begin{itemize}
    \item $(A^T)^T = A$
    \item $(tA + tB)^T = tA^T + tB^T$
    \item $(AB)^T = B^T \cdot A^T$
\end{itemize}

\subsection{La traccia}
Partiti da \hl{una matrice $A$ i cui elementi sono $A[a_{ij}] \in Mat(m,n;\mathbb{K})$,
la traccia di $A$ è la somma degli elementi sulla diagonale}.

\paragraph{Proprietà} La traccia ha alcune proprietà:
\begin{itemize}
    \item $Tr(sA + tB) = sTr(A) + tTr(B)$
    \item $Tr(A^T) = Tr(A)$
    \item $Tr(AB) = Tr(BA)$
\end{itemize}

\subsection{Matrici a scala, pivot e rango}
\paragraph{Pivot} Un pivot $Pi$ è \hl{il primo elemento non nullo della riga $i$}
della matrice.

\paragraph{Matrice a scala} Una matrice nella quale \hl{il pivot della prima riga
compare prima del pivot della seconda riga, che a sua volta compare prima del
pivot della terza riga e così andare}, con le eventuali righe nulle per ultime, si
dice matrice a scala.
\[
    A =
    \begin{bmatrix}
        P1 & *  & *  & * \\
        0  & P2 & *  & * \\
        0  & 0  & P3 & * \\
        0  & 0  & 0  & P4 \\
        & & 0_{m-r, n} &
    \end{bmatrix} \in Mat(m,n;K)
\]

\paragraph{Il rango di una matrice} \hl{Il rango $r(A)$ di una matrice $A$ è il
rango di una matrice a scala $S$ ottenuta tramite riduzione di Gauss di $A$}.

\paragraph{Il rango di una matrice a scala} \hl{Il rango $r(S)$ di una matrice
a scala $S$ è il numero di pivot in $S$}.

\subsection{Eliminazione di Gauss}
Il metodo di eliminazione di Gauss ci permette \hl{di ridurre qualsiasi matrice
in una nuova matrice a scala tramite alcune operazioni }. Esisteranno \hl{diverse
riduzioni, ma tutte hanno lo stesso rango}.

\paragraph{Le operazioni di Gauss}\label{par:op-gauss}
\begin{description}
    \item[Permutazione] \hl{Scambio due righe tra di loro}
    \item[Moltiplicazione per uno scalare non nullo] \hl{Moltiplico tutti gli elementi
        di una riga per un numero} diverso da $0$
    \item[Somma tra righe] \hl{Sommo uno a uno gli elementi di due righe e la riga
        risultante la inserisco al posto di uno dei due addendi}:
        $A_{R(i)} \to A_{R(i)} + tA_{R(j)} \text{ con } i \neq j$
\end{description}

\subsection{Le matrici elementari}\label{sec:matrici-elementari}
\hl{A ogni operazione di Gauss (descritte in {\ref{par:op-gauss}}) corrisponde una
matrice elementare. Una matrice elementare non è altro che una matrice $I_n$
alla quale viene applicata l'operazione di Gauss corrispondente}:
\begin{description}
    \item[Permutazione] $P(i,j)$
    \item[Prodotto con scalare] $T(i; t)$ con $t \in \mathbb{K}^*$
    \item[Somma tra righe] $T(i,j;t)$ con $t \in \mathbb{K}^*$
\end{description}

\hl{Le operazioni di gauss, possono essere quindi tradotte in un prodotto matriciale
tra la giusta matrice elementare e la matrice}:
\begin{description}
    \item[Permutazione] $P(i,j)A$
    \item[Prodotto con scalare] $T(i; t)A$ con $t \in \mathbb{K}^*$
    \item[Somma tra righe] $T(i,j;t)A$ con $t \in \mathbb{K}^*$
\end{description}

\hl{La riduzione a scala, quindi, consiste nel moltiplicare una matrice con
determinate matrici elementari in modo da ottenere una matrice a scala 
(vedi anche Dispensa 3.48)}.

\subsection{Il determinante}
Presa una matrice \hl{$A \in Mat(m,n;\mathbb{K})$ il determinante $|A|=det(A)$ è un 
numero appartenente a $\mathbb{K}$. Il determinante è uguale a}:
\begin{itemize}
    \item per \hl{$n = 1$, $|[a_{11}]| = a_{11}$}
    \item per \hl{$n > 1$, $|A| = \sum_{j=1}^m a_{1j} \cdot C_{1j}$} con
        \hl{$C_{ij} = (-1)^{i+j} \cdot |A_{\hat{i}\hat{j}}|$ detto complemento 
        algebrico} di $a_{ij}$
\end{itemize}

\paragraph{Determinante della matrice identità} Il determinante di qualsiasi
matrice identità è \hl{$|I_n| = 1$}.

\paragraph{Determinante di una matrice $2 \times 2$} Per le matrici $2 \times 2$,
il determinante di dimostra essere \hl{$|A| = a_{11}a_{22} - a_{12}a_{21}$}.

\subsection{Matrici inverse}
Prese tre matrici quadrate $A, B, C$ allora:
\begin{itemize}
    \item $B$ si dice \hl{inversa sinistra se $BA = I_{n}$}
    \item $C$ si dice \hl{inversa destra se $AC = I_{n}$}
\end{itemize}
\hl{$A$ si dice invertibile se $B = C$}. La matrice inversa di $A$ è unica e
si indica con $A^{-1}$.

\paragraph{Proprietà dell'inversione di matrice}\label{par:proprieta-inv-matrice}
Se abbiamo $A^{-1}, B^{-1}$, allora possiamo affermare che $B^{-1}A^{-1} = (AB)^{-1}$.
Generalizzando l'affermazione: \hl{$(A_1 \dotsm A_k)^{-1} = A_{k}^{-1} \dotsm A_1^{-1}$}

\subsubsection{Teorema di caratterizzazione delle matrici invertibili}
Possiamo affermare che \hl{$\exists A^{-1}$} se e solo se:
\begin{itemize}
    \item \hl{$r(A) = n$}
    \item Esiste l'inversa sinistra o la destra
\end{itemize}

\subsubsection{Dimostrazione del teorema di caratterizzazione delle matrici invertibili}
Per dimostrare il teorema, \hl{prima dimostreremo 4 \textit{sottoteoremi}}:
\begin{itemize}
    \item \hl{L'unicità della matrice inversa}
    \item \hl{Relazione tra operazioni di Gauss e le matrici elementari}
    \item \hl{Condizione sufficiente di invertibilità}
    \item \hl{Relazione tra rango e matrice inversa}
\end{itemize}

\paragraph{L'unicità della matrice inversa} Sia una matrice \hl{$A \in Mat(m,n;\mathbb{K})$
con $B, C$ inverse da destra e sinistra. Allora $B=C$ e sono uniche, quindi le 
indichiamo con $A^{-1}$. La dimostrazione è reperibile nella dispensa (3.43)}.

\paragraph{Relazione tra operazioni di Gauss e le matrici elementari} Come già
visto \hl{in {\ref{sec:matrici-elementari}}, le matrici elementari e le operazioni
di Gauss sono strettamente collegate}. Inoltre, \hl{le inverse della matrici elementari
esistono e sono a loro volta delle matrici elementari}:
\begin{itemize}
    \item \hl{$P(i,j)^{-1} = P(i,j)$}
    \item \hl{$T(i;t)^{-1} = T(i; \sfrac{1}{t}$}
    \item \hl{$T(i,j;t)^{-1} = T(i,j; \sfrac{1}{t}$}
\end{itemize}
Ciò ci permette quindi di affermare che \hl{l'algoritmo di riduzione è sempre invertibile:
infatti possiamo scrivere che $S = t_1 \dotsm E_kA \implies A = E^{-1}_1 \dotsm E^{-1}_kS$
(ogni matrice è decomponibile in un prodotto di $k$ matrici elementari e una
matrice a scala)}, verificata dalla proprietà {\ref{par:proprieta-inv-matrice}}.

\paragraph{Condizione sufficiente di invertibilità} \hl{Se $A \in Mat(m;\mathbb{K})$,
esiste la riduzione da $A$ alla matrice $I_n$ se e solo se $r(A)=n$ 
(vedi dispensa 3.5.2)}. Di conseguenza si può affermare che \hl{$A$ è prodotto di 
matrici elementari se e solo se $r(A)=n$ e vale anche il contrario}.

\paragraph{Relazione tra rango e matrice inversa} \hl{Se $r(A)<n$ allora non esistono
inverse a sinistra e a destra. Per la dimostrazione vedi dispensa 3.59}.

\subsubsection{Algoritmo di Gauss-Jordan}
L'algoritmo di Gauss-Jordan ci permette di \hl{calcolare la matrice inversa di una
matrice tramite le operazioni di Gauss}: data \hl{$A \in Mat(m; \mathbb{K})$ 
invertibile}, allora:
\begin{itemize}
    \item \hl{$\exists A^{-1} = E_1 \dotsm E_k$}
    \item \hl{$A^{-1} \cdot [A|I_n] = [A^{-1}A|A^{-1}I_n] = [I_n|A^{-1}]$}
\end{itemize}
La seconda ci permette, quindi, di calcolare la matrice inversa riducendo $[A|I_n]$
a $[I_n|A^{-1}]$ con le operazioni di Gauss.
