\documentclass[a4paper,11pt,oneside]{article}

\usepackage[a4paper,margin=2cm]{geometry}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{booktabs}
\usepackage[bookmarks,colorlinks]{hyperref}
\usepackage{enumerate}
\usepackage[dvipsnames]{xcolor}
\usepackage{soulutf8}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{listings}

\usepackage{tikz}
\usetikzlibrary{
  arrows,
  arrows.meta,
  automata,
  positioning
}

\definecolor{hlcolor}{RGB}{253,241,98}
\sethlcolor{hlcolor}

\theoremstyle{plain}
\newtheorem{lem}{Lemma}[section]
\newtheorem{prop}{Proposizione}[section]
\newtheorem{thm}{Teorema}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]

\theoremstyle{remark}
\newtheorem{esempio}{Esempio}[section]
\newtheorem*{nota}{Nota}

\newcommand{\mhl}[1]{\colorbox{hlcolor}{$\displaystyle #1$}}
\newcommand*{\deriv}[1][]{\xRightarrow[#1]{}}
\newcommand*{\derivstar}[1][]{\xRightarrow[#1]{\star}}
\newcommand*{\termsep}[0]{\blacklozenge}

\newcommand{\peq}{$\gets$}
\newcommand{\pcom}{$\triangleright$}

\DeclareMathOperator{\succc}{succ}
\DeclareMathOperator{\last}{last}

\lstdefinestyle{mystyle}{
  basicstyle=\ttfamily,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  mathescape=true,
  escapechar=`
}
\lstset{style=mystyle}
\lstdefinelanguage{pseudocodice}{
  morekeywords={if,else,for,to,while,do,return,NIL},
  sensitive=true,
  morestring=[b]" % chktex 18
}

\title{Appunti di ``Algoritmi e Principi dell'informatica''}
\author{Alexandru Gabriel Bradatan}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduzione ai modelli}\label{sec:modelli}

I modelli sono fondamentali nell'ingegneria. I modelli sono talvolta fisici e
spesso sono \hl{modelli formali, ossia oggetti matematici che fungono da
rappresentazioni astratte di entità reali complesse.} Un modello è \hl{adeguato
se i risultati ottenuti riflettono le proprietà che ci interessano del sistema
fisico entro i limiti della nostra approssimazione.} I modelli dell'informatica
si basano principalmente sulla matematica discreta. Definiamo i due tipi di
modelli che costruiremo.

\begin{defn}[\hl{Modello operazionale}]\label{def:modello-op}
  È un modello basato sul \hl{concetto di stato e di meccanismo} per la sua
  evoluzione.
\end{defn}

\begin{defn}[\hl{Modello descrittivo}]\label{def:modello-desc}
  È un modello che \hl{formula le proprietà desiderate o no del sistema}
  piuttosto che il suo funzionamento.
\end{defn}

\hl{Le differenze tra questi due tipi di modellizzazione non sono spesso molto
ben definite}. Le fasi dell'ingegneria del software ricalcano quelli della
modellizzazione di un problema:

\begin{itemize}
  \item Analisi dei requisiti: Stesura della specifica del sistema
  \item Progetto: Architettura del software
  \item Implementazione: Scrittura effettiva del codice
\end{itemize}

\begin{esempio}
  Modellizziamo lo stesso problema, l'ordinamento di una sequenza di interi,
  secondo i due criteri enunciati sopra:
  \begin{itemize}
    \item Modello operazionale: Calcola il minimo in tutto l'array e mettilo al
      primo posto. Continua ad eseguire l'operazione finché l'array non è in
      ordine.
    \item Modello descrittivo: Individua una permutazione degli elementi
      dell'array tale che \(\forall i a[i] \leq a[i+1]\).
  \end{itemize}
\end{esempio}

\section{I linguaggi}\label{sec:linguaggi}

Il \hl{meta-modello fondamentale che useremo sarà il linguaggio}. Il termine
linguaggio è un termine che conosciamo già ed è utilizzabile a diversi ambiti
diversi come la linguistica, l'informatica, la grafica e la musica.

\subsection{Costruzione di un linguaggio}\label{sec:linguaggi-costruzione}

Iniziamo a definire i vari elementi un linguaggio. Il \hl{primo elemento formale
di un linguaggio è l'alfabeto o vocabolario}. In matematica essi sono sinonimi
anche se in italiano naturale non lo sono.

\begin{defn}[\hl{Alfabeto}]\label{def:alfabeto}
  Si dice alfabeto un \hl{insieme finito} $A$ di \hl{simboli base}.
\end{defn}

Una volta definito il concetto di alfabeto possiamo anche definire il concetto
di \hl{stringa}.

\begin{defn}[\hl{Stringa}]\label{def:stringa}
  Si dice stringa una \hl{sequenza ordinata e finita di elementi dell'alfabeto}
  $A$.
\end{defn}

Naturalmente ogni stringa possiede una \hl{lunghezza $|a|$ pari al numero di
elementi dell'alfabeto contenuti al suo interno}. Definiamo anche la \hl{stringa
nulla $\epsilon$ tale che $|\epsilon|=0$}. Definiamo infine \hl{$A^\star$
l'insieme di tutte le stringhe scrivibili con un certo alfabeto}.

\begin{nota}
  L'operatore \hl{$\star$ è detto stella, star, iterazione o stella di Kleene.}
  L'insieme $A^\star$ è infinito numerabile.
\end{nota}

Sulle stringhe possiamo definire l'operazione di \hl{concatenazione}:

\begin{defn}[\hl{Concatenazione di stringhe}]\label{def:concatenazione-stringhe}
  \begin{equation}
    \begin{array}{cccc}
      . : & A^\star \times A^\star & \to & A^\star \\
          & (x, y) & \mapsto & x.y
    \end{array}
  \end{equation}
\end{defn}

\begin{nota}
  La scrittura dell'operatore di concatenazione può essere omessa, scrivendo al
  posto di $z = x.y$ $z = xy$.
\end{nota}

L'operazione di concatenazione gode della proprietà \hl{associativa ma non della
commutativa} e ha come \hl{elemento neutro la stringa vuota}. Possiamo quindi
definire il \hl{monoide non commutativo delle stringhe rispetto alla
concatenazione $\langle A^\star, . \rangle$}.

Definito tutti questi elementi possiamo finalmente definire un \hl{linguaggio}.

\begin{defn}[\hl{Linguaggio}]\label{def:linguaggio}
  Chiamiamo un linguaggio un \hl{insieme $L$} tale che:
  \hl{$L \subseteq A^\star$}.
\end{defn}

Notiamo che \hl{$L$ può anche essere infinito}. Inoltre, poiché $L$ è un
insieme, le \hl{operazioni insiemistiche sono tutte ben definite.} Un
\hl{insieme di linguaggi che condividono le stesso proprietà è detto famiglia di
linguaggi}.

\subsection{Operazioni sui linguaggi}

\begin{defn}[\hl{Concatenazione di linguaggi}]\label{def:concatenazione-ling}
  \begin{equation}
    \mhl{ L_1 . L_2 = \{ x.y : x \in L_1, y \in L_2 \} }
  \end{equation}
\end{defn}

\begin{defn}[\hl{Potenze di linguaggi}]\label{def:potenze-linguaggi}
  \begin{equation}
    \begin{aligned}
      L^0 & = \{\epsilon\} \\
      L^i & = L^{i-1}.L
    \end{aligned}
  \end{equation}
\end{defn}

\begin{defn}[\hl{Star (Linguaggi)}]\label{def:star-linguaggi}
  \begin{equation}
    \mhl{ L^\star = \bigcup^{\infty}_{i=0} L^i }
  \end{equation}
\end{defn}

\begin{defn}[\hl{Plus (Linguaggi)}]\label{def:plus-linguaggi}
  \begin{equation}
    \mhl{ L^+ = \bigcup^{\infty}_{i=1} L^i }
  \end{equation}
\end{defn}

\begin{nota}
  La differenza tra scrivere $L^\star$ e $L^+$ è che in $L^+$ è assente il
  linguaggio vuoto!
\end{nota}

\subsection{A cosa utilizzeremo i linguaggi?}

Il nostro \hl{principale utilizzo del concetto di linguaggio sarà per definire
in maniera astratta il concetto di problema informatico}. Il nostro \hl{primo}
problema informatico sarà:

\begin{equation}
  \mhl{ x \in A^\star, L \subseteq A^\star \quad  x \in L? }
\end{equation}

Questa semplice questione è capace di modellizzare una grande varietà di
problemi diversi, scelti $A$ ed $L$ in modo adatto. Un \hl{secondo} problema sarà
quello di \hl{trovare una traduzione}, ossia una funzione così definita:

\begin{defn}[\hl{Traduzione}]\label{def:traduzione}
  \begin{equation}
    \begin{array}{cccc}
      \tau: & L_1 & \to     & L_2 \\
            & x   & \mapsto & \tau(x)
    \end{array}
  \end{equation}
\end{defn}

\section{Modelli operazionali}\label{sec:modelli-operazionali}

\subsection{Automi a stati finiti}\label{sec:fsa}

Gli \hl{automi a stati finiti (FSA) sono il modello operazionale più semplice}.
Essi sono caratterizzati da un \hl{insieme finito di stati e da un insieme di
regole di transizione}. Gli \hl{stati} possono essere di \hl{accettazione o no}.
Le \hl{regole di transizione permettono al nostro automa di passare da uno stato
all'altro in base a ciò che forniamo come input}. Quando un automa riceve un
\hl{input lo elabora e produce un output. L'elaborazione inizia in uno stato
iniziale e consiste nel leggere l'input e spostarsi da uno stato all'altro
secondo le leggi di transizione}. \hl{Se alla fine} della lettura l'automa
\hl{si trova in uno stato di accettazione, diremo che esso ha accettato l'input,
altrimenti diremo che l'ha rifiutato}.

\begin{defn}[\hl{Automa a stati finiti}]\label{def:fsa}
  Un automa a stati finiti $\mathcal{A}$ è una \hl{quintupla}
  $\langle Q, I, \delta, q_0, F \rangle$ dove:
  \begin{enumerate}
    \item \hl{$Q$} è \hl{l'insieme finito non vuoto di tutti gli stati};
    \item \hl{$I$} è \hl{l'alfabeto} di ingresso;
    \item \hl{$\delta: Q \times I \to Q$} è la \hl{funzione (relazione) di
      transizione};
    \item \hl{$q_0 \in Q$} è lo \hl{stato iniziale};
    \item \hl{$F \subseteq Q$} è l'insieme degli \hl{stati di accettazione}.
  \end{enumerate}
\end{defn}

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[node distance=2cm,auto]
    \node[state,initial,accepting]  (q0)                {$q_0$};
    \node[state]                    (q1) [right of=q0]  {$q_1$};

    \path[->]
    (q0) edge                node {0,1}  (q1)
    (q1) edge  [loop above]  node {0,1}  ();
  \end{tikzpicture}
  \caption{Esempio di diagramma di stato di FSA.}\label{fig:fsa-diagramma-stato}
\end{figure}

Se \hl{$\delta$ è una funzione l'automa si dice completo, altrimenti non
completo}. Gli \hl{automi non completi} possono facilmente \hl{resi completi
``riempiendo'' le transizioni mancanti con delle transizioni verso uno stato di
``sink'' dal quale non si può più uscire}.

Un automa \hl{processa l'input tramite una sequenza di transizioni di stato
effettuate iterando su ogni carattere della stringa e muovendosi allo stato
corrispondente a $\delta(q, i)$}. Definiamo formalmente la sequenza di mosse
eseguita da un automa a stati finiti.

\begin{defn}[\hl{Sequenza di mosse}]\label{def:fsa-seq-mosse}
  Sia un FSA $\mathcal{A}$, la sequenza di mosse è una funzione:

  \begin{equation}
    \mhl{ \delta^\star: Q \times I^\star \to Q }
  \end{equation}

  Definita \hl{induttivamente} da:

  \begin{enumerate}
    \item \hl{$\delta^\star(q, \epsilon) = q$}
    \item \hl{$\delta^\star(q, y.i) = \delta(\delta^\star(q,y), i)$} con
      $y \in I^\star, i \in I$
  \end{enumerate}
\end{defn}

Gli \hl{FSA rappresentano dei linguaggi}, più nello specifico la famiglia dei
\hl{linguaggi regolari} ($\mathbf{REG}$). Se \hl{$L$ è un linguaggio accettato}
da $\mathcal{A}$ allora diremo che esso è il \hl{linguaggio di $\mathcal{A}$ e
lo indichiamo come $L(\mathcal{A})$}. Usando la definizione di \hl{sequenza di
mosse possiamo scrivere $L(\mathcal{A})$ come}:

\begin{equation}
  L(\mathcal{A}) = \{ x \in I^\star : \delta^\star(q_0, x) \in F \}
\end{equation}

Per \hl{rappresentare il linguaggio} accettato da un FSA possiamo usare la pura
notazione insiemistica oppure, visto che i linguaggi sono regolari, delle
\hl{espressioni regolari}. Definiamo la sintassi delle espressioni regolari.

\begin{defn}[Espressioni regolari]\label{def:regex}
  Dato una \hl{alfabeto $A$}, siano le \hl{seguenti operazioni}:

  \begin{itemize}
    \item \hl{$R + S = R \cup S$} con $R \subseteq A^\star$ e
      $S \subseteq A^\star$ (notazione equivalente: $R | S$)
    \item \hl{$\star$ la star di Kleene:}
      $R^\star = \{x^n : n \in \mathbb{N}, x \in R\}$
    \item \hl{$R^+ = \{x^n : n \in \mathbb{N}^*, x \in R\}$}
    \item la \hl{concatenazione}: $RS = \{xy : x \in R, y \in S\}$
  \end{itemize}

  Allora, dato un \hl{alfabeto $A$} e un \hl{insieme di simboli}
  \hl{$\{+,\star,(,),.,\emptyset\}$} si dice \hl{espressione regolare su $A$} la
  \hl{stringa} $R \in A \cup \{+,\star,(,),.,\emptyset\}$ che \hl{rende vera una
  delle seguenti condizioni}:

  \begin{enumerate}
    \item \hl{$R = \emptyset$};
    \item \hl{$R \in A$};
    \item \hl{$R = S + T$, $R = ST$, $R = S^\star$, $R = S^+$} con $S,T$
      espressioni regolari su $A$.
  \end{enumerate}
\end{defn}

Gli \hl{automi a stato finito sono modelli di calcolo con una memoria finita
pari al numero di stati}. Infatti \hl{ogni stato} rappresenta una \hl{istantanea
della situazione in cui si trova il sistema in un dato istante}. Il fatto di
\hl{possedere una memoria finita}, come vedremo in~\ref{sec:fsa-analisi}, è uno
degli \hl{aspetti più limitanti} degli automi a stato finito e ci costringeranno
a costruire modelli più sofisticati.

\subsubsection{Automi a stati finiti traduttori}\label{sec:fsa-trad}

Un FSA può anche essere \hl{usato come traduttore tra un linguaggio ed un
altro}. Ci basta \hl{aggiungere la capacità di dare un output} al nostro automa.

Un semplice FSA si limita semplicemente ad interpretare un input senza produrre
nessun output. Se ad un \hl{FSA} aggiungiamo la possibilità di \hl{produrre un
output} otteniamo un \hl{trasduttore}. Un \hl{particolare tipo} di trasduttore
che ci interessa è il \hl{traduttore}.

Prima di parlare di traduttori, però, formalizziamo come un automa produce un
output.

\begin{defn}[\hl{Funzione di transizione con uscita}]\label{def:fsa-transizione-uscita}
  Chiamiamo \hl{$\delta(q, i/w)$} con \hl{$i \in I$ e $w \in O$, $I, O$
  alfabeti}, una \hl{funzione di transizione tra stati che restituisce il nuovo
  stato del FSA e un simbolo complesso $w$}.
\end{defn}

Ora possiamo definire il traduttore.

\begin{defn}[\hl{Automa a stati finiti traduttore}]\label{def:fsa-trad}
  Sia $\mathcal{A} = \langle Q, I, \delta, q_0, F \rangle $ un \hl{automa a
  stati finiti} con funzione di transizione \hl{$\delta(q, i/w)$}, definiamo
  \hl{automa a stati finiti traduttore} $\mathcal{T}$ la \hl{terna} \hl{$\langle
  \mathcal{A}, O, \eta \rangle$} dove:

  \begin{itemize}
    \item \hl{$O$} è \hl{l'alfabeto di uscita} di $\delta$;
    \item \hl{$\eta : Q \times I \to O^\star$} \hl{funzione di traduzione}.
  \end{itemize}
\end{defn}

\hl{Analogamente a quanto fatto in}~\ref{def:fsa-seq-mosse} \hl{possiamo
iterare} la funzione di traduzione usando la stella di Kleene, così da \hl{poter
finalmente enunciare la traduzione come}:

\begin{equation}
  \mhl{\tau(x) = \eta^\star(q_0, x)}
\end{equation}

Se il \hl{traduttore conclude} la propria esecuzione \hl{su uno stato di
accettazione}, allora possiamo dire che la \hl{stringa di input e corretta e la
sua traduzione nel linguaggio di output è $\tau(x)$}.

\subsubsection{Analisi degli automi a stati finiti}\label{sec:fsa-analisi}

Gli automi a stati finiti sono un modello molto semplice ed intuitivo, applicato
a molti settori. Abbiamo già visto in~\ref{sec:fsa} che sono adatti a modellare
linguaggi regolari e che sono modelli a memoria finita. Studiamone ora nel
dettaglio le proprietà ed eventuali limitazioni.

Innanzitutto esiste qualche \hl{condizione affinché un automa a stati finiti sia
o no accettore di un linguaggi finito o infinito} (il caso di linguaggio vuoto è
banale)? Si dimostra che \hl{esistono} due condizioni \hl{necessarie e
sufficienti} per l'accettazione dei linguaggi, \hl{una per il caso finito e un
per l'infinito}.

\begin{thm}[\hl{Condizione di accettazione di un linguaggio finito}]\label{thm:accettazione-ling-finito}
  \hl{Condizione sufficiente e necessaria} affinché un \hl{automa a stati finiti
  accetti un linguaggio finito non vuoto} è che possa accettare una stringa di
  lunghezza inferiore al numero di stati (\hl{$|x| < |Q|$}).
\end{thm}

\begin{thm}[\hl{Condizione di accettazione di un linguaggio infinito}]\label{thm:accettazione-ling-infinito}
  \hl{Condizione sufficiente e necessaria} affinché un \hl{automa a stati finiti accetti
  un linguaggio infinito} è che possa accettare una stringa tale che:
  \hl{$|Q| \leq |x| < 2|Q|$}.
\end{thm}

Questo teorema deriva dall'osservazione che \hl{se un automa accetta un
linguaggio infinito, nel suo grafo saranno presenti dei cicli che potranno
essere percorsi un numero arbitrario di volte}. Nel \hl{caso peggiore} verranno
\hl{ripercorsi tutti gli stati} dell'automa \hl{tranne l'ultimo} che deve essere
di accettazione.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[node distance=2cm,auto]
    \node[state,initial]            (q0)                {$q_0$};
    \node[state]                    (q1) [right of=q0]  {$q_1$};
    \node[state]                    (q2) [right of=q1]  {$q_2$};
    \node[state]                    (q3) [right of=q2]  {$q_3$};
    \node[state]                    (q4) [right of=q3]  {$q_4$};
    \node[state,accepting]          (q5) [right of=q4]  {$q_5$};

    \path[->]
    (q0) edge                node {} (q1)
    (q1) edge                node {} (q2)
    (q2) edge                node {} (q3)
    (q3) edge                node {} (q4)
    (q4) edge                node {} (q5)
    (q5) edge  [bend left]   node {} (q0);
  \end{tikzpicture}
  \caption{Peggior caso del teorema~\ref{thm:accettazione-ling-infinito}.}\label{fig:fsa-accettazione-ling-infinito}
\end{figure}

Possiamo quindi vedere \hl{una stringa di un linguaggio infinito come composta
da 3 parti: un preambolo al ciclo, il ciclo e l'epilogo}. Il ciclo può ripetersi
un numero infinito di volte, generando la seguente espressione regolare:

\begin{equation}
  \mhl{x = x_i x_c^\star x_f}
\end{equation}

Questa è una \hl{condizione necessaria affinché una stringa di un linguaggio
infinito sia riconosciuta da un FSA} e rappresenta il \hl{contenuto del
``Pumping lemma''}:

\begin{lem}[\hl{Pumping lemma}]\label{thm:pumping-lemma}
  Sia un \hl{automa a stati finiti} $\mathcal{A}$ che accetta un linguaggio
  $L(\mathcal{A})$. Allora per ogni \hl{$x \in L$ con $|x| > |Q|$ esiste $q \in
  Q$ e $w \in I^+$} tali da verificare una di queste condizioni:

  \begin{itemize}
    \item \hl{$x = ywz$}
    \item \hl{$\delta^\star(q,w) = q$}
  \end{itemize}

  Come \hl{conseguenza} si ha: \hl{$\forall n \in \mathbb{N} \geq 0, y w^n z \in
  L$}.
\end{lem}

Il Pumping lemma, quindi, ci \hl{mette dei paletti sui tipi di linguaggi
infiniti che un FSA può accettare}. Infatti consideriamo il seguente linguaggio
infinito:

\begin{equation}
  \mhl{L = \{ a^n b^n : n \in \mathbb{N} \}}
\end{equation}

\hl{Supponiamo} che un \hl{FSA sia in grado di accettarlo}, allora consideriamo
la stringa \hl{$x = a^m b^m$ con $m > |Q|$} e applichiamo il \hl{Pumping lemma}.
Otterremo \hl{3 casi possibili}:

\begin{enumerate}
  \item \hl{$w = a^p$} e quindi dovrebbe essere:

    \begin{equation}
      \mhl{
      x = a^m b^m = a^r a^p a^s b^m = a^r w a^s b^m \in L \text{ con } r+p+s=m
      }
    \end{equation}

    \hl{Se il Pumping lemma dovesse valere}, allora si dovrebbe avere che
    \hl{$a^r w^k b^m \in L$} che ci porta ad un \hl{assurdo}.
  \item \hl{$w = b^p$ analogo al precedente}.
  \item \hl{$w = a^p b^q$} e quindi dovrebbe essere:

    \begin{equation}
      \mhl{
      x = a^m b^m = a^r a^p b^q b^s = a^r w b^s \in L \text{ con } r+p=q+s=m
      }
    \end{equation}

    \hl{Se il Pumping lemma dovesse valere}, allora si dovrebbe avere che
    \hl{$a^r w^k b^s \in L$} che ci porta ad un \hl{assurdo}.
\end{enumerate}

Ciò significa che \hl{considerare $L$ come accettato da una FSA è un assurdo}.
Il linguaggio $L$ appena costruito ci dimostra quindi la necessità di costruire
modelli di calcolo più potenti. \hl{Infatti per contare fino a $n$ non basta la
memoria finita degli FSA ma servirebbe una memoria infinita!}

\subsubsection{Proprietà di chiusura degli automi a stati finiti}\label{sec:chiusura-fsa}

Il concetto di chiusura di un insieme rispetto ad un'operazione o proprietà è
un concetto già affrontato nel precedente corso di logica e algebra lineare. In
questo contesto ci occupiamo della \hl{chiusura della famiglia dei linguaggi
regolari}.

\hl{La famiglia dei linguaggi regolari è chiusa rispetto a tutte le operazioni
insiemistiche, alla concatenazione, alla star di Kleene e praticamente tutte le
altre viste fino ad ora.} Proviamo a costruirne qualcuno.

\paragraph{Intersezione} Proviamo a costruire l'intersezione di due automi. Il
risultato sarà un automa che \hl{accetta solo stringe solo stringhe accettate da
entrambi gli automi di partenza}. Dati i due automi di partenza:

\begin{align}
  A^1 &= \langle Q^1, I, \delta^1, q_0^1, F^1 \rangle \\
  A^2 &= \langle Q^2, I, \delta^2, q_0^2, F^2 \rangle
\end{align}

Possiamo scrivere l'automa intersezione come:

\begin{gather}
  \langle A^1, A^2 \rangle = \langle Q^1 \times Q^2, I, \delta,
    \langle q_0^1, q_0^2 \rangle, F^1 \times F^2 \rangle\label{eqn:automa-intersezione} \\
  \delta(\langle q^1, q^2 \rangle, i) = \langle \delta(q^1, i),
    \delta(q^2, i) \rangle
\end{gather}

Con una semplice induzione si può dimostrare che il linguaggio di un automa così
definito è $L(\langle A^1, A^2 \rangle) = L(A^1) \cap L(A^2)$.

\begin{figure}[htb]
  \begin{tikzpicture}[node distance=2cm,auto]
    \node[state,initial]   (q0) {$q_0$};
    \node[state]           (q1) [right of=q0] {$q_1$};
    \node[state]           (q2) [right of=q1] {$q_2$};
    \node[state,accepting] (q3) [right of=q2] {$q_3$};
    \node[] (t) [left=1.5cm of q0] {$A^1$:};

    \path[->]
      (q0) edge node {$b$} (q1)
      (q1) edge node {$a$} (q2)
      (q2) edge node {$a$} (q3);
  \end{tikzpicture}
  \begin{tikzpicture}[node distance=2cm,auto]
    \node[state,initial]   (p0) {$p_0$};
    \node[state]           (p1) [right of=p0] {$p_1$};
    \node[state]           (p2) [right of=p1] {$p_2$};
    \node[state,accepting] (p3) [right of=p2] {$p_3$};
    \node[] (t) [left=1.5cm of p0] {$A^2$:};

    \path[->]
      (p0) edge node {$b$} (p1)
      (p1) edge node {$a$} (p2)
      (p2) edge node {$a$} (p3);
  \end{tikzpicture}
  \begin{tikzpicture}[node distance=3cm,auto]
    \node[state,initial]   (q0) {$\langle q_0, p_0 \rangle$};
    \node[state]           (q1) [right of=q0] {$\langle q_1, p_1 \rangle$};
    \node[state]           (q2) [right of=q1] {$\langle q_2, p_2 \rangle$};
    \node[state,accepting] (q3) [right of=q2] {$\langle q_3, p_3 \rangle$};
    \node[] (t) [left=1.5cm of q0] {$\langle A^1, A^2 \rangle$:};

    \path[->]
      (q0) edge node {$b$} (q1)
      (q1) edge node {$a$} (q2)
      (q2) edge node {$a$} (q3);
  \end{tikzpicture}
  \centering
  \caption{Due automi e la loro intersezione.}%
  \label{fig:esempio-automa-intersezione}
\end{figure}

\paragraph{Unione} Per l'unione il tutto funziona in \hl{modo analogo}. L'automa
risultante sarà un automa che \hl{accetterà stringhe accettate da almeno uno dei
due automi di partenza}. Eseguendo un procedimento simile a quello effettuato
per ottenere~\ref{eqn:automa-intersezione} abbiamo:

\begin{equation}
  \langle A^1, A^2 \rangle = \langle Q^1 \times Q^2, I, \delta,
    \langle q_0^1, q_0^2 \rangle, F^1 \times Q^2 \cup Q^1 \times F^2 \rangle
\end{equation}

Questo approccio presenta, però, un \hl{problema}: \hl{non funziona se l'automa
di partenza non è completo}. Con l'intersezione non avevamo problemi perché
l'automa intersezione accettava solo stringhe accettate da entrambi gli automi
iniziali, rimuovendo il problema della non completezza. Con l'unione invece
otterremo un automa che accetta anche quando uno degli automi iniziali non
accetta, rendendo problematico il caso di un errore dovuto alla parzialità della
funzione di transizione. \hl{Bisogna, quindi, ricordarsi di completare gli
automi aggiungendo eventuali stati di errore prima di eseguirne l'unione}.

\paragraph{Complemento} Per il complemento la situazione è \hl{analoga
all'unione}: anche qui dobbiamo stare attenti alla completezza dei due automi di
partenza.  L'automa complemento sarà praticamente uguale all'automa di partenza,
solo che \hl{gli stati di accettazione ora saranno di non accettazione e
viceversa}.

\begin{equation}
  \neg A = \langle Q, I, \delta, q_0, Q \setminus F \rangle
\end{equation}

\subsection{Automi a stati finiti con pila}\label{sec:pda}

Arricchiamo ora i nostri automi a stati finiti con della \hl{memoria}. Questa
memoria sarà \hl{strutturata come una pila (stack)} di dimensione potenzialmente
illimitata. L'automa può \hl{manipolare la pila tramite le due operazioni
fondamentali} delle pile: \hl{push e pop}. L'automa così strutturato si chiama
``automa a pila'', in breve PDA (dall'inglese ``Push Down Automata'').
Indicheremo convenzionalmente \hl{l'inizio della pila con $Z_0$}.

A grandi linee, una \hl{mossa} dell'automa a pila è \hl{strutturata in diversi
passi}:

\begin{enumerate}
  \item \hl{Leggi un simbolo (o nulla) dall'input} (d'ora in poi lo chiameremo
    nastro d'ingresso);
  \item Esegui una \hl{pop di un elemento} dalla stack;
  \item \hl{Cambia stato};
  \item Sposta di una posizione il puntatore del carattere corrente (la testina
    del nastro d'ingresso);
  \item Esegui una \hl{push di una serie di caratteri (anche nulla)};
  \item Se è un automa traduttore, \hl{scrivi una stringa (anche nulla)
    sull'output} (nastro d'uscita).
\end{enumerate}

Come per gli FSA, \hl{la stringa in ingresso viene riconosciuta se l'automa la
scandisce completamente e termina su uno stato di accettazione}. Lo stato della
\hl{pila non è rilevante}. Se l'automa è \hl{traduttore}, allora \hl{se accetta}
la stringa \hl{l'output corrisponde alla stringa tradotta, altrimenti} viene
detta \hl{indefinita} e lo indichiamo con \hl{$\tau(x) = \bot$}. In generale
useremo il simbolo $\bot$ anche con la funzione $\delta$ per indicare una
transizione indefinita.

Poiché le transizioni di stato sono più ricche di quelle degli FSA, dobbiamo
adottare una \hl{notazione ben definita per indicare le varie azioni compiute}:

\begin{equation}
  \mhl{a, A / B\ldots, c}
\end{equation}

\begin{itemize}
  \item \hl{$a$} il carattere \hl{letto in input};
  \item \hl{$A$} il carattere \hl{letto dalla pila tramite stack};
  \item \hl{$B\ldots$} i caratteri \hl{reinseriti nella pila} tramite la push;
  \item \hl{$c$} il carattere \hl{scritto sull'output}.
\end{itemize}

Nel caso in cui l'automa \hl{non leggesse nulla} dal nastro d'ingresso diciamo
che l'automa \hl{ha effettuato una $\epsilon$-mossa} e la indichiamo
\hl{riportando $\epsilon$ come carattere letto} dal nastro d'ingresso. Vale
\hl{lo stesso anche con il carattere della push}.

\begin{nota}
  Un automa a pila \hl{non può non leggere nessun carattere dalla pila!}
  Scrivere, quindi, \hl{$a, \epsilon/B$ è sbagliato}.
\end{nota}

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[node distance=3cm,auto]
    \node[state,initial]   (q0) []            {$q_0$};
    \node[state]           (q1) [right of=q0] {$q_1$};
    \node[state]           (q2) [right of=q1] {$q_2$};
    \node[state,accepting] (q3) [right of=q2] {$q_3$};

    \path[->]
    (q0) edge              node {$a, Z_0/Z_0 A$} (q1)
    (q1) edge [loop above] node {$a, A/AA$} ()
    (q1) edge              node {$b, A/\epsilon$} (q2)
    (q2) edge [loop above] node {$b, A/\epsilon$} ()
    (q2) edge              node {$\epsilon, Z_0/\epsilon$} (q3);
  \end{tikzpicture}
  \caption{Esempio di un automa a pila.}%
  \label{fig:automa-pila}
\end{figure}

Formalizziamo ora i concetti introdotti.

\begin{defn}[\hl{Automa a stati finiti a pila}]\label{defn:automa-pila}
  Definiamo un \hl{automa a stati finiti con pila} $\mathcal{A}$ una
  \hl{eptupla} \hl{$\langle I, \Gamma, \delta, q_0, Z_0, F \rangle$} dove:

  \begin{itemize}
    \item \hl{$Q, I, q_0, F$} sono definiti alla \hl{stessa maniera} di un
      \hl{FSA};
    \item \hl{$\Gamma$} è l'\hl{alfabeto di pila};
    \item \hl{$Z_0$} è il \hl{simbolo iniziale di pila};
    \item \hl{$\delta: Q \times (I \cup \epsilon) \times \Gamma \to Q \times
      \Gamma^\star$} è la \hl{funzione di transizione}.
  \end{itemize}
\end{defn}

\begin{defn}[Automa a stati finiti a pila traduttore]\label{defn:automa-pila-trad}
  Definiamo un automa a stati finiti con pila $\mathcal{A}$ una ennupla
  $\langle I, \Gamma, \delta, q_0, Z_0, F, 0, \eta \rangle$ dove:

  \begin{itemize}
    \item $Q, I, q_0, F, O$ sono definiti alla stessa maniera di un FSA;
    \item $\Gamma$ è l'alfabeto di pila;
    \item $Z_0$ è il simbolo iniziale di pila;
    \item
      $\delta: Q \times (I \cup \epsilon) \times \Gamma \to Q \times \Gamma^\star$
      è la funzione di transizione;
    \item $\eta: Q \times (I \cup \epsilon) \times \Gamma \to O^\star$ è la
      funzione di traduzione.
  \end{itemize}
\end{defn}

\begin{nota}
  La funzione $\eta$ è definita ovunque lo è anche $\delta$.
\end{nota}

La funzione di \hl{$\delta$ deve essere parziale} perché l'esistenza delle
\hl{$\epsilon$-mosse causerebbe non-determinismo} in caso di completezza
dell'automa: \hl{non possono esistere una $\epsilon$-mossa tra due stati $q, q'$
se tra questi due stati esiste già un'altra mossa che legge lo stesso simbolo
dalla pila}. Un automa a stati finiti non può decidere deterministicamente tra
una $\epsilon$-mossa e una regolare. Ciò che abbiamo detto fino ad ora può
essere formalizzato come:

\begin{equation}
  \mhl{
  \forall q, A \delta(q, \epsilon, A) \neq \bot \implies
    \forall i \delta(q,i,A) = \bot
  }
\end{equation}

Il concetto intuitivo di stato che avevamo introdotto con gli FSA oramai non è
più adeguato in quanto è troppo semplicistico. Negli automi a pila, infatti, si
aggiunge anche lo stato della pila che anch'esso contribuisce allo stato
generale dell'automa. Formalizziamo, quindi, \hl{la generalizzazione dello
stato: la configurazione}.

\begin{defn}[Configurazione]\label{def:configurazione}
  Chiamiamo \hl{configurazione} di un automa a pila la \hl{tripla} (se traduttore
  quadrupla) $c = \langle q, x \gamma \rangle$
  ($\langle q, x \gamma, z \rangle$) dove:

  \begin{itemize}
    \item \hl{$q \in Q$} è lo \hl{stato} dell'organo di controllo;
    \item \hl{$x$} è la \hl{stringa ancora da leggere} nel nastro d'ingresso;
    \item \hl{$\gamma$} è la \hl{stringa di caratteri nella pila} (rappresentati
      usando la convenzione ``alto-destra basso-sinistra'').
    \item \hl{$z$} la \hl{stringa già scritta sul nastro di uscita}.
  \end{itemize}
\end{defn}

Tra le configurazioni di un automa a pila è presente una \hl{relazione di
transizione indicata con $\vdash$}. Possiamo esprimere il passaggio di stato
come:

\begin{equation}
  \mhl{
  c = \langle q, x, \gamma \rangle \vdash c' = \langle q', x', \gamma' \rangle
  }
\end{equation}

Indichiamo con \hl{$\vdash^\star$ la chiusura transitiva e riflessiva di
$\vdash$}. Essa non è altro che l'insieme delle nostre \hl{vecchie sequenze di
mosse}. Usando $\vdash^\star$ possiamo anche \hl{formalizzare l'accettazione di
una stringa} (e la sua eventuale traduzione) come:

\begin{equation}
  c_0 = \langle q_0, x, Z_0, (\epsilon) \rangle \vdash^\star
    c_f = \langle q, \epsilon, \gamma , (z) \rangle
\end{equation}

\begin{nota}
  D'ora in poi indicheremo la chiusura transitiva e riflessiva di una relazione
  $A$ con $A^\star$. La chiusura solo transitiva sarà $A^+$.
\end{nota}

\subsubsection{Analisi degli automi a pila}\label{sec:automi-pila-analisi}

Come anche per gli FSA, è facile trovare un \hl{linguaggi non riconosciuti dagli
automi a pila}. Uno di questi è:

\begin{equation}
  L = \{ a^n b^n c^n \}\label{eqn:automi-pila-lang-non-ric}
\end{equation}

L'incapacità di riconoscere il linguaggio~\ref{eqn:automi-pila-lang-non-ric} può
essere ricondotta al fatto che \hl{la pila è una memoria distruttiva}: bisogna
distruggere ciò che ci è salvato dentro per leggerla (pop). \hl{Quanto appena
detto si dimostra con una estensione al Pumping lemma}. Noi non la tratteremo.

Studiamo ora la chiusura della famiglia dei linguaggi riconosciuti da un automa
a pila. Prendiamo due linguaggi $L_1 = \{ a^n b^n \}, L_2 = \{ a^n b^{2n} \}$.
Essi sono individualmente riconosciuti dagli automi a pila, la loro unione però
non lo è (ovviamente si può dimostrare). Ciò ci indica che \hl{la famiglia dei
linguaggi riconosciuti dagli automi a pila non è chiusa rispetto all'unione e
(per motivi analoghi) neanche rispetto all'intersezione}. Per quanto riguarda
invece il \hl{complemento}? Usiamo la \hl{stessa idea degli automi a stati
finiti}: scambiamo gli stati di accettazione con quelli di non accettazione.
Anche in questo caso \hl{$\delta$ va ``completata'' cercando di evitare di
cadere nel non-determinismo}. Le \hl{$\epsilon$-mosse possono ancora causare
problemi}:

\begin{itemize}
  \item Si può creare un \hl{ciclo di $\epsilon$-mosse} che fa entrare l'automa
    in blocco;
  \item Può esserci una \hl{sequenza di $\epsilon$-mosse in cui si alternano
    stati di accettazione e di non accettazione}.
\end{itemize}

In entrambi i casi \hl{si possono costruire automi equivalenti che eliminano
questi problemi}, nel primo caso eliminando i cicli, nel secondo forzando
l'accettazione alla fine di una sequenza di $\epsilon$-mosse.

\subsection{Macchine di Turing}\label{sec:macchine-turing}

L'ultimo automa che tratteremo sarà la macchina di Turing. Partiamo dalla
\hl{versione a $k$-nastri}, un po' più semplice di quella originaria, ma che
gode delle stesse proprietà.

Come indica il nome, la macchina di Turing a $k$-nastri è \hl{analoga a un
automa a stati finiti al quale aggiungiamo $k$ nastri di memoria}. Le
\hl{testine dei nastri possono muoversi in ambo le direzioni} arbitrariamente.
Come anche per gli altri automi avremo i \hl{soliti stati e alfabeti}. Per
convenzione storica, \hl{i nastri sono rappresentati da sequenze infinite di
celle} invece che da stringhe finite. Per rappresentare una \hl{cella
inutilizzata} usiamo il simbolo speciale \hl{``blank''}, rappresentato da
\hl{uno spazio vuoto, $\_$ o $\not{b}$}. Assumeremo che ogni nastro \hl{contenga
solo un numero finito di celle non contenenti blank}. Come nel caso degli automi
a pila indichiamo \hl{l'inizio del nastro con $Z_0$}.

La mossa della macchina di Turing è simile a quella dell'automa a pila ma un po'
più articolata. Possiamo suddividerla in \hl{due ``fasi''}:

\begin{enumerate}
  \item \hl{Lettura}:
    \begin{itemize}
      \item \hl{Legge} il carattere in corrispondenza della testina del
        \hl{nastro d'ingresso};
      \item \hl{Legge} i $k$ caratteri dai \hl{nastri};
      \item Valuta lo \hl{stato} dell'organo di controllo.
    \end{itemize}
  \item \hl{Scrittura}:
    \begin{itemize}
      \item Cambia \hl{stato};
      \item \hl{Scrittura} di un carattere sui \hl{nastri di memoria};
      \item Eventuale \hl{scrittura} di un carattere sul \hl{nastro di uscita};
      \item \hl{Spostamento delle testine} di una posizione.
    \end{itemize}
\end{enumerate}

Introduciamo anche la \hl{nuova notazione per le transizioni} (tra parentesi è
riportata la parte aggiuntiva nel caso di presenza di output):

\begin{equation}
  \mhl{
  i, \langle A_1, \ldots, A_k \rangle/(o),
    \langle A'_1, \ldots, A'_k \rangle,
    \langle M_0, \ldots, M_k, (M_{k+1}) \rangle
  }
\end{equation}

\begin{itemize}
  \item $i$: carattere letto dal nastro di ingresso;
  \item $\langle A_1, \ldots, A_k \rangle$: le letture dai vari nastri;
  \item $o$: il carattere scritto sul nastro di uscita;
  \item $\langle A'_1, \ldots, A'_k \rangle$: le scritture sui vari nastri;
  \item $\langle M_0, \ldots, M_k, (M_{k+1}) \rangle$: i movimenti effettuati
    dai vari nastri dove:
    \begin{itemize}
      \item $M_0$ è il movimento della testina di ingresso;
      \item $M_{k+1}$ è il movimento della testina di output.
    \end{itemize}
\end{itemize}

Le nuove funzioni di transizione e traduzioni saranno le seguenti:

\begin{align}
  \delta&: Q \times I \times \Gamma^k \to
    Q \times \Gamma^k \times {\{R,L,S\}}^{k+1} \\
  \eta&: Q \times I \times \Gamma^k \to
    Q \times \Gamma^k \times {\{R,L,S\}}^{k+1} \times O \times \{R,S\}
\end{align}

Lo \hl{stato iniziale} di una macchina di Turing è \hl{come ce lo
immagineremmo}: $Z_0$ seguito da blank nei nastri, uscita tutta blank, testine
in posizione 0 per ogni nastro, organo di controllo nello stato iniziale e
stringa iniziale scritta sul nastro iniziale a partire dalla posizione 0 seguita
da blank. \hl{Le condizioni di terminazione, e quindi di accettazione, sono
leggermente diverse da quelle viste fino ad ora}:

\begin{itemize}
  \item Gli \hl{stati di accettazione} sono sempre \hl{$F \subseteq Q$};
  \item Per convenzione la \hl{$\delta (\eta)$ non è definita a partire dagli
    stati finali}:

    \begin{equation}
      \forall q \in F \delta(q, \ldots) = \bot, (\eta (q, \ldots) = \bot)
    \end{equation}

  \item La macchina si \hl{ferma in uno stato $q$ quando
    $\delta(q, \ldots) = \bot$};
  \item La stringa in ingresso è \hl{accettata se e solo se dopo un numero
    finito di transizioni la macchina si ferma in uno stato di accettazione}.
\end{itemize}

Ciò significa che una stringa in ingresso non è accettata se la macchina si
ferma in uno stato di accettazione o se la macchina non si ferma.

\subsubsection{Proprietà di chiusura delle macchine di Turing}\label{sec:analisi-mt}

La famiglia dei linguaggi riconosciuti dalle macchine di Turing è \hl{chiusa}
per:

\begin{itemize}
  \item \hl{$\cup$} simulazione di \hl{esecuzione ``in parallelo''};
  \item \hl{$\cap$} simulazione di \hl{esecuzione ``in serie''};
  \item \hl{$ . $} simile a $\cap$;
  \item \hl{$\star$} come concatenazione.
\end{itemize}

Per quanto riguarda il \hl{complemento} la situazione è diversa. Il principale
problema sta nel fatto che \hl{non esistono macchine di Turing loop-free}, ossia
macchine di Turing equivalenti ad una data ma con eventuali cicli rimossi (come
si poteva fare per PDA e FSA). A causa di ciò il \hl{rischio di generare
computazioni infinite} non è eliminabile. La \hl{dimostrazione} di ciò verrà
vista in seguito nella \hl{parte di computabilità del corso}.

\subsubsection{Modelli equivalenti di macchine di Turing}\label{sec:modelli-mt}

Noi abbiamo introdotto il modello della macchina di Turing a $k$ nastri. \hl{Si
può dimostrare, però, che esistono diverse formulazioni equivalenti della
macchina di Turing. Queste diverse tipologie di macchina sono funzionalmente
equivalenti, con unica differenza la complessità dell'organo di controllo.}

\paragraph{TM a singolo nastro} Il primo modello, e anche il \hl{più
vecchio}, che enunceremo è quello della TM a singolo nastro. Essa, come implica
il nome, \hl{possiede un singolo nastro di memoria infinito sul quale la testina
si può muovere in entrambe le direzioni}. Questo unico nastro \hl{viene usato
come memoria, input ed eventualmente output}. Attenzione a \hl{non confonderla
con la TM con $k=1$ nastri} (chiamata ``ad un nastro di memoria'').

\paragraph{TM con nastro di memoria bidimensionale} Un altro modello
interessante è quello in cui il \hl{nastro di memoria è organizzato come una
tabella e la testina si può muovere in entrambe e 4 le direzioni cardinali
sequenzialmente}. Si ricorda che l'accesso randomico alla memoria non è
possibile con una semplice TM\@.

\paragraph{Macchine di von Neumann} La macchina di von Neumann è il
\hl{modello astratto di un computer con processore, memoria ad accesso randomico
e periferiche}. Le macchine di Turing \hl{possono simulare anche questo modello}
già più complesso. Il \hl{risultato} però è una macchina \hl{molto più complessa
e lenta}. Ciò vale in generale: \hl{si possono costruire modelli più prestanti e
complessi della macchina di Turing, ma essi non ne aumentano la capacità
espressiva}.

\subsection{Modelli operazionali non deterministici}\label{sec:modelli-non-det}

I modelli visti \hl{fino ad ora} sono detti \hl{deterministici}: \hl{in un certo
stato e con certi ingressi la mossa eseguita è sempre la stessa}. Se
\hl{neghiamo questa ipotesi} creiamo dei \hl{modelli} detti \hl{non
deterministici}. Ciò significa che \hl{per un certo stato l'automa potrà
eseguire più mosse diverse ed è lui a scegliere quale di queste eseguire}. Si
vengono a \hl{creare}, quindi, dei \hl{fili (thread) di esecuzione che l'automa
può seguire}. Il modello dell'esecuzione di un automa non deterministico può
visto come:

\begin{description}
  \item[Parallelo] L'automa esegue \hl{tutte le strade contemporaneamente}.
    L'automa si ferma quando tutti le fili di esecuzione hanno terminato o per
    esaurimento dell'input o per la parzialità delle funzione di transizione.
  \item[Sequenziale] L'automa \hl{esegue solo un filo, ma ad ogni biforcazione
    sceglie uno dei fili di esecuzione in modo casuale}.
\end{description}

Di solito i modelli non deterministici \hl{tendono ad essere più compatti
sacrificando l'operatività}. In un \hl{contesto pratico} essi possono essere
\hl{realizzati utilizzando più esecutori, seguendo il modello parallelo}.
Vediamo le varianti non deterministiche dei modelli già noti.

\subsubsection{Automi a stati finiti non deterministici}\label{sec:nfa}

Iniziamo con il primo modello, e il più semplice gli automi a stati finiti non
deterministici o NFA (``nondeterministic finite automata''). Diciamo che un
\hl{FSA è non deterministico se esiste $\delta(q, a) = \{q_1, q_2\}$} ossia
\hl{associati ad uno stato e ad un carattere è associato un insieme di
transizioni}. Sarà quindi necessario \hl{ridefinire $\delta$ e $\delta^\star$}.

\begin{gather}
  \mhl{ \delta: \; Q \times I \to \wp(Q) } \\
  \mhl{
    \delta^\star(q,x) =
    \begin{cases}
      \delta^\star(q, \epsilon) = \{q\} & x = \epsilon \\
      \delta^\star(q, y.i) = \bigcup_{q' \in \delta^\star(q, y)} \delta(q', i) &
        x=y.i, i \in I
    \end{cases}
  }
\end{gather}

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[node distance=3cm,auto]
    \node[state,initial]   (q0)  []                                  {$q_0$};
    \node[state]           (q1)  [above right of=q0, yshift=-0.25cm] {$q_1$};
    \node[state,accepting] (q11) [above right of=q1, yshift=-1cm]    {$q_{11}$};
    \node[state]           (q12) [below right of=q1, yshift=+1cm]    {$q_{12}$};
    \node[state]           (q2)  [below right of=q0, yshift=+0.25cm] {$q_2$};
    \node[state,accepting] (q21) [above right of=q2, yshift=-1cm]    {$q_{21}$};
    \node[state]           (q22) [below right of=q2, yshift=+1cm]    {$q_{22}$};

    \path[->]
    (q0) edge node {$a$} (q1)
    (q1) edge node {$a$} (q11)
    (q1) edge node {$a$} (q12)
    (q0) edge node {$a$} (q2)
    (q2) edge node {$b$} (q21)
    (q2) edge node {$a$} (q22);
  \end{tikzpicture}
  \caption{Un esempio di NFA.}\label{fig:nfa}
\end{figure}

Una \hl{stringa $x \in L$ appartiene al linguaggio modellato} da un NFA se:

\begin{equation}
  \mhl{ \delta^\star(q_0, x) \cap F \neq \emptyset }
\end{equation}

Ossia c'è \hl{almeno una sequenza di mosse che ci porta allo stato finale}. È
possibile anche considerare $\delta^\star(q_0, x) \subseteq F$.

È importante sottolineare che \hl{gli NFA non possiedono un potere riconoscitivo
maggiore dei semplici FSA}\@. Infatti \hl{possiamo sempre sintetizzare un FSA
equivalente ad uno non deterministico}. Intuitivamente, possiamo trasformare un
automa non deterministico in uno deterministico \hl{semplicemente unendo in un
singolo stato l'insieme di arrivo della $\delta$ e mantenendo gli archi}.
Possiamo anche operare nell'altro senso, ossia costruire un automa non
deterministico a partire da uno deterministico.

\subsubsection{Automi a pila non deterministici}\label{sec:npda}

Seguendo l'evoluzione svolta nella parte che tratta gli automi deterministici,
introduciamo gli automi a pila non deterministici o NPDA\@. Il concetto di \hl{non
determinismo è analogo a quello trattato in}~\ref{sec:nfa}. L'avevamo \hl{già
incontrato quando avevamo parlato delle $\epsilon$-mosse}. La nuova funzione di
transizione è:

\begin{equation}
  \mhl{
    \delta: Q \times (I \cup \{\epsilon\}) \times \Gamma \to
      \wp_f(Q \times \Gamma^\star)
  }
\end{equation}

Il \hl{pedice $f$} dell'insieme delle parti sta per \hl{``finito''}. Infatti i possibili
sottoinsiemi di $Q \times \Gamma^\star$ sono infiniti, ma noi \hl{consideriamo
solamente quelli ottenibili nell'immagine di $\delta$, ottenendo un insieme
delle parti finito}. Un \hl{NPDA accetta se} esiste una sequenza di mosse tale che:

\begin{equation}
  \mhl{
    c_0 \vdash^\star \{c_0, \ldots, c_n \}, \quad
      c_0 = \langle q_0, \epsilon, Z_0 \rangle,
      c_1 = \langle q, \epsilon, \gamma \rangle, \ldots, q \in F
  }
\end{equation}

\hl{Quindi la relazione $\vdash$ non è più univoca!}

Una \hl{semplice costruzione} come quella in \hl{figura}~\ref{fig:npda-unione}
ci permette di \hl{costruire sempre l'unione di due NPDA} e quindi di
\hl{dimostrare la chiusura degli NPDA rispetto all'unione}. Questa \hl{proprietà
non è condivisa dagli PDA}\@. Gli \hl{NPDA non sono, però, chiusi rispetto
all'intersezione}. Ciò implica che gli NPDA hanno un potere riconoscitivo
maggiore della controparte deterministica.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[node distance=3cm,auto]
    \node[state,initial]   (q0)  []             {$q_0$};
    \node[state]           (q01) [above of=q0]  {$q_0'$};
    \node[state]           (q11) [right of=q01] {$q_1'$};
    \node[state]           (q21) [right of=q11] {$q_2'$};
    \node[state,accepting] (qf)  [right of=q21] {$q_f$};
    \node[state]           (q02) [below of=q0]  {$q_0''$};
    \node[state]           (q12) [right of=q02] {$q_1''$};
    \node[state]           (q22) [right of=q12] {$q_2''$};
    \node[state]           (q32) [right of=q22] {$q_3''$};

    \path[->]
    (q0)  edge []           node {$\epsilon,Z_0/Z_0$}      (q01)
    (q01) edge []           node {$a,Z_0/Z_0A$}            (q11)
    (q11) edge [loop above] node {$a,A/AA$}                ()
    (q11) edge []           node {$b,A/\epsilon$}          (q21)
    (q21) edge []           node {$\epsilon,Z_0/\epsilon$} (qf)
    (q21) edge [loop above] node {$b,A/\epsilon$}          ()
    (q0)  edge []           node {$\epsilon,Z_0/Z_0$}      (q02)
    (q02) edge []           node {$a,Z_0/Z_0A$}            (q12)
    (q12) edge []           node {$b,A/A$}                 (q22)
    (q12) edge [loop above] node {$a,A/AA$}                ()
    (q22) edge [bend left]  node {$b,A/\epsilon$}          (q32)
    (q32) edge []           node {$b,A/A$}                 (q22)
    (q32) edge []           node {$\epsilon,Z_0/\epsilon$} (qf);
  \end{tikzpicture}
  \caption{NPDA unione di due PDA\@. Lo NPDA riconosce $L = \{a^n b^n\} \cup
  \{a^n b^{2n}\}$. Da notare \hl{la $\epsilon$-mossa non deterministica che
  parte da $q_0$ e lega i due automi}, ognuno corrispondente ad uno dei due
  rami.}%
  \label{fig:npda-unione}
\end{figure}

Se la famiglia dei linguaggi riconosciuti dagli NPDA è \hl{chiusa rispetto a
$\cup$ ma non rispetto a $\cap$ non può esserlo rispetto al complemento a causa
delle leggi di De Morgan}.

Proviamo a dimostrare il risultato sopra. Procedendo con le \hl{stesse modalità
del caso deterministico}, possiamo ottenere una \hl{computazione che termina e
accetta il linguaggio complemento}. Se però \hl{consideriamo} il caso:

\begin{equation}
  \langle q_0, x, z_0 \rangle = c_0 \vdash^\star \{
    \langle q_1, \epsilon, \gamma_1 \rangle,
    \langle q_2, \epsilon, \gamma_2 \rangle \}, \mhl{ q_1 \in F, q_2 \notin F }
\end{equation}

La stringa \hl{$x$ è accettata anche se scambio $F$ con $Q \setminus F$},
rendendo impossibile la costruzione del complemento.

\subsubsection{Macchine di Turing non deterministiche}\label{sec:ntm}

Definiamo il concetto di \hl{non determinismo} in una macchina di Turing in modo
\hl{analogo ai casi precedenti}, ossia la capacità di poter \hl{assumere diversi
stati contemporaneamente}. Definiamo la relazione di \hl{transizione} per le NTM
(dall'inglese ``nondeterministic Turing machines''):

\begin{equation}
  \mhl{
    \delta : Q \times I \times \Gamma^k \to
      \wp(Q \times \Gamma^k \times \{ L,S,R \})
  }
\end{equation}

Non ripeteremo la definizione di \hl{configurazione, transizione, sequenza di
transizioni e accettazioni} poiché la loro \hl{definizione è invariata}.

Come nel caso degli NFA, le \hl{NTM non aggiungono capacità riconoscitiva}. Per
dimostrare ciò, costruiamo un \hl{algoritmo che permetta ad una macchina di
Turing deterministica di emulare il comportamento di una non deterministica}.
Poiché una stringa è accettata da una NTM solo se esiste un calcolo che termina
in uno stato di accettazione, rappresentiamo la \hl{computazione sotto forma di
un albero chiamato appunto albero delle computazioni}.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[node distance=1cm and 3cm,every node/.style={scale=1.3}]
    \node[] (c0)  [] {$c_0$};
    \node[] (cx)  [below left=of c0] {$c_x$};
    \node[] (cy)  [below=of c0] {$c_y$};
    \node[] (cz)  [below right=of c0] {$c_z$};
    \node[] (cb)  [below right=of cx, xshift=-2.0cm] {\color{red}$c_b$};
    \node[] (cx2) [below=of cx] {$c_x$};
    \node[] (cd)  [below left=of cx, xshift=+2.0cm] {\color{ForestGreen}$c_d$};
    \node[] (cxi) [below=of cx2] {};
    \node[] (ce)  [below=of cy] {$c_e$};
    \node[] (cyi) [below right=of cy, xshift=-2.0cm] {};
    \node[] (cf)  [below right=of cz, xshift=-2.0cm] {\color{ForestGreen}$c_f$};
    \node[] (ca)  [below=of cz] {$c_a$};
    \node[] (ci)  [below left=of cz, xshift=+2.0cm] {\color{ForestGreen}$c_i$};
    \node[] (cf2) [below=of ce] {\color{ForestGreen}$c_f$};
    \node[] (cm)  [below=of ca] {\color{red}$c_m$};

    \path[->]
    (c0)  edge         (cx)
    (c0)  edge         (cy)
    (c0)  edge         (cz)
    (cx)  edge         (cb)
    (cx)  edge         (cx2)
    (cx)  edge         (cd)
    (cy)  edge         (ce)
    (cy)  edge[dotted] (cyi)
    (cz)  edge         (cf)
    (cz)  edge         (ca)
    (cz)  edge         (ci)
    (cx2) edge[dotted] (cxi)
    (ce)  edge         (cf2)
    (ca)  edge         (cm);
  \end{tikzpicture}
  \caption{Un esempio di albero delle computazioni. In verde sono indicati le
  configurazioni di accettazione, in rosso quelle di non accettazione e con le
  linee tratteggiati i percorsi che porterebbero ad una computazione infinita.}%
  \label{fig:albero-computazioni}
\end{figure}

Per emulare una NTM con una TM ci \hl{basterà}, quindi, \hl{percorrere in
larghezza questo albero, scandendo le varie configurazioni finché non ne
troveremo una di accettazione}. Per eseguire questa operazione in alberi
tradizionali esistono degli algoritmi ben consolidati
\hl{(``breadth first search'')}.

\section{Modelli descrittivi}\label{sec:modelli-descrittivi}

\subsection{Le grammatiche}\label{sec:grammatiche}

Le grammatiche sono un \hl{altro tipo di modello che possiamo usare per
modellare i linguaggi}. A differenza degli automi, le grammatiche sono un
\hl{modello generativo} più che riconoscitivo. Una automa, infatti, legge una
stringa in ingresso, la elabora e determina se appartiene al linguaggio; una
grammatica, invece, \hl{descrive una serie di regole per generare le stringhe
del linguaggio}.

Come anticipato, possiamo \hl{intuitivamente definire le grammatiche come un
insieme di regole usato per costruire le ``frasi''}, sinonimo di stringhe,
\hl{del linguaggio} di interesse. In modo \hl{analogo alle grammatiche dei
normali meccanismi linguistici}, una grammatica formale \hl{genera le stringe}
di un linguaggio attraverso un \hl{processo di riscrittura}. Esse \hl{descrivono
un oggetto principale} (la frase) come un \hl{insieme ordinato di componenti, a
loro volta descritti come composti da altri componenti fino ad arrivare agli
elementi fondamentali dell'alfabeto considerato}.

\begin{defn}[\hl{Grammatica}]\label{def:grammatica}
  Si definisce \hl{grammatica $G$} la \hl{quadrupla $(V_n, V_t, P, S)$} dove:

  \begin{itemize}
    \item \hl{$V_n$} è l'alfabeto o \hl{vocabolario non terminale};
    \item \hl{$V_t$} è l'alfabeto o \hl{vocabolario terminale};
    \item \hl{$S \in V_n$} è l'\hl{assioma} o simbolo iniziale;
    \item L'insieme di riscrittura o delle \hl{produzioni}:

      \begin{equation}
        P \subseteq V_n^+ \times {(V_t \cup V_n)}^\star =
          \{S \to \alpha, \alpha \to \beta, \ldots\}
      \end{equation}

  \end{itemize}
\end{defn}

\begin{nota}
  Per comodità indicheremo con \hl{$V$} l'insieme \hl{$V_t \cup V_n$}.
\end{nota}

\begin{defn}[\hl{Relazione di derivazione immediata}]\label{def:deriv}
  Definiamo la \hl{relazione di derivazione immediata $\deriv[G]$} per una
  grammatica $G = (V_t, V_n, P, S)$ come \hl{$\alpha \deriv[G] \beta$ se e solo
  se} dati $\alpha \in V^+$, $\beta \in V^\star$:

  \begin{enumerate}
    \item \hl{$\alpha = \alpha_1 \alpha_2 \alpha_3$};
    \item \hl{$\beta = \alpha_1 \beta_2 \alpha_3$};
    \item \hl{$\alpha_2 \to \beta_2 \in P$}.
  \end{enumerate}
\end{defn}

\begin{nota}
  Dove non ambiguo ometteremo il pedice indicante la grammatica per alleggerire
  la notazione. Indicheremo inoltre con $\derivstar$ la chiusura riflessiva
  e transitiva di $\deriv$.
\end{nota}

\begin{defn}[Linguaggio generato da una grammatica]\label{def:ling-gen-grammatica}
  Definiamo $L(G)$ il linguaggio generato dalla grammatica
  $G = (V_t, V_n, P, S)$ un linguaggio tale che:

  \begin{equation}
    L(G) = \{ x \in V_t^\star : S \derivstar x \}
  \end{equation}
\end{defn}

\begin{esempio}
  Definiamo una prima grammatica semplice: $V_t = \{a,b,c\}$, $V_n =
  \{S,A,B,C\}$ con assioma $S$ e produzioni $P = \{S \to A, A \to Aa, A \to B,
  B \to bB, B \to C, C \to cC, C \to \epsilon \}$. Una possibile derivazione
  consiste in $S \deriv A \deriv aA \deriv aaA \deriv aaB \deriv aaC \deriv aacC
  \deriv aaccC \deriv aacccC \deriv aaccc$ oppure $S \deriv A \deriv B \deriv bB
  \deriv bC \deriv b$. Evidentemente il linguaggio modellato è $L(G) =
  \{a^*b^*c^*\}$.
\end{esempio}

\begin{esempio}
  Definiamo una grammatica un po' più elaborata: $V_t = \{a,b\}$, $V_n = \{S\}$,
  assioma $S$ e produzioni $P = \{S \to aSbS, S \to \epsilon\}$. Studiando le
  varie derivazioni si vede che il linguaggio modellato è quello delle coppie di
  $a,b$ ``ben parentetizzate''.
\end{esempio}

\begin{esempio}
  Definiamo, infine, un'ultima grammatica ancora più complessa: $V_t =
  \{a,b,c\}$, $V_n = \{S,A,B,C,D\}$, assioma $S$ e produzioni
  $P = \{S \to aACD, A \to aAC, A \to \epsilon, B \to b, CD \to BDc, CB \to BC,
  D \to \epsilon\}$. Il linguaggio generato da questa grammatica sarà $L(G) =
  \{a^n b^n c^n \}$.
\end{esempio}

\subsection{Espressività di una grammatica}\label{sec:espressivita-grammatica}

Nella \hl{sezione precedente} abbiamo mostrato con degli \hl{esempi} che è
possibile \hl{generare linguaggi che sappiamo essere riconosciuti da degli
automi} (usando la potenza minima): il primo da un FSA, il secondo da un PDA e
il terzo da una TM\@. È possibile organizzare le \hl{grammatiche} in una
\hl{gerarchia in base al loro potere generativo} e inoltre \hl{è possibile che
ci sia una relazione tra le potenze riconoscitive dei vari autonomi e le
grammatiche}?

Alla \hl{prima domanda} ha già risposto \hl{Noam Chomsky con la sua gerarchia
delle grammatiche}. Essa è divisa in \hl{4 classi} a seconda delle
\hl{limitazioni imposte sulla forma delle produzioni} $\alpha \to \beta$
(tabella~\ref{tab:gerarchia-grammatiche}).

\begin{table}[htb]
  \centering
  \begin{tabular}{lll}
    \toprule
    Tipo & Nome            & Forma delle produzioni \\
    \midrule
    0    & \hl{Non limitate}    & --- \\
    1    & \hl{Contestuali}     & \hl{$|\alpha| \leq |\beta|$} \\
    2    & \hl{Non contestuali} & \hl{$|\alpha| = 1$} \\
    3    & \hl{Regolari}        & \hl{$|\alpha| = 1 \vee \beta \in V_t . V$} \\
    \bottomrule
  \end{tabular}
  \caption{La gerarchia delle grammatiche di Chomsky.}%
  \label{tab:gerarchia-grammatiche}
\end{table}

Utilizzando questa gerarchia una \hl{grammatica più potente genera tutti i
linguaggi di una meno potente}, andando a creare una situazione come in
figura~\ref{fig:gerarchia-grammatiche}. \hl{L'inclusione della famiglia di
grammatiche meno potenti in quella di grammatiche più potenti è però stretta}?

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \draw[line width=0.4mm] (5,5) ellipse (5cm and 3cm)       node (0) {};
    \draw[line width=0.4mm] (5,5) ellipse (3.75cm and 2.25cm) node (1) {};
    \draw[line width=0.4mm] (5,5) ellipse (2.5cm and 1.5cm)   node (2) {};
    \draw[line width=0.4mm] (5,5) ellipse (1.25cm and 0.75cm) node (3) {};

    \node at (5, 2.25cm) {Non limitate};
    \node at (5, 3cm)    {Contestuali};
    \node at (5, 4cm)    {Non contestuali};
    \node at (5, 4.5cm)  {Regolari};
  \end{tikzpicture}
  \caption{Diagramma di Venn rappresentante la gerarchia delle grammatiche.}%
  \label{fig:gerarchia-grammatiche}
\end{figure}

\subsection{Legame tra grammatiche e automi}\label{sec:gramamtiche-automi}

\hl{Generalizziamo ora l'osservazione della corrispondenza grammatica-automa}
fatta negli esempi in~\ref{sec:grammatiche}.

\paragraph{Grammatiche regolari} Come si potrebbe intuire, i \hl{linguaggi
generati fa grammatiche regolari coincidono con i linguaggi riconosciuti dagli
FSA (i linguaggi regolari)}. Poiché automa e grammatica sono in relazione di
equivalenza \hl{possiamo passare da un all'altro} (le due proposizioni si
dimostrano con una semplice induzione):

\begin{prop}[\hl{Passaggio da FSA a grammatica}]
  Per ottenere la grammatica equivalente ad un FSA\@:

  \begin{itemize}
    \item Poniamo \hl{$V_n = Q, V_t = I, S = \langle q_0 \rangle$};
    \item \hl{Per ogni $\delta(q, i) = q'$} diciamo che
      \hl{$\langle q \rangle \to i \langle q' \rangle \in P$}. Se
      \hl{$q' \in F$} aggiungiamo anche \hl{$\langle q \rangle \to i \in P$}.
  \end{itemize}
\end{prop}

\begin{prop}[\hl{Passaggio da grammatica a NFA}]
  Per ottenere lo NFA corrispondente alla grammatica:

  \begin{itemize}
    \item Poniamo \hl{$Q = V_n \cup \{q_f\}, I = V_t, q_0 = S, F = \{q_f\}$};
    \item Se \hl{$A \to bC \in P$} allora \hl{$\delta(A,b) = C$}. Invece se
      \hl{$A \to b \in P$} allora \hl{$\delta(A, b) = q_f$}.
  \end{itemize}
\end{prop}

\paragraph{Grammatiche non contestali} I linguaggi generati dalle
\hl{grammatiche libere dal contesto} coincidono con quelli riconosciuti dagli
\hl{NPDA}\@. La \hl{dimostrazione} di questa affermazione \hl{non è affatto
banale}, perciò diamo \hl{solamente l'intuizione} con la
\hl{figura}~\ref{fig:esempio-grammatica-to-npda}.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[node distance=3cm,auto]
    \node[state,initial]   (q0) []            {$q_0$};
    \node[state]           (q1) [right=of q0] {$q_1$};
    \node[state,accepting] (q2) [right=of q1] {$q_2$};

    \path[->]
    (q0) edge []                      node {$\epsilon, Z_0/Z_0S$}     (q1)
    (q1) edge []                      node {$\epsilon, Z_0/\epsilon$} (q2)
    (q1) edge [out=45,in=75,loop]     node [above,xshift=+0.25cm] {$\epsilon, S/\underline{b}S\underline{a}$} ()
    (q1) edge [out=105,in=135,loop]   node [above,xshift=-0.25cm] {$\epsilon, S/\underline{ba}$} ()
    (q1) edge [out=-105,in=-135,loop] node {$a,\underline{a}/\epsilon$} ()
    (q1) edge [out=-45,in=-75,loop]   node {$b,\underline{b}/\epsilon$} ();
  \end{tikzpicture}
  \caption{NPDA corrispondente a $(\{S\}, \{a,b\} \{S \to aSb, S \to ab\}, S)$.}%
  \label{fig:esempio-grammatica-to-npda}
\end{figure}

\paragraph{Grammatiche non ristrette} Le \hl{grammatiche non ristrette} sono
equivalenti alla \hl{macchine di Turing}. \hl{Costruire una macchina di Turing
(a nastro singolo) non deterministica a partire da una grammatica non ristretta}
$G$ non è assai difficile: con la \hl{stringa} $x$ posizionata sul \hl{nastro
d'ingresso} eseguiamo un \hl{ciclo}:

\begin{enumerate}
  \item \hl{Scandiamo} il nastro \hl{finché non troviamo una parte destra}
    $\beta$ di una \hl{produzione} $\alpha \to \beta$ della grammatica;
  \item \hl{Quando se ne trova una}, scegliendola non deterministicamente, essa
    \hl{viene sostituita} dalla corrispondente \hl{parte sinistra} $\alpha$.
\end{enumerate}

Questo ciclo è chiamato \hl{``processo di riduzione''} della stringa. Per come
abbiamo strutturato il ciclo abbiamo che \hl{$\alpha \deriv \beta$ se e solo se
$\langle q, Z_0, \alpha \rangle \vdash^\star \langle q, Z_0, \beta \rangle$}. Se
e quando il \hl{contenuto del nastro d'ingresso} diviene \hl{l'assioma $S$}
della grammatica, la \hl{stringa} d'ingresso verrà \hl{accettata}. Il
procedimento descritto \hl{non garantisce che la macchina di Turing generata
riesca a concludere sempre la computazione}.

\subparagraph{Grammatica equivalente a TM} Eseguire il processo inverso è più
complicato. Poiché una \hl{grammatica non può ricevere una stringa in input e
può manipolare solo gli elementi non terminali di una stringa}, dobbiamo fare in
modo che essa \hl{generi tutte le possibili stringhe del tipo $x \termsep X$}
con $x \in V_t^\star$, $\termsep \in V_n$ un separatore  e \hl{$X$ una ``copia''
di $x$ fatta solo di elementi non terminali}. Il nostro \hl{obiettivo} è
ottenere una \hl{derivazione $x \termsep X \derivstar x$ se e solo se $x$ è
accettata dalla TM}\@. Innanzitutto la grammatica deve possedere le
\hl{produzioni che generano $x \termsep X$} (consideriamo $V_t = \{a,b\}$):

\begin{align}
  \mhl{S \to SA'A, S \to SB'B, S \to \termsep} & \quad
    \text{(genero coppie di simboli)}\\
  \mhl{AA' \to A'A, BA' \to A'B} & \quad \text{(scorro le $A'$ a sx)}\\
  \mhl{AB' \to B'A, BB' \to B'B} & \quad \text{(scorro le $B'$ a sx)}\\
  \mhl{\termsep A' \to a\termsep, B'\termsep \to b\termsep} & \quad
    \text{(quando scorro attraverso $\termsep$ trasformo)}
\end{align}

Dobbiamo ora \hl{simulare ogni possibile mossa della macchina di Turing con una
derivazione}. Consideriamo una \hl{macchina di Turing con configurazione pari a
quella rappresentata in figura}~\ref{fig:grammatica-mt-stato}. Tale
configurazione è \hl{rappresentata dalla stringa $\termsep\alpha BqAC\beta$}. In
base ai valori assunti da $\delta$ \hl{possiamo agire in 3 diversi modi}:

\begin{enumerate}
  \item \hl{$\delta(q, A) = \langle q', A', R \rangle$} aggiungo \hl{$qA \to
    A'q'$} alle produzioni;
  \item \hl{$\delta(q, A) = \langle q', A', S \rangle$} aggiungo \hl{$qA \to
    q'A'$} alle produzioni;
  \item \hl{$\delta(q, A) = \langle q', A', L \rangle$} aggiungo \hl{per ogni
    $B$} nell'alfabeto della TM la produzione \hl{$BqA \to q'B'A'$}.
\end{enumerate}

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \draw[black,thin] (0,0) grid (13, 1);

    \node[xshift=-0.05cm] at (0.5 ,0.5) [] ()  {\huge $\not{b}$};
    \node[xshift=-0.05cm] at (1.5 ,0.5) [] ()  {\huge $\not{b}$};
    \node[xshift=-0.05cm] at (2.5 ,0.5) [] ()  {\huge $\not{b}$};
    \node[xshift=-0.05cm] at (3.5 ,0.5) [] ()  {\huge $\not{b}$};
    \node[]               at (4.5 ,0.5) [] ()  {\huge $\alpha$};
    \node[]               at (5.5 ,0.5) [] ()  {\huge $B$};
    \node[]               at (6.5 ,0.5) [] (A) {\huge $A$};
    \node[]               at (7.5 ,0.5) [] ()  {\huge $C$};
    \node[]               at (8.5 ,0.5) [] ()  {\huge $\beta$};
    \node[xshift=-0.05cm] at (9.5 ,0.5) [] ()  {\huge $\not{b}$};
    \node[xshift=-0.05cm] at (10.5,0.5) [] ()  {\huge $\not{b}$};
    \node[xshift=-0.05cm] at (11.5,0.5) [] ()  {\huge $\not{b}$};
    \node[xshift=-0.05cm] at (12.5,0.5) [] ()  {\huge $\not{b}$};

    \node[anchor=north,draw] at (6.5,-1) [] (q) {\Huge $q$};

    \draw[-{Stealth[length=3mm,width=3mm]}] (q) -- (6.5,0);
  \end{tikzpicture}
  \caption{Configurazione della macchina di Turing in esame.}%
  \label{fig:grammatica-mt-stato}
\end{figure}

Per come abbiamo scritto le produzioni, i \hl{due stati in
figura}~\ref{fig:grammatica-mt-stati} sono in \hl{relazione se e solo se
$\termsep\alpha BqAC\beta \deriv \termsep\alpha BA'q'C\beta$}. La costruzione
della grammatica va, infine, \hl{completata aggiungendo le regole che cancellano
tutto ciò che sta a destra del separatore $\termsep$ (separatore incluso) se e
solo se la configurazione della macchina di Turing è accettante}, ad esempio
$\termsep\alpha Bq_f AC\beta$.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \draw[black,thin] (0,0) grid (5, 1);
    \draw[black,thin] (6,0) grid (11, 1);

    \node[] at (0.5,0.5) [] () {\huge $\alpha$};
    \node[] at (1.5,0.5) [] () {\huge $B$};
    \node[] at (2.5,0.5) [] () {\huge $A$};
    \node[] at (3.5,0.5) [] () {\huge $C$};
    \node[] at (4.5,0.5) [] () {\huge $\beta$};

    \node[] at (6.5 ,0.5) [] () {\huge $\alpha$};
    \node[] at (7.5 ,0.5) [] () {\huge $B$};
    \node[] at (8.5 ,0.5) [] () {\huge $A$};
    \node[] at (9.5 ,0.5) [] () {\huge $C$};
    \node[] at (10.5,0.5) [] () {\huge $\beta$};

    \node[anchor=north,minimum size=1cm,draw] at (2.5,-1) [] (q)  {\huge $q$};
    \node[anchor=north,minimum size=1cm,draw] at (8.5,-1) [] (q') {\huge $q'$};

    \node[anchor=north] at (5.5, -1) [] () {\huge $\vdash$};

    \draw[-{Stealth[length=3mm,width=3mm]}] (q)  -- (2.5,0);
    \draw[-{Stealth[length=3mm,width=3mm]}] (q') -- (9.5,0);
  \end{tikzpicture}
  \caption{Due configurazioni della macchina di Turing in esame che sono legate
  dalla transizione.}\label{fig:grammatica-mt-stati}
\end{figure}

\paragraph{Grammatiche monotone} Possiamo notare che le \hl{produzioni si
accorciano man mano che vengono ``espanse''}. Quindi se \hl{costruiamo} una
\hl{macchina di Turing equivalente} utilizzando i \hl{metodi descritti} in
questa sezione, essa \hl{userà al massimo la memoria occupata inizialmente dalla
stringa} in ingresso $x$. Possiamo, allora, \hl{limitare la potenza} della
nostra macchina fornendogli una \hl{memoria di dimensione finita}, ottenendo
così la macchina a minore potenza equivalente alle grammatiche monotone. Un
\hl{automa così definito è detto automa lineare}.

\section{Altri modelli utili}\label{sec:altri-modelli}

Degli \hl{altri modelli molto usati per descrivere i linguaggi sono i pattern e
le espressioni regolari}. Non andremo per nulla del dettaglio e ci limiteremo a
\hl{enunciare solo la definizione e al massimo qualche proprietà}.

\subsection{I pattern}\label{sec:pattern}

\begin{defn}[Sistema di pattern]
  Un sistema di pattern è una tripla $\langle A, V, p \rangle$ dove $A$ è un
  alfabeto, $V$ è un insieme di variabili disgiunto con $A$ e $p$ è una stringa
  su $A \cup V$ detta pattern.
\end{defn}

\subsection{Le espressioni regolari}\label{sec:regex}

Le espressioni regolari, o regex, le abbiamo \hl{già incontrate nella
sezione}~\ref{sec:fsa} e definite con la definizione~\ref{def:regex}. Esse
seguono la \hl{stessa idea generale dei pattern, ma hanno un potere espressivo
molto diverso}. Le regex hanno la\hl{ stessa espressività dei linguaggi regolari
e delle grammatiche regolari}. Faremo solo una \hl{dimostrazione veloce di un
solo senso dell'implicazione} in quanto l'altro è più complesso e oggetto di
altri corsi:

\begin{proof}[$\mathcal{L}\in\mathbf{RE} \implies \mathcal{L}\in\mathbf{REG}$]%
  \label{proof:equiv-re-reg-1}
  Guardando la definizione di espressione regolare si può notare che ciascuno
  dei casi base definisce un linguaggio regolare. Poiché le regex sono anche
  chiuse rispetto alle operazioni definite tutte le regex generalo linguaggi
  regolari.
\end{proof}

Con un semplice \hl{esempio} si può anche vedere che \hl{la famiglia dei
linguaggi descrivibili da una regex non coincide con quella dei linguaggi
generati dai sistemi di pattern}:

\begin{itemize}
  \item $L = \{xx : x \in \{0,1\}\}$ non è un linguaggio regolare, ma può essere
    descritto con il pattern $\langle \{0,1\}, \{x\}, xx\rangle$;
  \item Il linguaggio generato da $0^* 1^*$ è, ovviamente, regolare ma non è
    esprimibile con un pattern.
\end{itemize}

\hl{Quindi non è contenimento tra le due famiglie. Diciamo che esse sono non
confrontabili}.

\section{Modelli dichiarativi}\label{sec:modelli-dichairativi}

\subsection{La logica}\label{sec:logica}

La logica è un \hl{elemento fondamentale dell'informatica}. Esistono molti
linguaggi logici, ognuno con diversi livelli di espressività e utilizzo.
Nell'ambito di questo corso \hl{useremo la logica del primo ordine} con due
scopi: \hl{definizione di linguaggi e specifica della proprietà dei programmi}.
La logica, infatti, ci \hl{permetterà} sia di \hl{definire i linguaggi
specificandone le proprietà} e di esprimere le condizioni che le nostre
computazioni devono rispettare.

\begin{esempio}
  Consideriamo il linguaggio $\{a^n b^n : n \geq 0\}$, come possiamo descriverlo
  con una formula del primo ordine? Una forma è:

  \[
    \forall x (x \in L \iff \exists n (N \geq 0 \land a = a^n . b^n))
  \]
  Definendo opportunamente i predicati $\in L$, $\geq$ e $=$ e le funzioni di
  concatenazione ed elevamento a potenza.
\end{esempio}

\begin{esempio}
  Caratterizziamo un altro linguaggio: $L_1 = a^*b^*$. Uno strumento utile per
  caratterizzare un linguaggio con la logica è quello di pensare in modo
  induttivo. Ragionando induttivamente possiamo ricavare una forma logica
  corrispondente a $L_1$:

  \[
    \forall x (x \in L_1 \iff (x = \epsilon) \lor
    \exists y (x = ay \land y \in L_1) \lor \exists y (x = yb \land y \in L_1))
  \]
\end{esempio}

\begin{esempio}
  Consideriamo il linguaggio $L_2 = a^* b^* c^*$. Possiamo vederlo come $a^* b^*
  . b^* c^*$ dove entrambi questi sotto-linguaggi, chiamiamoli $L_1$ ed $L_3$,
  hanno struttura simile a $L_1$. Quindi una stringa appartiene ad $L_2$ se
  appartiene ad uno dei due linguaggi oppure se la prima sotto-stringa inizia
  con `a' seguita da un suffisso $y$ composto sia da `a' e `b' e la seconda
  sotto-stringa è composta da un prefisso $y$ di `b' e `c' seguito da una c. Una
  formula del primo ordine che caratterizza la seguente è:

  \[
    \forall x (x \in L_2 \iff x\in L_1 \lor x \in L_3 \lor
      \exists y (x = ay \land (y \in L_2 \lor y \in L_3)) \lor
      \exists y (x = yc \land (y \in L_2 \lor y \in L_3)))
  \]

  Si può ridurre questa formula? Certo. Mantenendo la struttura come
  concatenazione possiamo scrivere:

  \[
    \forall x (x \in L_2 \iff \exists z \exists y
     (y \in L_1 \land z \in L_3 \land x = y.z))
  \]
\end{esempio}

\begin{esempio}
  Consideriamo l'ulteriore esempio del linguaggio:

  \[
    L_4 = \{x \in \{a,b\} : \text{ numero di } a \text{ è uguale al numero di } b\}
  \]

  Per indicare ``il numero di'' introduciamo la funzione di arietà 2 $\#(x,a)$ che
  indica il numero di occorrenze del carattere `a' nella stringa $x$ così
  formalmente definita:

  \begin{align*}
    & \forall x \forall y ((x = \epsilon \implies \#(x,a)=0) \land \\
      & \quad (x = a.y \implies \#(x,a) = \#(y,a)+1) \land \\
      & \quad (x=b.y \implies \#(x,a) = \#(y,a))) \land \\
      & \quad \forall x \forall y ((x = \epsilon \implies \#(x,b)=0) \land \\
      & \quad (x = b.y \implies \#(x,b) = \#(y,b)+1) \land \\
      & \quad (x=a.y \implies \#(x,b) = \#(y,b)))
  \end{align*}

  Il nostro linguaggio in formula del primo ordine sarà
  $\forall x (x \in L_4 \iff \#(x,a)=\#(x,b))$.
\end{esempio}

\subsubsection{Precondizioni e postcondizioni}

Approfondiamo qui un uso della logica che ci interessa ai fini del corso: la
specifica di condizioni che il nostri programma deve rispettare. Si preferisce
specificare le condizioni iniziali e finali dell'ambiente invece di specificare
il funzionamento specifico del programma perché di solito è più importante e
flessibile definire correttamente cosa un programma faccia rispetto a definire
una specifica implementazione. Infatti diverse implementazione possono
rispettare le stesse precondizioni e postcondizioni, portando a programmi
ottimizzati per diverse situazioni (uso minore di memoria, velocità di
esecuzione maggiore, uso di particolari costrutti ecc\ldots) che però sono
funzionalmente equivalenti.

Per la specifica delle condizioni useremo la notazione di Hoare:

\begin{equation}
  \begin{aligned}
    & \{ \mathit{Precondizione} \} \\
    & \mathit{Programma} \\
    & \{ \mathit{Postcondizione} \}
  \end{aligned}
\end{equation}

Se valgono le precondizioni, affinché il programma sia considerato corretto
dopo l'esecuzione dovranno valere le postcondizioni. Nella pratica la condizioni
possono essere definite tramite linguaggio naturale nei commenti, funzioni di
\texttt{assert} o linguaggi ad-hoc. Nel nostro contesto teorico le definiremo
usando la logica del primo ordine arricchita dei necessari predicati.

\begin{esempio}
  Sia un programma $P$ che implementa la ricerca di un
  elemento $x$ in un array ordinato di $n$ elementi. La precondizione sarà che
  l'array sia ordinato e la postcondizione che la variabile logica
  $\mathtt{found}$ debba essere vera se e solo se l'elemento $x$ esiste
  nell'array $a$. Notiamo che non ci interessa che tipo di algoritmo è
  implementato da $P$. Usando la notazione di Hoare, scriveremo:

  \[
    \begin{aligned}
      & \{ \forall i (1 \leq i \leq n-1 \implies a[i] \leq a[i+1] ) \} \\
      & P \\
      & \{ \mathtt{found} \iff \exists i (1 \leq u \leq n \land a[i] = x ) \}
    \end{aligned}
  \]

  Dove abbiamo definito ``implicitamente'' $a[i]$ l'$i$-esimo elemento di $a$.
\end{esempio}

\subsection{La logica monadica del primo ordine}\label{sec:mfo}

Consideriamo un \hl{frammento di logica del primo ordine} che ci permette di
\hl{descrivere parole su un alfabeto $I$}. Chiamiamo questa logica \hl{``logica
monadica del primo ordine''} o MFO\@. La sintassi di questa logica è la
seguente:

\begin{defn}[\hl{Sintassi della MFO}]\label{def:mfo-sintassi}
  Una \hl{formula} $\phi$ fa parte della \hl{logica monadica del primo ordine}
  se:

  \begin{equation}
    \mhl{
    \phi\coloneqq a(x) \mid x < y \mid\neg\phi\mid\phi\land\phi\mid\forall x(\phi)
    }
  \end{equation}

  Con:
  \begin{enumerate}
    \item \hl{$a \in I$ e $a(x)$} un predicato per ogni singolo simbolo
      dell'alfabeto;
    \item $<$ la relazione di minore;
    \item $\mathbb{N}$ dominio delle variabili.
  \end{enumerate}
\end{defn}

Data una \hl{parola $w \in I^+$} ed un \hl{simbolo $a\in I$, $a(x)$} è \hl{vera
se e solo se l'$x$-esimo simbolo di $w$ è $a$} (il primo simbolo di $w$ ha
indice 0). Per esempio una formula che è vera su tutte e solo le parole il cui
primo simbolo esiste ed è `a' è: $\exists x(x = 0 \land a(x))$. Considereremo
solo parole non vuote nelle definizioni poiché alleggerisce molto la notazione.
I concetti possono, però, essere estesi anche a parole vuote.

\hl{Il resto dei predicati consueti viene definito con}:

\begin{align}
  \mhl{\phi_1 \lor \phi_2}     & \mhl{\triangleq \neg(\neg\phi_1\land\neg\phi_2)} \\
  \mhl{\phi_1 \implies \phi_2} & \mhl{\triangleq \neg\phi_1\lor\phi_2} \\
  \mhl{\exists x(\phi)}        & \mhl{\triangleq \neg \forall x(\neg\phi)} \\
  \mhl{x = y}                  & \mhl{\triangleq \neg(x<y) \land \neg(y<x)} \\
  \mhl{x \leq y}               & \mhl{\triangleq \neg(y<x)}
\end{align}

Possiamo anche definire \hl{altri predicati utili}:

\begin{itemize}
  \item La \hl{costante $0$} tale che \hl{$x = 0 \triangleq \forall
    y(\neg(y<x))$};
  \item Il predicato \hl{``$y$ successore di  $x$''}
    \hl{$\succc(x,y) \triangleq x<y \land \neq\exists z(x<z \land z<y)$};
  \item Le \hl{costanti $1,2,3,\ldots$ come i successori di $0,1,2,\ldots$}.
\end{itemize}

Per comodità possiamo \hl{definire} altre \hl{abbreviazioni} del tipo
\hl{$y=x+1$ per indicare $\succc(x,y)$}, \hl{$y=x+k$} per indicare lo
\hl{spiazzamento di $k$}, \hl{$y=x-1$ per indicare $\succc(y,x)$},
\hl{analogamente $y=x-k$} e \hl{$\last(x)$ per indicare l'ultima posizione}.

Per definire la \hl{semantica} introduciamo la definizione di \hl{assegnamento}.

\begin{defn}[\hl{Assegnamento}]\label{def:mfo-assegnamento}
  Siano \hl{$w\in I^+$ e $V_1$ l'insieme delle variabili}. Un \hl{assegnamento}
  è una \hl{funzione $\nu_1 : V_1 \to \{0,1,\ldots,|w|-1\}$} tale che:

  \begin{itemize}
    \item \hl{$w,\nu_1 \models a(x)$} se e solo se \hl{$w = uav$} e
      $|u| = \nu_1(x)$;
    \item \hl{$w,\nu_1 \models x < y$} se e solo se \hl{$\nu_1(x) < \nu_1(y)$};
    \item \hl{$w,\nu_1 \models \neg\phi$} se e solo se \hl{non
      $w\nu_1 \models \phi$};
    \item \hl{$w,\nu_1 \models \phi_1 \land \phi_2$} se e solo se
      \hl{$w,\nu_1 \models \phi_1$ e $w,\nu_1 \models \phi_2$};
    \item \hl{$w,\nu_1 \models a(x)$} se e solo se \hl{$w,\nu_1' \models \phi$
      per ogni $v_1'$ con $\nu_1'(y) = \nu_1(y)$, $y \neq x$}.
  \end{itemize}
\end{defn}

Possiamo così definire il \hl{linguaggio corrispondente ad una formula chiusa
$\phi$ come}:

\begin{equation}
  \mhl{ L(\phi) = \{w\in I^+ : \exists\nu_1 (w, \nu_1 \models \phi)\} }
\end{equation}

\subsubsection{Proprietà}\label{sec:mfo-proprietà}

I \hl{linguaggi esprimibili tramite MFO} sono \hl{chiusi rispetto a unione,
intersezione e complemento come potrebbe far intendere la buona definizione di
$\land$, $\lor$ e $\neg$}. Essi però \hl{non sono chiusi rispetto alla star di
Kleene}. I linguaggi esprimibili da MFO sono perciò detti \hl{``star-free''}.

Un altra peculiarità delle formule MFO è il fatto che siano \hl{meno potenti
degli FSA\@}. Essi infatti \hl{non possono rappresentare un particolare
linguaggio regolare $L_p$ fatto di tutte e sole le parole di lunghezza pari su
un alfabeto di una singola lettera}. Quindi possiamo dire che i \hl{linguaggi
``star-free'' sono un sottoinsieme dei linguaggi regolari}.

\subsection{Logica monadica del secondo ordine}\label{sec:mso}

Per permettere alla logica \hl{MFO} di avere lo stesso potere espressivo degli
FSA bisogna \hl{aumentarne la potenza}. Un modo per fare ciò è \hl{permettere di
quantificare i predicati monadici}. Ammettiamo quindi \hl{formule del tipo
$\exists X(\phi)$ dove $X$ è una variabile il cui dominio è l'insieme dei
predicati monadici}.

In questo caso la \hl{semantica} prevede anche un \hl{secondo assegnamento}
\hl{$\nu_2: V_2 \to \wp(\{0,1,\ldots,|w|-1\})$} con le seguenti \hl{regole
aggiuntive}:

\begin{itemize}
  \item \hl{$w, \nu_1, \nu_2 \models X(x)$} se solo se
    \hl{$\nu_2(x) \in \nu_2(X)$};
  \item \hl{$w, \nu_1, \nu_2 \models \exists X(\phi)$} se e solo se
    \hl{$w, \nu_1, \nu_2' \models \phi$ per qualche $\nu_2'$ con
    $\nu_2'(Y) = \nu_2(Y)$, $Y \neq X$}.
\end{itemize}

\begin{esempio}
  Possiamo allora descrivere il linguaggio $L_p$ visto precedentemente
  (perdonate la lisp-syntax ma la formula era troppo lunga):

  \begin{align*}
    & \exists P( \\
    &   \quad \forall x(a(x) \land \neg P(0) \land \\
    &   \quad \quad \forall y (y = x+1 \implies (\neg P(x) \iff P(y))) \land \\
    &   \quad \quad (\last(x) \implies P(x)))) \\
  \end{align*}

  Dove $P$ indica un insieme di posizioni dispari.
\end{esempio}

\subsubsection{Trasformare da MSO a FSA}\label{sec:mso-fsa}

In generale, grazie alle quantificazioni del secondo ordine, \hl{per ogni FSA},
è possibile scrivere una \hl{formula MSO equivalente}. In modo non rigoroso
possiamo procedere così. Innanzitutto \hl{ogni predicato monadico quantificato
corrisponde a ciascuno stato}. \hl{Se} nell'automa \hl{non è possibile essere in
diversi stati contemporaneamente (determinismo), mettiamo i vari stati in
esclusione mutua}. Infine \hl{codifichiamo} tutte le \hl{transizioni} usando
\hl{$last(x)$} e \hl{$\neg last(x)$} per \hl{caratterizzare gli stati finali e
non finali} rispettivamente.

Si può eseguire anche la \hl{trasformazione inversa} tramite un processo ben
definito, il \hl{teorema di Büchi-Elgot-Trakhtenbrot}. Noi \hl{non lo vedremo}
poiché è un teorema molto tecnico.

Le due trasformazioni descritte ci premettono, quindi, di \hl{sancire
l'equivalenza tra FSA e MSO}\@.

\section{Teoria della computazione}\label{sec:teoria-computazione}

Automi, grammatiche e altri formalismi possono essere considerati modelli
meccanici per risolvere problemi matematici. Alcuni di questi sono più potenti
di altri, ad esempio le TM riconosco una famiglia di linguaggi che contiene
linguaggi non riconoscibili dai PDA\@. Abbiamo inoltre affermato che nessun
formalismo sarà più potente di una macchina di Turing. \hl{Questi modelli
possono, però, catturare l'essenza di un generico solutore meccanico?} Inoltre
se un \hl{problema è stato formalizzato adeguatamente, possiamo sempre
risolverlo mediante dispositivi meccanici?}

\subsection{Formalizzazione di problema}

Prima di parlare di risoluzione e risolvibilità, dobbiamo definire bene il
concetto di problema. Nei precedenti capitoli abbiamo \hl{già incontrato la
formalizzazione di un problema come riconoscimento} ($x \in L$) \hl{o
traduzione} ($y = \tau(x)$) \hl{di stringhe} su un alfabeto. Con questi due
formalismi \hl{è possibile descrivere tutti problemi con dominio numerabile}.
Infatti un \hl{problema con dominio numerabile può essere sempre ricondotto al
calcolo di una funzione $f: \mathbb{N} \to \mathbb{N}$}. Si può inoltre
dimostrare che riconoscimento e traduzione possono ridotte l'una all'altra:

\begin{thm}[Equivalenza tra traduzione e riconoscimento]\label{thm:equiv-trad-ric}
  Se una macchina può \hl{risolvere tutti i problemi del tipo $y = \tau(x)$}
  allora può \hl{risolvere anche i problemi del tipo $x \in L$ e viceversa}.
\end{thm}
\begin{proof}[Dimostrazione teorema~\ref{thm:equiv-trad-ric}]
  Dimostriamo i due versi dell'implicazione separatamente:

  \begin{description}
    \item[$\implies$] Se ho una macchina che può risolvere tutti i problemi
      della forma $y = \tau (x)$ e voglio usarla per risolvere il problema $x
      \in L$ è sufficiente definire $\tau(x) = 1$ se $x \in L$ e $0$ altrimenti.
    \item[$\impliedby$] Se ho una macchina che può risolvere tutti i problemi
      della forma $x \in L$, posso definire un linguaggio $L_{\tau} = \{
      x\termsep y : y = \tau(x) \}$. Per una $x$ fissata, posso enumerare
      tutte le possibili stringhe $y$ sull'alfabeto di uscita e per ognuna di
      esse posso chiedere alla macchina se $x\termsep y \in L_{\tau}$.
  \end{description}
\end{proof}

La \hl{classe dei problemi che possono essere risolti da una TM è indipendente
dall'alfabeto scelto, sempre che ci siano almeno due simboli}. Prima abbiamo
detto che le macchine di Turing sono il più potente formalismo di calcolo
automatico. Questa è la cosiddetta tesi di Church. Esso non è un teorema poiché
andrebbe verificato ogni volta che qualcuno inventa un nuovo modello
computazionale. Nonostante ciò è largamente accettato dagli studiosi come
teorema.

\begin{prop}[\hl{Tesi di Church}]\label{thm:tesi-church-1}
  \hl{Se un problema è umanamente calcolabile, allora esisterà una macchina di
  Turing in grado di risolverlo}.
\end{prop}

Tutti i \hl{modelli che abbiamo esaminato sono discreti}, in accordo con la
tecnologia digitale nel quale vengono impiegati. Inoltre \hl{data una TM, si può
costruire un programma in Java, C o FORTRAN che la simula e viceversa}. Questi
\hl{linguaggi di programmazione} hanno, quindi, la stessa espressività delle
macchine di Turing e \hl{vengono detti ``Turing-completi''}. Esistono anche dei
linguaggi che non sono Turing-completi come ad esempio il Datalog.

\subsection{Gli algoritmi}

Introduciamo un altro concetto fondamentale per l'informatica: l'algoritmo.

\begin{defn}[Algoritmo (intuitivo)]\label{def:alg-non-formale}
  Con algoritmo intendiamo una \hl{procedura per risolvere problemi} mediante un
  dispositivo di \hl{calcolo automatico}.
\end{defn}

Gli \hl{algoritmi} godono di alcune \hl{importanti proprietà}, anche queste
enunciate in modo informale.

\begin{prop}
  Sia \hl{$A$ un algoritmo} in esecuzione su una \hl{macchina $\mathcal{M}$
  dotata di un processore meccanico (digitale) e di una memoria utilizzabile dal
  processore in modo arbitrario}. Si ha che:

  \begin{enumerate}
    \item La \hl{sequenza di istruzioni} di $A$ dev'essere \hl{finita};
    \item \hl{Qualunque istruzione} di $A$ dev'essere \hl{eseguibile dal
      processore} meccanico di $\mathcal{M}$;
    \item La \hl{computazione è discreta}, ossia l'informazione è codificata
      digitalmente e la computazione procede attraverso passi discreti;
    \item $A$ è \hl{eseguito in modo deterministico};
    \item \hl{Non c'è limite} sulla quantità di \hl{dati in ingresso e in
      uscita};
    \item \hl{Non c'è limite} sulla quantità di \hl{memoria richiesta} per
      effettuare una computazione;
    \item \hl{Non c'è limite} sul \hl{numero di passi discreti richiesti} per
      effettuare una computazione.
  \end{enumerate}
\end{prop}

Possiamo \hl{descrivere ogni problema calcolabile tramite un sistema di calcolo
automatico con un algoritmo} e quindi, per la tesi di Church, \hl{ogni algoritmo
potrà essere eseguito da una macchina di Turing}. Possiamo allora
\hl{riformulare} la tesi di Church come:

\begin{prop}[Tesi di Church]\label{thm:tesi-church-2}
  \hl{Ogni algoritmo si può codificare con un macchina di Turing}.
\end{prop}

Quali sono però i problemi che possiamo risolvere tramite un algoritmo? Una
risposta un po' banale è: quelli risolvibili dalle macchine di Turing.

\subsection{Enumerazione algoritmica}\label{sec:enum-alg}

\begin{defn}[\hl{Insieme enumerabile algoritmicamente}]\label{def:enum-alg}
  Un \hl{insieme} $S$ può essere \hl{enumerato algoritmicamente} se possiamo
  trovare una \hl{biezione $E$ tra $S$ $\mathbb{N}$ calcolabile con un
  algoritmo}.
\end{defn}

Dimostriamo che l'insieme delle macchine di Turing è un insieme enumerabile
algoritmicamente:

\begin{prop}[\hl{Enumerazione delle TM}]\label{thm:enum-alg-TM}
  L'\hl{insieme} di \hl{tutte le macchine di Turing} a \hl{nastro singolo} con
  alfabeto \hl{$A = \{ 0,1,\not{b}\}$} è un \hl{insieme enumerabile
  algoritmicamente}.
\end{prop}
\begin{proof}
  Consideriamo le TM con solo due stati. Iniziamo con il calcolare la
  cardinalità dell'insieme di tutte le possibili funzioni di transizione
  realizzabili. Sappiamo che $\delta$ ha forma:

  \[
    \delta : Q \times A \to Q \times A \times \{R,L,S\} \cup \{\bot\}
  \]

  Sfruttando un risultato della teoria insiemistica che ci dice che il numero di
  funzioni $f: D \to R$ è $|R|^{|D|}$, otteniamo che il numero di funzioni sarà:

  \[
    {[|Q|*|A|*(3+1)]}^{2*3} = 19^6
  \]

  Considerando che abbiamo 4 possibili scelte di stati finali, otteniamo un
  numero di $4*19^6$ macchine di Turing a 2 stati. Iterando il procedimento,
  otterremo sempre un numero finito e al più numerabile di TM\@. Ora ci basta
  ordinare secondo un ordine arbitrario e quindi scrivere un algoritmo che le
  enumeri.
\end{proof}

Il numero \hl{$E(M)$} è detto \hl{numero di Gödel} della TM $\mathcal{M}$ ed
indica il suo \hl{indice nella enumerazione algoritmica} di tutte le TM
creabili. La \hl{$E$} è detta la \hl{gödelizzazione} dell'insieme delle TM\@.

\subsection{Macchina di Turing universale}\label{sec:utm}

Chiediamoci ora se le \hl{macchine di Turing} riescano a \hl{modellare i
calcolatori programmabili}. Una TM con queste caratteristiche è \hl{detta
macchina di Turing universale (UTM)}. La \hl{UTM computa} la funzione
\hl{$g(y,x) = f_y(x)$} dove $f_y$ è la funzione calcolata dalla TM di indice
$y$.

Per come è definita, sembrerebbe che la UTM sia un diverso tipo di automa
rispetto alla TM\@: la prima lavora $\mathbb{N}^2$ mentre la seconda si
$\mathbb{N}$. Sappiamo però che la \hl{cardinalità di $\mathbb{N}^2$ è uguale a
quella di $\mathbb{N}$} in quanto esiste la biezione:

\begin{equation}
  d(x,y) = \frac{(x+y)(x+y+1)}{2} + x
\end{equation}

Possiamo quindi esprimere $g(y,x)$ come $\hat{g}(n) = g(d^{-1}(n))$ con $n =
d(y,x)$.

Una \hl{UTM} che calcola $\hat{g}$ \hl{opererà in questo modo}:

\begin{enumerate}
  \item Dato $n$ calcola \hl{$\langle y,x \rangle = d^{-1}(n)$};
  \item Costruisce la \hl{funzione di transizione $M_y$} calcolando
    \hl{$E^{-1}$} e la memorizza sul nastro;
  \item In un'altra porzione di nastro \hl{memorizza una codifica della
    configurazione di $M_y$};
  \item Lascia sul nastro \hl{$f_y(x)$ se e solo se $M_y(x)$ termina la
    computazione}.
\end{enumerate}

\subsection{Funzioni calcolabili e problemi definibili}\label{sec:func-calc}

\hl{È possibile calcolare tutte le funzioni da $\mathbb{N}$ a $\mathbb{N}$?} In
caso \hl{negativo}, per la tesi di Church, \hl{esisterebbero problemi non
risolvibili tramite algoritmi}. Calcoliamo la cardinalità dell'insieme di
funzioni naturali a variabile naturale. Consideriamo il sottoinsieme di $f:
\mathbb{N} \to \{0,1\}$. È possibile calcolare la cardinalità di questo insieme
ed essa è pari a quella di $\wp(\mathbb{N})$ ossia $2^{\aleph_0}$ ossia la
cardinalità del continuo. Visto che abbiamo considerato un sottoinsieme, la
\hl{cardinalità che cerchiamo è sicuramente maggiore di quella del continuo}.
Poiché l'\hl{insieme di funzioni calcolabili dalle macchine di Turing è per
definizione numerabile} (di cardinalità $\aleph_0$) \hl{non abbiamo abbastanza
TM} per risolvere tutti i problemi possibili.

Precedentemente abbiamo detto che i problemi di dominio naturale sono sempre
traducibili come la computazioni di una funzione di dominio naturale. Abbiamo
però visto che non tutte le funzioni di questo tipo sono computabili. \hl{Per
definire un problema}, infatti, \hl{ci serve una frase}, una stringa, \hl{di
qualche linguaggio} che li caratterizzi. Per definizione, \hl{ogni linguaggio è
un insieme numerabile} e ciò renderà anche l'\hl{insieme dei problemi definibili
numerabile}. Quindi esistono problemi che non sono nemmeno definibili.

Concentriamoci sui problemi che sono \hl{definibili}. \hl{Essi sono sempre
risolvibili?} La risposta è \hl{negativa}. Un famoso problema definibile ma non
risolvibile è il \hl{``Halting Problem''}.

\begin{prop}[\hl{Halting problem}]
  \hl{Costruito un programma che, dati dei dati in ingresso, esegue una
  computazione che potrebbe terminare, è possibile determinare in anticipo se
  terminerà?}
\end{prop}
\begin{proof}
  Formalmente ci stiamo cheidendo se esiste una TM tale che data $f_y$, calcola
  $g(y,x)$ totale tale che:

  \[
    g(y,x) =
    \begin{cases}
      1 & \quad \text{se } f_y(x) \neq \bot \\
      0 & \quad \text{se } f_y(x) = \bot
    \end{cases}
  \]

  Esistono diversi metodi di dimostrazione. Noi useremo quello per
  diagonalizzazione a quello usato da Cantor per dimostrare la cardinalità del
  continuo.

  Enumeriamo tutte le funzione calcolabili da una macchina di Turing e i loro
  valori:

  \[
    \begin{array}{c|ccc} % chktex 44
          & 1      & 2      & \ldots \\
      f_1 & f_1(1) & f_1(1) & \ldots \\
      f_2 & f_2(1) & f_2(1) & \ldots \\
      f_3 & f_3(1) & f_3(1) & \ldots \\
      \vdots & \vdots & \vdots & \ddots
    \end{array}
  \]

  Ipotizziamo allora per assurdo che la nostra funzione $g$ sia calcolabile.
  Definiamo

  \[
    h(x) =
    \begin{cases}
      1    & \quad \text{se } g(x,x) = 0\\
      \bot & \quad \text{altrimenti} \\
    \end{cases}
  \]

  La funzione $h$ è ovviamente calcolabile se anche $g$ lo sarà. Abbiamo quindi
  definito una funzione che si pone sulla diagonale della nostra tavola delle
  funzioni.

  Se $h$ è calcolabile, e lo è visto che per ipotesi anche $g$ è calcolabile,
  esisterà una TM di indice $i$ tale che $h = f_i$. Provando a calcolare $h(i)$
  otteniamo:

  \begin{enumerate}
    \item Se $h(i) = f_i(i) = 1$ allora $g(i,i)=0$, ossia $f_i(i)=\bot$ che è
      una contraddizione
    \item Se $h(i) = f_i(i) = 0$ allora $g(i,i)=1$, ossia $f_i(i)\neq\bot$ che è
      una contraddizione
  \end{enumerate}

  Siamo arrivati così ad un assurdo e quindi $g$ non è calcolabile.
\end{proof}

L'Halting Problem ci permette di affermare con fermezza che l'insieme dei
\hl{problemi risolvibili è strettamente incluso in quello dei problemi
descrivibili}.

\subsubsection{Specializzazioni e generalizzazioni}\label{sec:spec-gen}

Consideriamo la funzione:

\begin{equation}
  h'(x) =
  \begin{cases}
    1 & \quad \text{se } f_y(x) = \bot \\
    0 & \quad \text{altrimenti}
  \end{cases}
\end{equation}

Essa è un \hl{caso particolare del Halting Problem} dove abbiamo imposto che in
$g(y,x)$ sarà $y=x$. Si dimostra che \hl{$h'$ non è computabile}. Nota bene che
$h'$ \hl{non è un corollario e non deriva dal Halting problem}. Infatti un
\hl{caso specifico} di un problema \hl{non risolvibile non è detto che sia
anch'esso non risolvibile} e, viceversa, la \hl{generalizzazione di un problema
risolvibile non per forza mantiene la risolvibilità}. Se un problema è già
\hl{risolvibile}, però, la \hl{sua specializzazione sarà anch'essa risolvibile}
e viceversa la \hl{generalizzazione di un problema non risolvibile sarà ancora
non risolvibile}.

\subsubsection{Il problema delle funzioni totali}\label{sec:func-tot}

Consideriamo un'altra funzione:

\begin{equation}\label{eqn:func-k}
  k(y) =
  \begin{cases}
    1 & \quad \text{se } \forall x \in \mathbb{N}, f_y(x) \neq \bot\\
    0 & \quad \text{altrimenti}\\
  \end{cases}
\end{equation}

La \hl{funzione, in pratica, vale $1$ se la funzione $f_y(x)$ è totale e 0
altrimenti}. Da un punto di vista pratico, questa funzione è ancora più
importante di quella derivata dal problema dell'arresto. In questo caso,
infatti, ci chiediamo se il programma non termini per qualsiasi ingresso, invece
di chiedercelo solo per uno. Dimostriamo che la \hl{funzione $k$ non è
computabile}.

\begin{prop}
  \hl{La funzione}~\ref{eqn:func-k} \hl{non è computabile}.
\end{prop}
\begin{proof}
  Ipotizziamo per assurdo che la funzione sia computabile. Definiamo $g(x) = w$
  con $w$ pari al numero di Gödel della $x$-esima TM che calcola una funzione
  totale. Se $k$ è computabile, allora lo sarà ang $g$. Infatti possiamo trovare
  una procedura algoritmica che calcola $g(x)$:

  \begin{enumerate}
    \item Calcoliamo $k(x)$ al crescere di $x$. Trovato $x_0|_{k(x_0)=1}$
      poniamo $g(0) = x_0$ e riprendiamo a calcolare $k$ da $x_0 + 1$ in avanti;
    \item Trovato $x_1|_{k(x_1)=1}$ poniamo $g(1) = x_1$ e iteriamo.
  \end{enumerate}

  È facile vedere che $g$ è strettamente monotona. Possiamo calcolare $g^{-1}$
  anch'essa strettamente monotona ma non totale. Definiamo

  \[
    f_{g(x)}(x) +1 = f_w(x) +1
  \]

  Sappiamo che $f_w(x)$ è calcolabile e totale per definizione di $g$ e quindi
  anche $h$ lo sarà. Esisterà allora un $\bar{w}|_{f_{\bar{w}}(\cdot)=h(\cdot)}$
  Dato che $h$ è totale, sicuramente anche $g^{-1} \neq \bot$. Poniamo allora
  $g^{-1}(\bar{w})=\bar{x}$. Studiamo la forma di $h$: per definizione avremo

  \[
    h(\bar{x})=f_{g(\bar{x})}(\bar{x}) + 1 = f_{\bar{w}}(\bar{x}) +1
  \]

  Ma, siccome $h(\cdot) = f_{\bar{w}}(\cdot)$, abbiamo anche $h(\bar{x}) =
  f_{\bar{w}}(\bar{x})$. Cadiamo allora in un assurdo e quindi la funzione $k$
  non è calcolabile.
\end{proof}

Nota bene che la \hl{definizione di $k$ ha una quantificazione universale}.
Potrebbe essere, quindi, che \hl{per qualche valore specifico di $x$ possiamo
stabilire il valore di $k(y)$. Non siamo però in grado di dire se possiamo farlo
per ogni singolo ingresso possibile}.

\subsection{Decidibilità e semidecidibilità}\label{sec:problemi-decisione}

Concentriamoci sui problemi \hl{con risposta binaria, detti anche di decisione}.
Questo tipo di problemi hanno \hl{due risposte possibili ``vero'' e ``falso''}.
Un problema di questo tipo \hl{può essere decidibile, semidecidibile o
indecidibile}. La prima e la ultima hanno significato ovvio, approfondiamo la
seconda.

\begin{defn}[\hl{Problema semidecidibile}]\label{def:problema-semidecidibile}
  Un problema si dice \hl{semidecidibile} se esiste un \hl{algoritmo che ritorni
  ``vero'' se il problema ha risposta affermativa}.
\end{defn}

\begin{nota}
  La definizione di semidecidibilità \hl{non ci impone limiti su quello che fa
  l'algoritmo in caso di esito negativo}. Esso può ritornare il valore ``falso''
  oppure anche non terminare.
\end{nota}

\paragraph{Semidecidibilità e testing} Un \hl{gran numero di problemi
indecidibili} si può dimostrare che \hl{sono semidecidibili}. Uno di questi è il
\hl{problema dell'arresto}: l'algoritmo basta che guardi se la TM si ferma su
uno stato di accettazione. \hl{L'utilità della proprietà di semidecidibilità è
che ci permette di rilevare se c'è un errore invece di garantirne la sua
assenza}. Ciò ha importanti applicazioni pratiche nella verifica di programmi
basata sul \hl{testing}:

\begin{lem}[Asserzione di Dijkstra]
  Il testing può dimostrare la presenza di errori, non la loro assenza.
\end{lem}

Tutti i \hl{problemi di decisione} possono essere \hl{riformulati come ``dato un
insieme $S$, $x \in S$''}. Alternativamente possiamo calcolare la \hl{funzione
caratteristica} dell'insieme $S$:

\begin{equation}
  \mhl{
    \mathbf{1}_{S} =
    \begin{cases}
      1 & \quad \text{se } x \in S \\
      0 & \quad \text{altrimenti}
    \end{cases}
  }
\end{equation}

L'insieme \hl{$S$} può essere \hl{ricorsivo (decidibile) oppure ricorsivamente
numerabile (semidecidibile)}.

\begin{defn}[\hl{Insieme ricorsivo (decidibile)}]\label{def:insieme-decidibile}
  Un insieme $S$ si dice \hl{ricorsivo} o decidibile se e solo se la sua
  \hl{funzione caratteristica è computabile}.
\end{defn}

\begin{defn}[\hl{Insieme ricorsivamente enumerabile (semidecidibile)}]\label{def:insieme-semidecidibile}
  Un insieme $S$ si dice \hl{ricorsivamente enumerabile} (RE) o semidecidibile
  se e solo se:

  \begin{itemize}
    \item $S$ è l'insieme \hl{vuoto};
    \item $S$ è l'\hl{immagine di una funzione totale computabile}:

      \begin{equation}
        S = I_{g_S} = \{ x: g_S(y), y \in \mathbb{N} \}
      \end{equation}
  \end{itemize}
\end{defn}

Il termine di insieme semidecidibile deriva dal fatto che se $x \in S$
enumerando gli elementi di $S$ prima o poi lo troverò, altrimenti sono mai certo
di poter rispondere ``falso'' enumerando in quanto potrei non aver ancora
trovato $x$.

Tra la decidibilità e la semidecidibilità ci sono diversi importanti legami:

\begin{thm}[\hl{Decidibilità implica semidecidibilità}]\label{thm:dec-semidec}
  Se un insieme $S$ è \hl{ricorsivo (decidibile), esso è ricorsivamente
  enumerabile (semidecidibile)}.
\end{thm}
\begin{proof}
  Studiamo i vari casi possibili nella definizione di semidecidibilità:

  \begin{itemize}
    \item Se $S$ è vuoto, è RE per definizione;
    \item Se $S$ non è vuoto, costruiamo una funzione totale e computabile di
      cui $S$ è immagine. Poiché sappiamo che esisterà un $k \in S$ tale che
      $\mathbf{1}_S(k) = 1$, possiamo definire la funzione come

      \[
        g_S(x) =
        \begin{cases}
          x & \quad \text{se } \mathbf{1}_S(x) = 1 \\
          k & \quad \text{se } \mathbf{1}_S(x) = 0
        \end{cases}
      \]

      Con $g_S$ totale, computabile e $\mathbf{I}_{g_S} = S$. $S$ sarà, quindi,
      RE\@.
  \end{itemize}
\end{proof}

\begin{thm}[\hl{semidecidibilità + semidecidibilità = decidibilità}]\label{thm:0.5+0.5=1}
  Si un insieme $S$, esso sarà \hl{ricorsivo se e solo se sono ricorsivamente
  enumerabili sia $S$ che $\bar{S} = \mathbb{N} \setminus S$}.
\end{thm}
\begin{proof}
  Dimostriamo che se $S$ è ricorsivo allora $S$ e $\bar{S}$ sono RE\@.
  Innanzitutto $S$ è ricorsivo, per il teorema~\ref{thm:dec-semidec} esso è
  anche RE\@. Inoltre possiamo osservare che anche $\mathbf{1}_{\bar{S}}$ è
  calcolabile in quanto è pari a $\mathbf{1}_S$ con 1 e 0 scambiati,
  permettendoci di affermare che anche $\bar{S}$ è RE\@.

  Dimostriamo ora l'implicazione inversa. Osserviamo che $S \cup \bar{S} =
  \mathbb{N}$ e $S \cap \bar{S} = \emptyset$ e quindi una qualunque $x \in
  \mathbb{N}$ appartiene a una e una sola delle due enumerazioni di $S$ e
  $\bar{S}$:

  \[
    (\forall x \in \mathbb{N}, \exists y : x = g_{\bar{S}}(y) \lor x = g_S(y))
      \land (\neg \exists z : g_{\bar{S}}(z) = g_S(z))
  \]

  Posso essere, quindi, certo di trovare qualsiasi $x$ nell'enumerazione

  \[
    \{ g_S(0), g_{\bar{S}}(0), g_S(1), g_{\bar{S}}(1), \ldots \}
  \]

  Nel momento in cui trovo $x$ in posizione dispari concludo che $x \notin S$,
  altrimenti $x \in S$. So quindi calcolare $\mathbf{1}_S$, rendendo $S$
  ricorsivo.
\end{proof}

Da questi due teoremi possiamo trarre un po' di risvolti importanti.
Innanzitutto possiamo affermare che gli \hl{insiemi decidibili sono chiusi
rispetto al complemento}. Possiamo usare la teoria degli insiemi ricorsivi per
cercare di capire se possiamo enumerare tutte le funzioni calcolabili e totali.

\begin{prop}[\hl{Definione delle funzioni calcolabili e totali}]\label{thm:def-func-tot}
  Dato un insieme \hl{$S$} per cui:

  \begin{enumerate}
    \item \hl{Se $i \in S$ allora $f_i$ e calcolabile e totale};
    \item \hl{Se $f$ e totale e computabile allora $\exists i \in S : f_i =
      f_i$}.
  \end{enumerate}

  Esso \hl{non sarà ricorsivamente enumerabile}.
\end{prop}
\begin{proof}
  Ipotizziamo per assurdo che esista una funzione caratteristica
  $\mathbf{1}_S(i)$ computabile. Definiamo:

  \[
    h(x) =
    \begin{cases}
      f_x(x) + 1 & \quad \text{se } \mathbf{1}_S(x) = 1 \\
      0          & \quad \text{altrimenti}
    \end{cases}
  \]

  Abbiamo che per ogni $x$ $h(x) \neq f_{\mathbf{1}_S(x)}(x)$. Quindi $h(x)$ è
  calcolabile ma diversa da tutte le funzioni calcolabili, il che è un assurdo.
\end{proof}

\hl{Non è possibile quindi definire con un formalismo RE} (automi, grammatiche e
funzioni ricorsive) \hl{capace di definire l'insieme dei tutte e sole le
funzioni calcolabili totali}. Quindi non posso descrivere in nessun modo
descrivere come è fatto l'insieme di tutti e soli i programmi che terminano
sempre:

\begin{itemize}
  \item Gli FSA e i PDA definiscono solo funzioni totali ma non tutte
  \item Le TM definiscono tutte le funzioni calcolabili, ma anche quelle non
    totali
  \item Il C mi permette di scrivere qualunque algoritmo, ma anche che non
    terminato
\end{itemize}

Riusciamo, però, ad \hl{aggirare il problema e considerare l'insieme di tutte le
funzioni e rimuovere con un artificio le funzioni parziali?} Arricchiamo
$\mathbb{N}$ con un nuovo valore $\bot$ oppure assegnamo un calore convenzionale
ad $f$ quando non è definita. \hl{Matematicamente questa operazione non causa
problemi, però si può dimostrare che per la funzione}:

\begin{equation}
  \mhl{
    g(x) =
    \begin{cases}
      f_x(x) + 1 & \quad \text{se } f_x(x) \neq \bot \\
      \bot       & \quad \text{altrimenti}
    \end{cases}
  }
\end{equation}

\hl{Non è possibile costruire una funzione computabile e totale che la estenda}.
Inoltre possiamo dimostrare che \hl{esistono insiemi che sono semidecidibili
senza essere decidibili}.

\begin{proof}[Dimostrazione. Esistenza di insiemi semidecidibili non decidibili]
  Consideriamo

  \[
    S = \{ x : f_x(x) \neq \bot \}
  \]

  Essa è il dominio della funzione $h(x) = f_x(x)$ che è computabile ma
  parziale. Poiché è possibile riscrivere questa affermazione in termini di
  immagine di una $g$ totale computabile, $S$ è RE\@. Sappiamo anche che la
  funzione caratteristica $\mathbf{1}_S = 1$ se $f_x(x) \neq \bot$, e 0
  altrimenti non è computabile e quindi $S$ non è decidibile.
\end{proof}

Possiamo costruire quindi un \hl{gerarchia di inclusioni strette} tra vari tipi di
insiemi:

\begin{equation}
  \mhl{
    \text{Ricorsivi} \subset \text{RE} \subset \wp(\mathbb{N})
  }
\end{equation}

\subsection{Stabilire decidibilità e semidecidibilità di un problema}\label{sec:stabilire-dec-sec-}

Enunciamo il \hl{primo risultato importante che ci servirà per capire se e come
possiamo stabilire la decidibilità (semidecidibilità)} di un problema.

\begin{thm}[\hl{Teorema di Kleene del punto fisso}]\label{thm:kleene-punto-fisso}
  Sia una \hl{funzione $t(\cdot)$ totale e computabile}. È \hl{sempre possibile
  trovare un $p \in \mathbb{N}$} tale che \hl{$f_p = f_{t(p)}$}. La funzione
  \hl{$f_p$ è detta punto fisso di $t(\cdot)$}.
\end{thm}
\begin{proof}
  Dato $u \in \mathbb{N}$ definiamo una TM che effettua il seguente calcolo
  sull'ingresso $x$: calcola $f_u(u) = z$ e quando il calcolo termina calcola
  $f_z(x)$. Possiamo costruire questa TM e cercare il suo numero di Gödel $g(u)$
  per una qualsiasi $u$. Otteniamo che la funzione della TM sarà esprimibile
  come:

  \[
    f_{g(u)}(x) =
    \begin{cases}
      f_{f_u(u)}(x) & \quad \text{se } f_u(u) \neq \bot \\
      \bot          & \quad \text{altrimenti}
    \end{cases}
  \]

  Sappiamo che data $g(\cdot)$ totale e calcolabile e $t(\cdot)$ anch'essa
  totale e computabile, lo sarà anche $t(g(\cdot))$. Chiamiamo $v$ il numero di
  Gödel di $t(g(\cdot))$, ottenendo $t(g(\cdot)) = f_v(\cdot)$. Ripetendo la
  costruzione che abbiamo fatto precedentemente otteniamo che:

  \[
    f_{g(v)} =
    \begin{cases}
      f_{f_v(v)}(x) & \quad \text{se } f_v(v) \neq \bot \\
      \bot          & \quad \text{altrimenti}
    \end{cases}
  \]

  Ricordando che $t(g(\cdot))$ è totale e computabile, otteniamo che
  $f_{g(v)}(x) = f_{f_v(v)}(x)$ per ogni $x$. Sostituendo nel secondo membro
  otteniamo:

  \[
    f_{g(v)} = f_{t(g(v))}
  \]

  Rendendo, quindi $f_{g(v)}$ il punto fisso di $t(\cdot)$.
\end{proof}

Possiamo ora enunciare una \hl{condizione per la decidibilità di un insieme}.

\begin{thm}[\hl{Teorema di Rice}]\label{thm:rice}
  Siano \hl{$F$ un'insieme di funzioni computabili} e l'insieme \hl{$S$ degli
  indici delle TM che calcolano le funzioni di $F$}. L'insieme \hl{$S = \{x:f_x
  \in F\}$} è \hl{decidibile se e solo se}:

  \begin{itemize}
    \item \hl{$F = \emptyset$};
    \item \hl{$F$ è l'insieme di tutte le funzioni computabili}.
  \end{itemize}
\end{thm}
\begin{proof}
  Supponiamo per assurdo che $S$ sia decidibile con $F$ non vuoto e diverso
  dall'insieme di tutte le funzioni computabili. Consideriamo la funzione
  caratteristica di $S$ $\mathbf{1}_S(x)$, essa sarà calcolabile per ipotesi.
  Possiamo quindi calcolare:

  \begin{enumerate}
    \item Il più piccolo $i \in \mathbb{N}$ tale che $f_i \in F$
    \item Il più piccolo $j \in \mathbb{N}$ tale che $f_j \notin F$
  \end{enumerate}

  Per quanto detto, la funzione:

  \[
    h_S(x) =
    \begin{cases}
      i & \quad \text{se } f_x \notin F \\
      j & \quad \text{altrimenti}
    \end{cases}
  \]

  Sarà anch'essa calcolabile e totale. Applicando il teorema di Kleene alla
  funzione appena definita otteniamo che esiste un punto fisso $f_{\bar{x}}$ tale
  per cui $f_{\bar{x}} = f_{h_s(\bar{x})}$. Siamo così arrivati ad una
  contraddizione in quanto:

  \begin{itemize}
    \item Se $h_S(\bar{x}) = i$: per definizione di $h_S$ abbiamo che
      $f_{\bar{x}} \notin F$, ma da quanto detto per il teorema di Kleene
      $f_{\bar{x}} = f_{h_S(\bar{x})} = f_i$ da cui, per come definito $i$, $f_i
      \in F$.
    \item Se $h_S(\bar{x}) = j$: per definizione di $h_S$ abbiamo che
      $f_{\bar{x}} \in F$, ma da quanto detto per il teorema di Kleene
      $f_{\bar{x}} = f_{h_S(\bar{x})} = f_j$ da cui, per come definito $j$, $f_j
      \notin F$.
  \end{itemize}
\end{proof}

Quindi \hl{in tutti i casi non banali l'insieme delle funzioni calcolabili con
una data caratteristica desiderata non è decidibile!} Quindi non sarà possibile
rispondere a molti quesiti importanti:

\begin{itemize}
  \item Il programma $P$ è corretto? Risolve un dato problema?
  \item È possibile stabilire l'equivalenza tra due programmi?
  \item È possibile stabilire se un generico programma gode di una qualsiasi
    proprietà non banale riferita alla funzione che calcola?
\end{itemize}

\hl{Stabilire se un generico problema è decidibile (semidecidibile) o meno è
indecidibile}. Ragionando in \hl{modo pratico} abbiamo \hl{tre possibili
alternative} per \hl{stabilire la decidibilità di} un problema:

\begin{enumerate}
  \item Se \hl{troviamo un algoritmo che termina sempre}, allora il problema è
    \hl{decidibile};
  \item Se \hl{troviamo un algoritmo che termina sempre se la risposta è
    ``vero'' ma può non terminare se la risposta è ``falso''}, allora il
    problema è \hl{semidecidibile};
  \item Se riusciamo a \hl{trovare una dimostrazione diagonale della
    indecidibilità} del problema allora esso è \hl{indecidibile}.
\end{enumerate}

La \hl{terza opzione} è \hl{molto laboriosa ma fattibile}. Il \hl{teorema di
Rice} ci permette facilmente di \hl{stabilire se un problema non è decidibile},
ma esso \hl{può} lo stesso \hl{essere semidecidibile}. Una \hl{tecnica
alternativa}, molto generale, è quella della \hl{riduzione dei problemi}: ci
permette di \hl{dimostrare in modo agevole l'indecidibilità di alcuni problemi}.

\subsubsection{La tecnica di riduzione di un problema}\label{sec:riduzione}

Consideriamo prima una visione operativa della tecnica di riduzione. Se abbiamo
un algoritmo per risolvere un problema $P$, possiamo riusarlo modificandolo per
risolvere problemi $P'$ simili a $P$. In generale se trovo un algoritmo che,
dato un esemplare di $P'$ ne costruisce la soluzione usando un esemplare di $P$
che so risolvere, ho ridotto $P'$ a $P$. Ciò significa che affinché $P'$ sia
riducibile a $P$, si dovrà avere che:

\begin{enumerate}
  \item $P$ risolvibile;
  \item C'è un algoritmo che per ogni istanza di $P'$ determina una
    corrispondente istanza di $P$ e costruisce algoritmicamente la soluzione
    dell'istanza di $P'$ usando la soluzione dell'istanza di $P$.
\end{enumerate}

Formalizziamo il tutto:

\begin{prop}[\hl{Tecnica di riduzione}]\label{thm:riduzione}
  \hl{Consideriamo due problemi: $y \in S'$ e $x \in S$} di cui il \hl{secondo è
  quello che vogliamo risolvere}. \hl{Se troviamo un a funzione $t$ calcolabile
  e totale} per cui:

  \begin{equation}
    \mhl{
      x \in S \iff t(x) \in S'
    }
  \end{equation}

  Il \hl{problema d'interesse è risolvibile}. Inoltre, dato $x$, \hl{calcolare
  $\mathbf{1}_{S'}(t(x))$ equivale a calcolare $\mathbf{1}_S(x)$}.

  La proposizione \hl{funziona anche in direzione inversa}: se so che \hl{$y \in
  S'$ non è risolvibile, se trovo la $t$ che rispetta la precedente condizione
  allora anche $x \in S$ non sarà risolvibile}.
\end{prop}

\begin{esempio}
  Dall'indecidibilità del problema dell'arresto della TM deduciamo
  l'indecidibilità del problema della terminazione del calcolo in generale. I
  passaggi operativi per dimostrare ciò che possiamo eseguire sono:

  \begin{enumerate}
    \item Costruiamo un programma $P$ che simuli una TM $M_i$ e memorizziamo il
      numero $x$ in un file $f$.
    \item Il programma $P$ termina la computazione su $f$ se e solo se $g(i,x)
      \neq \bot$.
    \item Se riuscissimo a scrivere un programma che riesce a predire il
      contenuto di $f$ avremmo risolto il problema dell'arresto, ma ciò non è
      possibile.
  \end{enumerate}
\end{esempio}

\begin{esempio}
  È decidibile dire se, durante l'esecuzione di un generico programma $P$ si
  accede ad una variabile non inizializzata?

  Supponiamo per assurdo che questo problema sia decidibile. Riduciamolo al
  problema dell'arresto:

  \begin{enumerate}
    \item Dato un generico programma $Q(n)$, costruisco un programma $P$ fatto
      in questo modo:

      \begin{align*}
        & \{ \\
        & \quad \mathtt{int \; x,y;} \\
        & \quad \mathtt{Q(n);} \\
        & \quad \mathtt{y=x;} \\
        & \}
      \end{align*}

      Avendo cura di usare variabili non presenti in $\mathtt{Q}$.
    \item L'accesso $\mathtt{y=x}$ alla variabile non inizializzata $x$ da parte
      di $P$ è fatto se e solo se $\mathtt{Q}$ termina.
  \end{enumerate}

  Se fossi in grado di decidere il problema dell'accesso a variabile non
  inizializzata, potrei decidere il problema della terminazione del calcolo, che
  è un assurdo.
\end{esempio}

\hl{Le proprietà dimostrate nei precedenti esempi non sono decidibili, ma sono
semidecidibili. Abbiamo un metodo operativo per determinare la semidecidibilità
di un problema?}

Innanzitutto il problema \hl{$\exists z : f_x(z) \neq \bot$ è semidecidibile}.
Una \hl{idea di dimostrazione è simulare l'azione della funzione ``in
diagonale''} eseguendo \hl{una sola mossa alla volta per ogni input} finché non
troviamo l'input per la quale la computazione non si arresta. \hl{Simulando
abbastanza passi di $f_x$, eventualmente lo troverò}.

\section{Complessità del calcolo}\label{sec:complessita}

Calcolare la \hl{complessità} di una computazione significa \hl{calcolare
l'efficienza e il costo in tempo di una computazione}. Come possiamo misurare
questi parametri?  Dobbiamo prima definire degli strumenti per valutare la
complessità e successivamente potremmo enunciare degli algoritmi e strutture
dati notevoli. Il nostro obiettivo sarà sviluppare gli strumenti per saper
progettare e combinare algoritmi e strutture dati che realizzano soluzioni
efficienti.

Per \hl{quantificare l'efficienza} di un algoritmo, faremo \hl{analisi
qualitative di due metriche}:

\begin{itemize}
  \item \hl{Tempo di calcolo impiegato};
  \item \hl{Spazio occupato} (registri, cache, RAM, disco o nastro).
\end{itemize}

Si possono fare analisi anche su altri aspetti come i costi di sviluppo, ma noi
non li considereremo. Per la \hl{tesi Church}, un \hl{problema è calcolabile
indipendentemente dallo strumento usato}, purché tale strumento sia \hl{Turing
completo}. Possiamo dire lo \hl{stesso per la complessità}? Purtroppo \hl{no}.
Dobbiamo, quindi, costruire uno \hl{strumento che tralasci considerazioni
superflue e sia utilizzabile per la maggioranza dei modelli di calcolo}. Noi
considereremo per convenzione quello della \hl{macchina di Turing
deterministica}.

Formalizziamo le nostre due metriche:

\begin{defn}[\hl{Complessità temporale}]\label{def:comp-temporale}
  Data la \hl{computazione $c_1 \vdash^\star c_r$} di una \hl{macchina di Turing
  $\mathcal{M}$} (a $k$ nastri) \hl{deterministica} la \hl{complessità temporale
  è $T_\mathcal{M}(x) = r$ se} $\mathcal{M}$ \hl{termina} in $c_r$, \hl{$\infty$
  altrimenti}.
\end{defn}

\begin{defn}[Complessità spaziale]\label{def:comp-spaziale}
  Data la \hl{computazione $c_1 \vdash^\star c_r$ di $\mathcal{M}$} (a $k$ nastri)
  \hl{deterministica}, la \hl{complessità spaziale} è:

  \begin{equation}
    \mhl{
      S_\mathcal{M}(x) = \sum_{j=1}^{k} \max_{i \in \{0, \ldots, r\}}(|\alpha_{ij}|)
    }
  \end{equation}

  Con $\alpha_{ij}$ il contenuto del $j$-esimo nastro alla $i$-esima mossa.
\end{defn}

\begin{nota}
  \hl{Complessità spaziale e temporale} sono \hl{legate} dalla seguente
  relazione:

  \begin{equation}
    \mhl{
      \forall x \quad \frac{S_\mathcal{M}(x)}{k} \leq T_\mathcal{M}(x)
    }
  \end{equation}
\end{nota}

Ad eseguire l'analisi su una TM, dobbiamo gestire \hl{un po' troppi dettagli}.
Effettuiamo, allora, delle \hl{semplificazioni}: esprimeremo la \hl{complessità
in base alla ``dimensione'' $n$ dei dati in ingresso}. A causa di questa
semplificazione dobbiamo \hl{considerare 3 casi per gestire la variazione di
ingressi diversi ma di stessa lunghezza}:

\begin{description}
  \item[\hl{Caso pessimo}]
    \begin{equation}
      \mhl{
        T_\mathcal{M}(n) = \max_{|x|=n} T_\mathcal{M}(x)\label{def:caso-pessimo}
      }
    \end{equation}

  \item[\hl{Caso ottimo}]
    \begin{equation}
      \mhl{
        T_\mathcal{M}(n) = \min_{|x|=n} T_\mathcal{M}(x)\label{def:caso-ottimo}
      }
    \end{equation}

  \item[\hl{Caso medio}]
    \begin{equation}
      \mhl{
        T_\mathcal{M}(n) = \frac{\sum_{|x|=n} T_\mathcal{M}(x)}{|I|^n}\label{def:caso-medio}
      }
    \end{equation}
\end{description}

Noi \hl{considereremo sempre il caso pessimo in quanto è il più rilevante}.
Inoltre \hl{l'analisi del caso medio risulta assai complessa} in quanto dovrebbe
tenere conto di ipotesi probabilistiche sulla distribuzione dei dati.

I \hl{valori esatti} delle due complessità per un dato $n$ \hl{non sono
particolarmente utili}. La prima (e più forte) semplificazione che facciamo è
quella di \hl{considerare solo il comportamento asintotico, ossia $n \to
\infty$}. Per esprimere il comportamento asintotico abbiamo \hl{3 notazioni}:

\begin{itemize}
  \item \hl{$\mathcal{O}$-grande}: limite asintotico \hl{superiore};
  \item \hl{$\Omega$-grande}: limite asintotico \hl{inferiore};
  \item \hl{$\Theta$-grande}: limite asintotico \hl{sia superiore che
    inferiore}.
\end{itemize}

Eseguire una approssimazione di questo genere causa \hl{due problemi pratici}.
Il primo è che il \hl{comportamento asintotico potrebbe essere un pessimo
modello per valori piccoli di $n$}; il secondo è che \hl{in qualche raro caso la
complessità per $n$ piccolo non corrisponde alla nostra intuizione, ossia una
algoritmo con complessità asintotica minore può essere più lento per valori
piccoli di $n$ rispetto ad uno con complessità asintotica maggiore}.

\begin{defn}[\hl{$\mathcal{O}$-grande}]\label{def:o-grande}
  Data una \hl{funzione $g(n)$, $\mathcal{O}(g(n))$ è l'insieme}:

  \begin{equation}
    \mhl{
      \mathcal{O}(g(n)) = \{ f(n) : \exists c>0, n_0>0 \; (\forall n>n_0 \;
        0 \leq f(n) \leq cg(n)) \}
    }
  \end{equation}
\end{defn}

\begin{defn}[\hl{$\Omega$-grande}]\label{def:omega-grande}
  Data una \hl{funzione $g(n)$, $\Omega(g(n))$ è l'insieme}:

  \begin{equation}
    \mhl{
      \Omega(g(n)) = \{ f(n) : \exists c>0, n_0>0 \; (\forall n>n_0 \;
        0 \leq cg(n) \leq f(n)) \}
    }
  \end{equation}
\end{defn}

\begin{defn}[\hl{$\Theta$-grande}]\label{def:theta-grande}
  Data una \hl{funzione $g(n)$, $\Theta(g(n))$ è l'insieme}:

  \begin{equation}
    \mhl{
      \Theta(g(n)) = \{ f(n) : \exists c_1>0, c_2>0 n_0>0 \; (\forall n>n_0 \;
        0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n)) \}
    }
  \end{equation}
\end{defn}

\begin{prop}[Proprietà delle notazioni asintotiche]
  \begin{enumerate}
    \item
      \[
        \mhl{
          f(n) \in \Theta(g(n)) \iff f(n) \in \mathcal{O}(g(n)) \land
          f(n) \in \Omega(g(n))
        }
      \]
    \item Le tre notazioni godono della \hl{proprietà transitiva};
    \item Le tre notazioni godono della \hl{proprietà riflessiva};
    \item \hl{$f(n) \in \Theta(g(n)) \iff g(n) \in \Theta(f(n))$};
    \item \hl{$f(n) \in \mathcal{O}(g(n)) \iff g(n) \in \Omega(f(n))$}.
  \end{enumerate}
\end{prop}

\begin{nota}
  Per le proprietà enunciate sopra, la \hl{$\Theta$ è una relazione di
  equivalenza}.
\end{nota}

Noi abbiamo definito le notazioni asintotiche in termini insiemistici, ma
possiamo anche \hl{definirle come limiti}:

\begin{defn}[\hl{$\Theta$-grande (come limite)}]\label{def:theta-grande-lim}
  Diciamo che $f(n) \in \Theta(g(n))$ se e solo se:

  \begin{equation}
    \mhl{
      \lim_{n \to \infty} \frac{f(n)}{g(n)} = c , c \neq 0, c \neq \infty
    }
  \end{equation}
\end{defn}

\begin{defn}[\hl{$\mathcal{O}$-grande (come limite)}]\label{def:o-grande-lim}
  Diciamo che $f(n) \in \mathcal{O}(g(n))$ se e solo se:

  \begin{equation}
    \mhl{
      \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0
    }
  \end{equation}

  Possiamo anche scrivere $\Theta(f(n)) < \Theta(g(n))$
\end{defn}

\subsection{Complessità degli altri modelli visti fino ad ora}\label{sec:comp-fsa-pda-tm}

Analizziamo velocemente la complessità degli altri modelli di calcolo introdotti
fino ad ora: gli FSA, i PDA e le TM a nastro singolo.

\paragraph{FSA} Gli FSA avranno sempre complessità spaziale \hl{$S_{FSA}(n) \in
\Theta(1)$ (complessità costante), in quanto privi di memoria}, e complessità
temporale \hl{$T_{FSA}(n) \in \Theta(n)$ poiché leggono l'input in sequenza}, un
carattere alla volta.

\paragraph{PDA} A causa di come funziona la stack, la complessità spaziale sarà
\hl{$S_{PDA}(n) \in \Theta(n)$}, mentre quella temporale sarà, come nel caso
degli FSA, \hl{$T_{PDA}(n) \in \Theta(n)$}.

\paragraph{TM a nastro singolo} La complessità spaziale \hl{non potrà mai
scendere sotto la linearità} ($S_\mathcal{M}(n) \in \Theta(n)$) mentre quella
temporale si dimostra essere \hl{al massimo $T_\mathcal{M}(n) \in \Theta(n^2)$}.
Le TM a nastro singolo sono, quindi, più potenti dei PDA ma anche più lente.

\subsection{Teoremi di accelerazione lineare}\label{sec:acc-lin}

Enunciamo ora dei teoremi che ci permettono di \hl{migliorare con facilità,
sebbene al massimo linearmente la complessità} dei nostri algoritmi.

\begin{thm}[\hl{Primo teorema di accelerazione lineare}]\label{thm:acc-lin-1}
  Se $L$ è il linguaggio accettato da una \hl{TM $\mathcal{M}$ a $k$ nastri in
  $S_\mathcal{M}(n)$}, per ogni $c \in \mathbb{R}^+$ posso costruire una \hl{TM
  $\mathcal{M}'$ a $k$ nastri che accetta $L$ con}:

  \begin{equation}
    \mhl{ S_{\mathcal{M}'}(n) < c \cdot S_\mathcal{M}(n) }
  \end{equation}
\end{thm}

Il trucco usato da questo teorema, e anche degli altri teoremi a seguire, sta
nel \hl{aumentare la dimensione dell'alfabeto in favore della complessità}. Scegliamo
infatti un \hl{``fattore di compressione'' $r$ tale che $rc>2$}. \hl{Per ogni alfabeto
$\Gamma_i$ dell'$i$-esimo nastro di $\mathcal{M}$ costruisco $\Gamma_i'$ di
$\mathcal{M}'$ assegnando un elemento per ogni $s\in\Gamma_i^r$}. Ci basterà
infine costruire l'\hl{organo di controllo di $\mathcal{M}'$} in modo tale per cui:

\begin{enumerate}
  \item Calcoli con i nuovi simboli sui nastri \hl{emulando le mosse di
    $\mathcal{M}$ spostando le testine sui nastri ogni $r$ movimenti di
    $\mathcal{M}$};
  \item \hl{Memorizzi la posizione della testina arricchendo ulteriormente gli
    alfabeti $\Gamma_i$}, oppure arricchendo l'insieme degli stati.
\end{enumerate}

\begin{thm}[\hl{Secondo teorema di accelerazione lineare}]\label{thm:acc-lin-2}
  Se $L$ è il linguaggio accettato da una \hl{TM $\mathcal{M}$ a $k$ nastri in
  $S_\mathcal{M}(n)$}, posso costruire una \hl{TM $\mathcal{M}'$ a $1$ nastro}
  che accetta $L$ con:

  \begin{equation}
    \mhl{ S_{\mathcal{M}'}(n) = c \cdot S_\mathcal{M}(n) }
  \end{equation}
\end{thm}

\begin{thm}[\hl{Terzo teorema di accelerazione lineare}]\label{thm:acc-lin-3}
  Se $L$ è il linguaggio accettato da una \hl{TM $\mathcal{M}$ a $k$ nastri in
  $S_\mathcal{M}(n)$}, per ogni $c \in \mathbb{R}^+$ posso costruire una \hl{TM
  $\mathcal{M}'$ a $1$ nastro} che accetta $L$ con:

  \begin{equation}
    \mhl{ S_{\mathcal{M}'}(n) < c \cdot S_\mathcal{M}(n) }
  \end{equation}
\end{thm}

\begin{thm}[\hl{Quarto teorema di accelerazione lineare}]\label{thm:acc-lin-4}
  Se $L$ è il linguaggio accettato da una \hl{TM $\mathcal{M}$ a $k$ nastri in
  $T_\mathcal{M}(n)$}, per ogni $c \in \mathbb{R}^+$ posso costruire una \hl{TM
  $\mathcal{M}'$ a $k+1$ nastri} che accetta $L$ con

  \begin{equation}
    \mhl{ T_{\mathcal{M}'} = \max(n+1, c\cdot T_\mathcal{M}) }
  \end{equation}
\end{thm}

Come anche il primo teorema di accelerazione lineare, il \hl{trucco sta nel
codificare in modo compresso i simboli dell'alfabeto} di $\mathcal{M}$. Dobbiamo
considerare che la compressione avviene a runtime, quindi \hl{$T_{\mathcal{M}'}$
sarà al massimo lineare}. Comprimendo $r$ simboli in uno, \hl{nel caso pessimo,
possono servirmi 3 mosse di $\mathcal{M}'$ per emulare $r+1$ di $\mathcal{M}$}.

I teoremi sopra sono \hl{validi} anche per le \hl{macchine di Von Neumann}.
Infatti equivalgono ad \hl{aumentare la dimensione della parola di memoria della
macchina} o, equivalentemente, ad usare operazioni vettoriali. Possiamo avere
speedup lineari arbitrariamente grandi aumentando il parallelismo fisico (stando
ai limiti della fisica ovviamente). \hl{Miglioramenti più che lineari sono
possibili solo cambiando algoritmo}. Quindi concepire algoritmi più efficienti è
di gran lunga più efficace della forza bruta.

\subsection{Un nuovo modello di calcolo: la macchina RAM}\label{sec:ram}

Le macchine di Turing e i calcolatori hanno solo \hl{differenze marginali}: un
\hl{calcolatore} è capace di fare \hl{operazioni aritmetiche su tipi a
dimensione finita in tempo costante}, la TM invece deve eseguire l'aritmetica
bit per bit. Ciò può essere ``emulato'' considerando una TM con un alfabeto
molto vasto: $|I| = 2^w$ se volgiamo operare in binario con una parola di $w$
bit. Inoltre un \hl{calcolatore} può \hl{accedere direttamente ad una cella di
memoria}, mentre la TM deve spendere $\Theta(n)$ ($n$ distanza tra la cella e la
testina) tempo per scorrere il nastro. È giunta ora di aggiornare il nostro
modello di calcolo per \hl{modellare un po' più da vicino i calcolatori reali:
la macchina RAM\@}.

La macchina RAM è dotata di un \hl{nastro di lettura $\mathtt{In}$ e uno di
scrittura $\mathtt{Out}$} come anche una TM\@. Il \hl{programma che esegue lo
assumiamo essere cablato} nell'organo di controllo e \hl{composto da diverse
istruzioni} che essa esegue. Anche la logica per la gestione del program counter
assumiamo essere già cablata. La \hl{RAM è dotata di una memoria ad
indirizzamento diretto e randomico} (da cui prende il nome)
\hl{$\mathtt{M}[n]$}, $n\in\mathbb{N}$. Tutte le \hl{istruzioni} del programma
useranno \hl{la prima cella di memoria $\mathtt{M}[0]$, detta accumulatore. Ogni
cella di memoria inoltre conterrà un intero}.

Enunciamo l'instruction set e la corrispettiva semantica.

\begin{table}[htb]
  \centering
  \begin{tabular}{ll}
    \toprule
    Istruzione & Semantica \\
    \midrule
    \texttt{LOAD X}   & $\mathtt{M}[0] \gets \mathtt{M}[X]$ \\
    \texttt{LOAD= X}  & $\mathtt{M}[0] \gets X$ \\
    \texttt{LOAD* X}  & $\mathtt{M}[0] \gets \mathtt{M}[\mathtt{M}[X]]$ \\
    \texttt{STORE X}  & $\mathtt{M}[X] \gets \mathtt{M}[0]$ \\
    \texttt{STORE* X} & $\mathtt{M}[\mathtt{M}[X]] \gets M[0]$ \\
    \texttt{ADD X}    & $\mathtt{M}[0] \gets \mathtt{M}[0] + \mathtt{M}[X]$ \\
    \texttt{SUB X}    & $\mathtt{M}[0] \gets \mathtt{M}[0] - \mathtt{M}[X]$ \\
    \texttt{MUL X}    & $\mathtt{M}[0] \gets \mathtt{M}[0] * \mathtt{M}[X]$ \\
    \texttt{DIV X}    & $\mathtt{M}[0] \gets \mathtt{M}[0] / \mathtt{M}[X]$ \\
    \texttt{ADD= X}   & $\mathtt{M}[0] \gets \mathtt{M}[0] + X$ \\
    \texttt{SUB= X}   & $\mathtt{M}[0] \gets \mathtt{M}[0] - X$ \\
    \texttt{MUL= X}   & $\mathtt{M}[0] \gets \mathtt{M}[0] * X$ \\
    \texttt{DIV= X}   & $\mathtt{M}[0] \gets \mathtt{M}[0] / X$ \\
    \texttt{HALT}     & $-$ \\
    \texttt{READ X}   & $\mathtt{M}[X] \gets In$ \\
    \texttt{READ* X}  & $\mathtt{M}[\mathtt{M}[X]] \gets In$ \\
    \texttt{WRITE X}  & $Out \gets \mathtt{M}[X]$ \\
    \texttt{WRITE= X} & $Out \gets X$ \\
    \texttt{WRITE* X} & $Out \gets \mathtt{M}[\mathtt{M}[X]]$ \\
    \texttt{JUMP l}   & $PC \gets l$ \\
    \texttt{JZ l}     & $PC \gets l$ se $\mathtt{M}[0] = 0$ \\
    \texttt{JGZ l}    & $PC \gets l$ se $\mathtt{M}[0] > 0$ \\
    \texttt{JLZ l}    & $PC \gets l$ se $\mathtt{M}[0] < 0$ \\
    \bottomrule
  \end{tabular}
  \caption{Instruction set (non completo) della macchina RAM\@.}%
  \label{tab:ram-isa}
\end{table}

Si può notare dalla tabella~\ref{tab:ram-isa} che sono ammesse 3 modalità di
indirizzamento: diretto, immediato e indiretto. Le tre modalità funzionano come
in un normale linguaggio assembly.

\subsection{Limiti del criterio di costo}\label{sec:log-costo}

Consideriamo il caso del calcolo di $2^{2^n}$ con una una macchina RAM\@. Uno
schema di implementazione può essere:

\begin{lstlisting}
  read(n);
  x = 2;
  for (int i = 0; i < n; i++) x = x * x;
  write(x);
\end{lstlisting}

La complessità temporale dell'implementazione sopra è $T_{RAM} = \Theta(n)$.
Qualcosa non quadra: mi servono $2^n$ bit solo per scrivere il risultato! \hl{Il
criterio di costo fino ad ora usato considera un intero arbitrario di dimensione
costante. L'approssimazione regge fin quando una singola parola della macchina
reale contiene gli interi che maneggiano}. Se questo \hl{non accade}, dobbiamo
tener conto del \hl{numero di cifre necessarie per rappresentare un intero,
perdendo il costo costante del salvataggio di interi e delle operazioni
elementari su di essi}. In questi casi eseguire operazioni su un intero $i$
costa tanto quanto il suo numero di cifre in base $b$.

Forniamo allora una \hl{criterio di costo più realistico}: il \hl{costo
logaritmico}. Una \hl{operazione} su un intero $i$ \hl{costerà tanto più sono le
sue cifre in una base $b \geq 2$}, ossia $\log_b(i) = \Theta(\log(i))$. Possiamo
\hl{omettere la base} del logaritmo poiché i \hl{logaritmi in diverse basi
differiscono tra di loro solo per una costante} (vedasi la formula di
cambiamento di base del logaritmo).

Applichiamo il nuovo criterio di costo alle principali operazioni aritmetiche.
Consideriamo $d = \log_2(i)$:

\begin{itemize}
  \item Le \hl{addizioni} e sottrazioni sono operazioni effettuate bit per bit,
    quindi avranno costo \hl{$\Theta(d)$}.
  \item La \hl{moltiplicazione} eseguita con il \hl{metodo scolastico} richiede
    di iterare sulle cifre 2 volte, quindi avrà costo \hl{$\Theta(d^2)$}. Studi
    sull'aritmetica ci dicono che la complessità è migliorabile, sebbene con un
    compromesso nella facilità di calcolo:

    \begin{itemize}
      \item \hl{Primo miglioramento: $\Theta(d^{\log_2(3)}) \approx
        \Theta(d^{1.58})$}
      \item \hl{Secondo miglioramento: $\Theta(d\log(d)\log(\log(d)))$}
      \item \hl{Miglior costo attualmente scoperto: $\Theta(d\log(d))$}
    \end{itemize}

  \item Le \hl{divisioni} con il \hl{metodo scolastico}, analogamente alle
    moltiplicazioni, hanno costo \hl{$\Theta(d^2)$}. Si è dimostrato che la
    complessità \hl{può essere migliorata} a \hl{$\Theta(\log^2(d)) \cdot
    \mathit{costo\_mul}$} dove $\mathit{costo\_mul}$ è il costo dell'algoritmo
    di moltiplicazione scelto.
\end{itemize}

Le operazioni di \hl{\texttt{JUMP} e \texttt{HALT}} hanno \hl{costo costante},
così come \hl{anche le istruzioni di salto condizionale}.

\hl{Per scegliere} tra i due criteri basta che \hl{controlliamo l'accuratezza
dell'approssimazione a costo costante}: se la dimensione di ogni singolo
elemento in ingresso non varia in modo significativo durante l'esecuzione usiamo
il costo cotante e altrimenti quello logaritmico.

\subsection{Legami tra le complessità dei vari modelli di calcolo}

Quanto effetto ha sulla complessità cambiare modello di calcolo? \hl{Sotto
ragionevoli ipotesi di criteri di costo, se un problema è risolvibile da una TM
$\mathcal{M}$ in $T_\mathcal{M}(n)$, allora è risolvibile da un qualsiasi altro
modello Turing-completo in $\pi(T_\mathcal{M})$ dove $\pi(\cdot)$ è un opportuno
polinomio}. Questa affermazione è detta \hl{``tesi di correlazione lineare''}.
Noi la dimostreremo solo tra i due modelli più importanti, la macchina \hl{RAM}
e la \hl{macchina di Turing}.

Prima di passare alla tesi di correlazione lineare, dimostriamo un \hl{risultato
intermedio} che useremo.

\begin{lem}[\hl{Occupazione sul nastro principale}]\label{thm:occupazione-nastro}
  Sia una macchina di Turing e una macchina RAM\@. Lo \hl{spazio occupato sul
  nastro principale è $\mathcal{O}(T_{RAM}(n))$}.
\end{lem}
\begin{proof}
  Ogni cella della RAM occupa $\log(i_j)+\log(\mathtt{M}[i_j])$ spazio e viene
  materializzata se e solo se la RAM effettua un operazione di \texttt{STORE}.
  L'istruzione di \texttt{STORE} costa alla RAM $\log(i_j) +
  \log(\mathtt{M}[i_j])$, quindi per riempire $r$ celle la RAM impiegherà
  $\sum_{j=1}^r \log(i_j) + \log(\mathtt{M}[i_j])$. Questa quantità di tempo è
  identica allo spazio che $r$ celle di memoria occuperanno sul nastro della
  TM\@.
\end{proof}

\begin{thm}[\hl{Correlazione temporale tra TM a $k$ nastri e RAM}]\label{thm:corr-tm-ram}
  Se un \hl{problema è risolvibile da una TM a $k$ nastri $\mathcal{M}$ in
  $T_\mathcal{M}(n)$ e da una macchina RAM in $T_{RAM}(n)$}, allora si avrà:

  \begin{equation}
    \mhl{ T_\mathcal{M}(n) = \pi(T_{RAM}(n)) }
  \end{equation}

  Dove $\pi(\cdot)$ è un opportuno polinomio.
\end{thm}
\begin{proof}
  Come primo passo dimostriamo che possiamo emulare un TM con una RAM in tempo
  polinomiale. Mappiamo innanzitutto le varie parti di una TM su una RAM\@:

  \begin{itemize}
    \item Lo stato della TM corrisponderà all'accumulatore della RAM;
    \item Una cella della RAM corrisponderà ad una cella di nastro;
    \item La RAM è suddivisa in blocchi da $k$ celle.
  \end{itemize}

  I blocchi saranno riempiti secondo questa strategia: nel blocco $0$ saranno
  salvate in ogni cella le posizioni delle $k$ testine; nei rimanenti $n>0$
  blocchi sarà salvato lo $n$-esimo blocco di ognuno dei $k$ nastri. La RAM
  emulerà la lettura di un carattere sotto la testina con un accesso indiretto,
  usando l'indice contenuto nel blocco $0$. Studiamo ora il costo
  dell'emulazione delle due operazioni che può effettuare una macchina di
  Turing:

  \begin{description}
    \item[Lettura] La lettura del blocco $0$ e dello stato avviene in
      $\Theta(k)$ mosse. La lettura dei valori sui nastri in corrispondenza
      delle testine avverrà in $\Theta(k)$ accessi indiretti.
    \item[Scrittura] La scrittura dello stato avviene in una mossa
      ($\Theta(1)$). Le scritture sui nastri e nel blocco $0$, come anche le
      letture, impiegano rispettivamente $\Theta(k)$ accessi indiretti e
      $\Theta(k)$ mosse.
  \end{description}

  La RAM è quindi capace di emulare una mossa della TM con $k$ mosse:

  \[
    T_{RAM}(n) = \Theta(T_\mathcal{M}(n))
      [ = \Theta(T_\mathcal{M}(n)\log(T_\mathcal{M}(n))) \text{ costo logaritmico} ]
  \]

  Studiamo ora l'emulazione da parte di una MT di una macchina RAM\@. Per
  semplicità ometteremo l'emulazione delle operazione di \texttt{MUL} e
  \texttt{DIV} in quanto non ledono alla generalità della dimostrazione.
  Organizziamo il nastro della TM come in figura~\ref{fig:thm-corr-tm-ram}.

  \begin{figure}[htb]
    \centering
    \begin{tikzpicture}
      \draw[black,thin,step=0.9] (0,0) grid (11.7, 0.9);

      \node[] at (0.45,  0.45) [] () {$\ldots$};
      \node[] at (1.35,  0.45) [] () {\Large$\triangleleft$};
      \node[] at (2.25,  0.45) [] () {\large$i_j$};
      \node[] at (3.15,  0.45) [] () {\Large$\ddagger$};
      \node[] at (4.05,  0.45) [] () {$\mathtt{M}[i_j]$};
      \node[] at (4.95,  0.45) [] () {\Large$\triangleright$};
      \node[] at (5.85,  0.45) [] () {$\ldots$};
      \node[] at (6.75,  0.45) [] () {\Large$\triangleleft$};
      \node[] at (7.65,  0.45) [] () {\large$i_k$};
      \node[] at (8.55,  0.45) [] () {\Large$\ddagger$};
      \node[] at (9.45,  0.45) [] () {$\mathtt{M}[i_k]$};
      \node[] at (10.35, 0.45) [] () {\Large$\triangleright$};
      \node[] at (11.25, 0.45) [] () {$\ldots$};
    \end{tikzpicture}
    \caption{Configurazione del nastro della TM\@.}%
    \label{fig:thm-corr-tm-ram}
  \end{figure}

  Il nastro è inizialmente vuoto, salveremo solo le celle in cui è avvenuta una
  \texttt{STORE}. Usiamo anche un ulteriore nastro per contenere il valore di
  $\mathtt{M}[0]$ in binario. Useremo anche un ultimo nastro come stoccaggio
  temporaneo per quando serve salvare per la prima volta $\mathtt{M}[i_j]$ ma
  $\mathtt{M}[i_k]$ e $\mathtt{M}[i_l]$ con $i_k < i_j < i_l$ sono già state
  salvate. Con questa configurazione, studiamo l'emulazione delle varie
  istruzioni della RAM (nel dettaglio vedremo solo \texttt{LOAD}, \texttt{STORE}
  e \texttt{ADD}):

  \begin{description}
    \item[\texttt{LOAD x}] Devo effettuare una ricerca di \texttt{x} sul nastro
      principale e poi copiare la porzione di nastro accanto nella zona dati di
      $\mathtt{M}[0]$ usando il nastro di supporto.
    \item[\texttt{STORE x}] Devo anche qui effettuare una ricerca di \texttt{x}
      sul nastro principale, se lo trovo vi salvo nello spazio adiacente il
      valore di $\mathtt{M}[0]$, altrimenti creo dello spazio usando il nastro
      di servizio se necessario e lo salvo.
    \item[\texttt{ADD x}] Effettuo ancora una ricerca di \texttt{x}, copio
      $\mathtt{M}[x]$ sul nastro di supporto, ne calcolo la somma scrivendo il
      risultato in $\mathtt{M}[0]$.
  \end{description}

  In generale possiamo dire grazie al lemma~\ref{thm:occupazione-nastro} che % chktex 44
  simulare una mossa della RAM richiede alle TM un numero di mosse minore o
  uguale ad una costante per la lunghezza del nastro principale. Quindi la TM
  impiega al più $\Theta(T_{RAM}(n))$ per simulare una mossa della RAM\@. Poiché
  ogni mossa della RAM ha costo unitario, possiamo dire che essa esegue
  $T_{RAM}(n)$ mosse. Quindi la simulazione completa della RAM da parte della TM
  costa al più

  \[
    \Theta(T_{RAM}(n) \cdot T_{RAM}(n)) = \Theta({(T_{RAM}(n))}^2)
  \]

  Quindi il legame tra $T_{RAM}(n)$ e $T_\mathcal{M}(n)$ è polinomiale.
\end{proof}

L'esistenza della correlazione polinomiale tra macchina RAM e macchina di Turing
ci permette di \hl{definire una classe di problemi risolvibili in tempo
polinomiale (classe P)}. Questo risultato ha portato ad una \hl{``testi di
trattabilità''}: i \hl{problemi risolvibili in P sono quelli trattabili}. In
questo caso per trattabili \hl{intendiamo risolvibili con complessità con
esponente ragionevole}. Infatti la classe P comprende anche polinomi come
$n^{30}$, ma \hl{empiricamente si è visto che la maggioranza dei problemi
polinomiali di interesse pratico ha un grado polinomiale accettabile} di solito
inferiore a $4$.

\section{Studio della complessità di algoritmi}\label{sec:comp-alg}

Dato un certo problema, un buon flusso di lavoro è il seguente:

\begin{enumerate}
  \item Concepire un algoritmo che lo risolve;
  \item Valutare la complessità dell'algoritmo trovato;
  \item Se la complessità calcolata è soddisfacente, implementare l'algoritmo
    nel linguaggio scelto.
\end{enumerate}

Per controllare la correttezza di un algoritmo, come abbiamo già visto, non
esiste una procedura generale. Ciò però non nega il fatto che per alcuni
particolari casi possiamo costruire degli algoritmi di controllo.

Il \hl{modello che useremo} per valutare la complessità è, ovviamente, la
\hl{macchina RAM}\@. Ci \hl{concentreremo}, inoltre, sulla \hl{complessità
temporale calcolata con il criterio di costo costante} in quanto gli algoritmi
trattati non hanno espansioni significative della dimensione dei singoli dati.
Nei rari casi in cui ciò accade, possiamo rappresentare questi numeri molto
grandi come vettori di numeri più piccoli. Il \hl{linguaggio} che useremo per
definire gli algoritmi è lo \hl{``pseudocodice''}. Esso non è altro che
\hl{un'astrazione delle caratteristiche dei linguaggi di programmazione più
comuni} (C, Java, ecc\dots). Una assunzione fondamentale sullo pseudocodice sarà
che \hl{ogni statement di esso può essere tradotto in un numero costante di
istruzioni RAM}\@.

\subsection{La sintassi dello pseudocodice}\label{sec:syntax-pseudocodice}

Innanzitutto ogni algoritmo sarà \hl{definito come una procedura, ossia una
funzione che prende un input e non ritorna nulla}. Forniamo ora la sintassi base
dello pseudocodice che useremo.

\paragraph{Operatori} Sono definiti i \hl{soliti operatori matematici}. Useremo
\hl{\peq{} come assegnamento e riserveremo \texttt{=} per uguaglianza}. Per i
\hl{confronti useremo i soliti} \texttt{<}, $\leq$, \texttt{>} e $\geq$.

\paragraph{Commenti} Sono legali solo \hl{commenti monoriga delimitati da
\pcom{}} oppure usando \texttt{//}.

\paragraph{Costrutti di controllo del flusso} Sono presenti \hl{tutti} i
costrutti di controllo del flusso che ci si aspetterebbe \hl{in un normale
linguaggio C-like}: \texttt{if}, \texttt{else}, \texttt{else if} e i cicli
\texttt{for\dots}, \texttt{while} e \texttt{do\dots while}.

\paragraph{Strutture dati e tipi aggregati} Sono definiti gli \hl{array} e sono
accessibili con una notazione C-like:

\begin{itemize}
  \item \hl{\texttt{A[i]} per indirizzare} lo $i-1$-esimo elemento
    (indicizzazione a partire da 0);
  \item \hl{\texttt{A[i..j]}} per estrarre un \hl{sotto-array} (slice).
\end{itemize}

È anche possibile creare \hl{tipi aggregati, i vari campi di questi sono
indirizzabili tramite `\texttt{.}'}. I tipi aggregati sono di default
\hl{identificati da un puntatore alla struttura}, quindi la notazione
`\texttt{.}' equivale al \texttt{->} del C/C++. Un \hl{puntatore indefinito ha
valore \texttt{NIL}}.

\paragraph{Chiamata di funzioni} Le chiamate a funzione seguono le consuete
regole di chiamata e ritorno di un valore. I \hl{parametri di tipo base sono
passati per valore, mentre i tipi aggregati per riferimento} (convenzione
Java-like).

\begin{esempio}
  Consideriamo la rimozione di un elemento rispettivamente da un vettore e da
  una lista.

  \begin{lstlisting}[language=pseudocodice]
    CancellaElV(v, len, e)
      i `\peq{}` 0
      while v[i] $\neq$ e
        i `\peq{}` i + 1
      while i < len -1
        i `\peq{}` v[i + 1]
        i `\peq{}` i + 1
      v[len - 1] `\peq` $\bot$

    CancellaElL(l, e)
      p `\peq` l
      while p.next $\neq$ NIL and p.next.value $\neq$ e
        p `\peq` p.next
      if p.next.value = e
        p.next `\peq` p.next.next
  \end{lstlisting}

  Sono entrambi, nel caso pessimo, $\Theta(n)$ dove $n$ è il numero degli
  elementi del vettore/lista.
\end{esempio}
\begin{esempio}
  Consideriamo il seguente algoritmo di moltiplicazione di matrici:

  \begin{lstlisting}[language=pseudocodice]
    A `\peq` Matrix(n, m)
    B `\peq` Matrix(m, o)

    MatrixMultiply(A, B)
      C `\peq` Matrix(n, o)
      for i `\peq` 0 to A.n - 1
        for j `\peq` 0 to B.o - 1
          C[i][j] `\peq` 0
          for k `\peq` 0 to A.m - 1
            C[i][j] `\peq` C[i][j] + A[i][k] * B[k][j]
      return C
  \end{lstlisting}

  La riga 5 viene eseguita $n*o$ volte, la 10 $n*m*o$ volte quindi l'algoritmo
  avrà complessità $\Theta(n*m*o)$ sia in generale che nel caso pessimo. Se le
  due matrici sono quadrate avremo $\Theta(n^3)$.
\end{esempio}

\subsection{Studio di algoritmi ricorsivi}\label{sec:alg-ricorsivi}

È possibile incontrare algoritmi la cui complessità non è immediatamente
esprimibile in forma chiusa. Il caso \hl{tipico} sono algoritmi che seguono la
\hl{strategia ``divide et impera''}. Questa strategia consiste nel
\hl{suddividere il problema in sottoproblemi con input $\frac{1}{b}$
dell'originale}; quando il sottoproblema ha \hl{ingresso} di dimensioni $n$
\hl{piccole a sufficienza può essere risolto a tempo costante}. Chiamiamo
\hl{$D(n)$} il costo del \hl{suddividere il problema} e con \hl{$C(n)$} il
\hl{costo di combinare le soluzioni}. Possiamo quindi esprimere il \hl{costo
totale $T(n)$} con la seguente \hl{equazione di ricorrenza}:

\begin{equation}
  \mhl{
    T(n) =
    \begin{cases}
      \Theta(1)                              & \quad \text{se } n < c \\
      D(n) + aT\left(\frac{n}{b}\right)+C(n) & \quad \text{altrimenti}
    \end{cases}
  }
\end{equation}

Dove le costanti \hl{$a$, $b$ e $c$} indicano \hl{rispettivamente} il \hl{numero
di chiamate ricorsive, il numero di suddivisioni dell'input e il numero sotto al
quale il sottoproblema ha costo costante}.

Per risolvere un problema in questa forma possiamo utilizzare \hl{3 tecniche:
sostituzione, studio dell'albero di ricorsione e il teorema dell'esperto}
(``master theorem'').

\subsubsection{Metodo di sostituzione}\label{sec:alg-ricorsivi-sost}

Il metodo di sostituzione è \hl{sostanzialmente una dimostrazione per induzione
del fatto che una soluzione ``intuita'' è effettivamente una soluzione}. Esso si
articola in 3 fasi:

\begin{enumerate}
  \item Intuire una soluzione (ovviamente);
  \item Sostituire la presunta soluzione nella ricorrenza;
  \item Dimostrare per induzione che la presunta soluzione è tale per
    l'equazione/disequazione alle ricorrenze.
\end{enumerate}

Usiamo come caso di studio l'algoritmo di ricerca binaria in quanto semplice e
dall'implementazione intuitiva. Possiamo esprimere la \hl{complessità della ricerca
binaria} come:

\begin{equation}
  \mhl{
    T(n) = \Theta(1) + T\left(\frac{n}{2}\right) + \Theta(1)
  }
\end{equation}

\hl{``Intuiamo'' una soluzione $T(n) = \mathcal{O}(\log(n))$}, ossia $T(n) \leq
c\log(n)$. Dobbiamo quindi dimostrare che

\begin{equation}
  T(n) = \Theta(1) + T\left(\frac{n}{2}\right) + \Theta(1) \leq c\log(n)
\end{equation}

Consideriamo \hl{vero per l'ipotesi di induzione $T(\frac{n}{2}) \leq
c\log(\frac{n}{2})$} in quanto $\frac{n}{2} < n$ e sostituiamo ottenendo:

\begin{equation}
  \mhl{
    T(n) \leq c\log\left(\frac{n}{2}\right) + \Theta(k) = c\log(n) - c\log(2) +
      \Theta(k) \leq c\log(n)
  }
\end{equation}

\begin{esempio}
  Determiniamo un limite superiore per $T(n) = 2T(\frac{n}{2})+n$. Intuiamo
  $\mathcal{O}(n\log(n))$, dimostriamo quindi che $T(n) \leq cn\log(n)$.
  Supponiamo vero per induzione che $T(n/2) \leq c(n/2\log(n/2))$ e sostituiamo:

  \begin{align*}
    T(n) & \leq 2c(n/2\log(n/2)) + n \leq cnlog(n/2)+n = \\
      & = cn\log(n) - cn\log(2) + n \leq \\
      & \leq cn\log(n) + (1-c\log(2))n < \\
      & < cn\log(n)
  \end{align*}

  Per dimostrare il caso base possiamo trovare un $n_0$ per il quale vale la
  nostra ipotesi, in questo caso $n_0 = 3$.
\end{esempio}
\begin{esempio}
  Troviamo un limite superiore per $T(n) = 2T(n/2) + 1$. Tentiamo di provare che
  $\mathcal{O}(n)$, ossia $T(n) = cn$. Supponiamo vero, sempre per induzione,
  $T(n/2) = cn/2$. Sostituiamo ottenendo che $T(n) \leq 2cn/2 + 1 = cn+1$. Non
  possiamo trovare un valore che faccia rispettare l'ipotesi: $cn+1 \geq cn$
  sempre. In questo caso non siamo riusciti a dimostrare il limite tramite
  sostituzione. Attenzione: ciò non implica che $T(n) \neq \mathcal{O}(n)$!
  Infatti se prendiamo come ipotesi $T(n) \leq cn-b$ con $b$ costante consente
  di dimostrare che $T(n) = \mathcal{O}(n)$.
\end{esempio}

\subsubsection{Studio dell'albero di ricorsione}\label{sec:alg-ricorsivi-albero}

Lo studio dell'albero di ricorsione ci \hl{fornisce un aiuto per trovare una
congettura da verificare con il metodo di sostituzione}. L'albero di ricorsione è
una \hl{rappresentazione delle chiamate ricorsive}, indicando per ognuna la
complessità. Ogni chiamata costituisce un nodo dell'albero, i chiamati appaiono
come figli del chiamante. Rappresentiamo \hl{l'albero di}:

\begin{equation}
  \mhl{
    T(n) = T\left(\frac{n}{3}\right) + T\left(\frac{2n}{3}\right) + n\label{eqn:albero-ricorsione}
  }
\end{equation}

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[
    level 1/.style = {sibling distance=4cm},
    level 2/.style = {sibling distance=2cm},
    level 3/.style = {sibling distance=1cm},
    level 4/.style = {sibling distance=1cm}
  ]
    \node{$n$}
      child{
        node{$\frac{n}{3}$}
          child{
            node{$\frac{n}{3^2}$}
              child {
                node{$\vdots$}
                  child{
                    node{$\frac{n}{3^k}$}
                  }
                  child{
                    node{$\frac{2n}{3^k}$}
                  }
              }
              child {
                node{$\vdots$}
              }
          }
          child{
            node{$\frac{2n}{3^2}$}
              child {
                node{$\vdots$}
              }
              child {
                node{$\vdots$}
              }
          }
      }
      child{
        node{$\frac{2n}{3}$}
          child{
            node{$\frac{2n}{3^2}$}
              child {
                node{$\vdots$}
              }
              child {
                node{$\vdots$}
              }
          }
          child{
            node{$\frac{2^2 n}{3^2}$}
              child {
                node{$\vdots$}
              }
              child {
                node{$\vdots$}
                  child{
                    node{$\frac{2^{k-1}n}{3^k}$}
                  }
                  child{
                    node{$\frac{2^k n}{3^k}$}
                  }
              }
          }
      };
  \end{tikzpicture}
  \caption{L'albero di ricorsione di~\ref{eqn:albero-ricorsione}.}%
  \label{fig:albero-ricorsione}
\end{figure}

L'albero ha la \hl{ramificazione a profondità massima posta sull'estrema destra}
della figura~\ref{fig:albero-ricorsione}. Sappiamo che essa ha \hl{profondità $k$
pari a}:

\begin{equation}
  \mhl{
    \frac{2^k}{3^k} n = 1 \implies{} k = \log_3(2^k n) = \cdots = c\log_3(n)
  }
\end{equation}

Il \hl{costo pessimo per il contributo di un dato livello è la $n$} della radice
dell'albero, \hl{congetturiamo allora che $T(n) = \Theta(n\log(n))$}. Possiamo
quindi proseguire la dimostrazione con i metodi visti
in~\ref{sec:alg-ricorsivi-sost}.

\subsubsection{Teorema dell'esperto}\label{sec:alg-ricorsivi-esperto}

Il teorema dell'esperto è uno \hl{strumento per risolvere buona parte delle
equazioni alle ricorrenze}. Affinché sia applicabile, la \hl{ricorrenza deve
avere la seguente forma}:

\begin{equation}
  \mhl{
    T(n) = aT\left(\frac{n}{b}\right) + f(n)
  }
\end{equation}

Con \hl{$a \geq 1$ e $b>1$}. L'idea di fondo è quella di \hl{confrontare
$n^{\log_b(a)}$ (effetto delle chiamate ricorsive) con quello di $f(n)$ (costo
di ogni singola chiamata)}. Le \hl{ipotesi} del teorema sono le seguenti:

\begin{enumerate}
  \item \hl{$a$ deve essere costante e maggiore di 1};
  \item \hl{$f(n)$ deve essere sommata, non sottratta o altro a
    $aT(\frac{n}{b})$};
  \item \hl{Il legame tra $n^{\log_b(a)}$ e $f(n)$ deve essere polinomiale}.
\end{enumerate}

Se queste ipotesi sono valide, è possibile ricavare informazioni sulla
complessità a seconda del caso in cui ci si trova:

\paragraph{Caso 1} Nel primo caso abbiamo che \hl{$f(n) =
\mathcal{O}(n^{\log_b(a) - \epsilon})$ per un $\epsilon>0$}. La \hl{complessità
risultante sarà $T(n) = \Theta(n^{\log_b(a)})$}. Intuitivamente si può intendere
questa situazione come il caso nel quale \hl{il costo della ricorsione domina
quello della chiamata}.

\paragraph{Caso 2} Nel secondo caso abbiamo che \hl{$f(n) =
\mathcal{O}(n^{\log_b(a)}{(\log(n))}^k)$}. La \hl{complessità risultante sarà
$T(n) = \Theta(n^{\log_b(a)}{(\log(n))}^{k+1})$}. In questo caso, invece,
possiamo intuire che \hl{il contributo della ricorsione e quello della singola
chiamata differiscono per meno di un termine polinomiale}.

\paragraph{Caso 3} Nell'ultimo caso abbiamo che \hl{$f(n) = \Omega(n^{\log_b(a)
+ \epsilon})$ per un $\epsilon >0$}. Se questo è vero, \hl{deve anche valere
$af(\frac{n}{b})<cf(n)$} per un qualche valore di \hl{$c<1$}. Se le ipotesi sono
rispettate \hl{abbiamo che $T(n) = \Theta(f(n))$}. Qui, invece, \hl{domina il
costo della singola chiamata}.

\subsection{Algoritmi di ordinamento}\label{sec:alg-odinamento}

Tra i problemi che capita più spesso di dover risolvere, l'ordinamento di una
collezione di oggetti è un classico. \hl{Un punto chiave dell'utilità
dell'ordinamento è consentire utilizzare una ricerca più efficiente, come ad
esempio quella binaria, sulla nostra collezione}. Analizzeremo soluzioni diverse
considerando la loro complessità temporale, spaziale e relative peculiarità.

\begin{defn}[\hl{Proprietà di stabilità di un ordinamento}]\label{def:prop-stabilita}
  Diciamo che un ordinamento gode della \hl{proprietà di stabilità se non modifica
  l'ordine di elementi duplicati}.
\end{defn}

\end{document}
